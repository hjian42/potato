{"id":"9258764","text":["Nueva Granada University in Colombia, South America using Microsoft's Kinect skeletal tracking for developing and assess the design of workspaces in several areas such as laboratories."],"abstract":[["Anthropometry is known as the science that studies the human body dimensions, this measurements are acquire using special devices and techniques whose results are analyzed through statistics. Anthropometry plays an important role within the industrial design process in areas such as clothing, ergonomics, and biomechanics, where statistical data about body medians allow optimizing product design. Recently, image processing and hardware advances are allowing the development of applications that allow an user to preview wardrobe, costumes, games or advergames and even different types of environments according to the user measurements. This project proposes the development of a complimentary tool for acquiring user anthropometric data for characterizing users in the Mil. Nueva Granada University in Colombia, South America using Microsoft's Kinect skeletal tracking for developing and assess the design of workspaces in several areas such as laboratories."]]}
{"id":"220647579","text":["Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. "],"abstract":[["Zero-shot learning (ZSL) addresses the unseen class recognition problem by leveraging semantic information to transfer knowledge from seen classes to unseen classes. Generative models synthesize the unseen visual features and convert ZSL into a classical supervised learning problem. These generative models are trained using the seen classes and are expected to implicitly transfer the knowledge from seen to unseen classes. However, their performance is stymied by overfitting, which leads to substandard performance on Generalized Zero-Shot learning (GZSL). To address this concern, we propose the novel LsrGAN, a generative model that Leverages the Semantic Relationship between seen and unseen categories and explicitly performs knowledge transfer by incorporating a novel Semantic Regularized Loss (SR-Loss). The SR-loss guides the LsrGAN to generate visual features that mirror the semantic relationships between seen and unseen classes. Experiments on seven benchmark datasets, including the challenging Wikipedia text-based CUB and NABirds splits, and Attribute-based AWA, CUB, and SUN, demonstrates the superiority of the LsrGAN compared to previous state-of-the-art approaches under both ZSL and GZSL. Code is available at https: \/\/ github. com\/ Maunil\/ LsrGAN"]]}
{"id":"221191301","text":["Second to last, we discuss real world use cases coming from and outside LinkedIn. "],"abstract":[["Anomaly detection has been widely studied and used in diverse applications. Building an effective anomaly detection system requires researchers and developers to learn complex structure from noisy data, identify dynamic anomaly patterns, and detect anomalies with limited labels. Recent advancements in deep learning techniques have greatly improved anomaly detection performance, in comparison with classical approaches, and have extended anomaly detection to a wide variety of applications. This tutorial will help the audience gain a comprehensive understanding of deep learning based anomaly detection techniques in various application domains. First, we give an overview of the anomaly detection problem, introducing the approaches taken before the deep model era and listing out the challenges they faced. Then we survey the state-of-the-art deep learning models that range from building block neural network structures such as MLP, CNN, and LSTM, to more complex structures such as autoencoder, generative models (VAE, GAN, Flow-based models), to deep one-class detection models, etc. In addition, we illustrate how techniques such as transfer learning and reinforcement learning can help amend the label sparsity issue in anomaly detection problems and how to collect and make the best use of user labels in practice. Second to last, we discuss real world use cases coming from and outside LinkedIn. The tutorial concludes with a discussion of future trends."]]}
{"id":"34967081","text":["We found that visitors tended to follow the robot tour guide from a greater distance (more than 3 meters away from the robot) more frequently when the robot was oriented towards the visitors than when it was oriented towards the point of interest. "],"abstract":[["In this paper, we describe a field study with a tour guide robot that guided visitors through a historical site. Our focus was to determine how a robot\u2019s orientation behaviour influenced visitors\u2019 orientation and the formations groups of visitors formed around the robot. During the study a remotecontrolled robot gave short guided tours and explained some points of interest in the hall of Festivities in the Royal Alcazar in Seville (Spain). To get insight into visitors\u2019 reactions to the robot\u2019s non-verbal orientation behaviour, two orientations of the robot were tested; either the robot was oriented with its front towards the visitors, or the robot was oriented with its front towards the point of interest. From the study we learned that people reacted strongly to the orientation of the robot. We found that visitors tended to follow the robot tour guide from a greater distance (more than 3 meters away from the robot) more frequently when the robot was oriented towards the visitors than when it was oriented towards the point of interest. Further, when the robot was oriented towards the point of interest, people knew where to look and walked towards the robot more often. On the other hand, people also lost interest in the robot more often when it was oriented towards the point of interest. The analysis of visitors\u2019 orientation and formations led to design guidelines for effective robot guide behaviour."]]}
{"id":"195754821","text":["We present the collaborative Virtual Reality Neurorobotics Lab, which allows multiple collocated and remote users to experience, discuss and participate in neurorobotic experiments in immersive virtual reality. "],"abstract":[["We present the collaborative Virtual Reality Neurorobotics Lab, which allows multiple collocated and remote users to experience, discuss and participate in neurorobotic experiments in immersive virtual reality. We describe the coupling of the Neurorobotics Platform of the Human Brain Project with our collaborative virtual reality and 3D telepresence infrastructure and highlight future opportunities arising from our work for research on direct human interaction with simulated robots and brains."]]}
{"id":"249039235","text":["We found evidence that the FP\/FN trade-o\ufb00 can impact learners\u2019 performance when working cooperatively with the assistant, and that the soft highlighting condition may generate less trust from learners compared to the other conditions. "],"abstract":[[". In a workplace learning scenario in which workers in a simulated Material Recovery Facility learn to recognize and manipulate objects on conveyer belts, we studied how imperfect guidance from a machine learning (ML) assistant may impact learners\u2019 experience and behaviors. Speci\ufb01cally, in a randomized experiment ( n = 181 participants from Amazon MTurk) we varied the assistant\u2019s False Positive (FP) and False Negative (FN) rates in detecting non-recyclable objects and assessed the impact on learners\u2019 performance, learning, and trust. We also explored a soft highlighting [8] condition, whereby the assistant provides \ufb01ne-grained information about how con\ufb01dent it is. We found evidence that the FP\/FN trade-o\ufb00 can impact learners\u2019 performance when working cooperatively with the assistant, and that the soft highlighting condition may generate less trust from learners compared to the other conditions. There was tentative evidence that workers\u2019 behaviors were impacted by the FP\/FN trade-o\ufb00 of their assigned experimental condition even after the ML assistant was removed. Finally, in a follow-up study ( n = 27) we found evidence that learners modulate their behaviors based on the \ufb01ne-grained con\ufb01dence values conveyed by the assistant."]]}
{"id":"38934160","text":["The latter has the advantage of learning long-range temporal orders, yet the former is more adaptive to partial orders. "],"abstract":[["There has been a recent line of work automatically learning scripts from unstructured texts, by modeling narrative event chains. While the dominant approach group events using event pair relations, LSTMs have been used to encode full chains of narrative events. The latter has the advantage of learning long-range temporal orders, yet the former is more adaptive to partial orders. We propose a neural model that leverages the advantages of both methods, by using LSTM hidden states as features for event pair modelling. A dynamic memory network is utilized to automatically induce weights on existing events for inferring a subsequent event. Standard evaluation shows that our method significantly outperforms both methods above, giving the best results reported so far."]]}
{"id":"9323254","text":["Streaming video has received a lot of attention from industry and academia. "],"abstract":[["Streaming video has received a lot of attention from industry and academia. In this work, we study the characteristics and challenges associated with large-scale live video delivery. Using logs from a commercial Content Delivery Network (CDN), we study live video delivery for a major entertainment event that was streamed by hundreds of thousands of viewers in North America. We analyze Quality-of-Experience (QoE) for the event and note that a significant number of users suffer QoE impairments. As a consequence of QoE impairments, these users exhibit lower engagement metrics."]]}
{"id":"778533","text":["The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized. "],"abstract":[["Self-adaptive systems tend to be reactive and myopic, adapting in response to changes without anticipating what the subsequent adaptation needs will be. Adapting reactively can result in inefficiencies due to the system performing a suboptimal sequence of adaptations. Furthermore, when adaptations have latency, and take some time to produce their effect, they have to be started with sufficient lead time so that they complete by the time their effect is needed. Proactive latency-aware adaptation addresses these issues by making adaptation decisions with a look-ahead horizon and taking adaptation latency into account. In this paper we present an approach for proactive latency-aware adaptation under uncertainty that uses probabilistic model checking for adaptation decisions. The key idea is to use a formal model of the adaptive system in which the adaptation decision is left underspecified through nondeterminism, and have the model checker resolve the nondeterministic choices so that the accumulated utility over the horizon is maximized. The adaptation decision is optimal over the horizon, and takes into account the inherent uncertainty of the environment predictions needed for looking ahead. Our results show that the decision based on a look-ahead horizon, and the factoring of both tactic latency and environment uncertainty, considerably improve the effectiveness of adaptation decisions."]]}
{"id":"220365019","text":["Based on previous works in this field, this research will discuss how feeling of difficulty (i.e. a type of ME) functions in real-time based on the hypothesis that eye-tracking and SCR data can provide objective measures. "],"abstract":[["Metacognitive experience (ME) plays an important role in self-regulated learning. To date, through mainly self-reporting methodology, metacognition assessment lacks objective evidence and therefore hinder the discussion of its subjective and implicit nature. In exploring ME, eye-tracking and skin conductance response (SCR) offer certain advantages over self-reporting methods. However, to date, most studies tend to focus on utilizing these measures to explore metacognitive skills (MS) rather than ME. Also, while some studies do explore ME with these measures tend to utilize the data from a summative perspective rather than aligning the data with the real-time ME behaviours. Based on previous works in this field, this research will discuss how feeling of difficulty (i.e. a type of ME) functions in real-time based on the hypothesis that eye-tracking and SCR data can provide objective measures. Through such, a better understanding of how FOD functions could be gained and therefore contribute to the support of learners\u2019 metacognitive competencies."]]}
{"id":"133495834","text":["This form of network benefits, first proposed by [3], is in line with the celebrated Metcalfe\u2019s law, where the function is identity. "],"abstract":[["How to Hide in a Network? Francis Bloch, Bhaskar Dutta, and Marcin Dziubi\u0144ski3(B) 1 Universit\u00e9 Paris 1 and Paris School of Economics, 48 Boulevard Jourdan, 75014 Paris, France francis.bloch@univ-paris1.fr 2 University of Warwick and Ashoka University, Coventry CV4 7AL, UK b.dutta@warwick.ac.uk 3 Institute of Informatics, University of Warsaw, Banacha 2, 02-097 Warsaw, Poland m.dziubinski@mimuw.edu.pl We propose a model of strategic hiding in a network in face of a hostile authority. Given a set of nodes, the hider chooses a network over these nodes together with a node. The network chosen by the hider is observed by the seeker (the hostile authority) but the location choice is not observed. The seeker chooses one of the nodes in the network to inspect. The inspected node is removed from the network. If the hider hides in the inspected node or one of its neighbours, he is caught by the seeker and suffers a penalty. Otherwise, he enjoys the benefits from the network that are a convex and increasing function of the number of nodes (including himself) that the hider can access (directly or not) in the network. This form of network benefits, first proposed by [3], is in line with the celebrated Metcalfe\u2019s law, where the function is identity. The objectives of the seeker are to minimize the payoff of the hider and the proposed model takes the form of a two-stage zero-sum game. The hide and seek stage in our model is similar to the hide and seek games on graphs of [2], with the difference that in their case the penalty from being caught is 0 and benefits from not being caught are fixed and independent of the graph. Unlike in the model of [1], in our model the authorities choose their seeking strategy knowing the network and only one node chooses the network topology to hide himself. This is similar to the model of [4]. However, unlike in their model, the authorities are strategic and they take into account the incentives and strategic behaviour of the hider when choosing the seeking strategy. Although very stylised and simple, the model allows us to capture the trade-off between secrecy and network benefits. We provide optimal networks for the hider and characterize optimal strategies of the two players on these networks. In general, the optimal networks consists of a number of singleton nodes and a connected component which is either a cycle or a core-periphery network. If the component is a cycle, in equilibrium the hider mixes uniformly across its nodes. If the component is a core-periphery network, the hider mixes uniformly across the periphery nodes. This provides theoretical support to the claim that the hider chooses networks where his centrality is small and indistinguishable from the centralities of the other nodes. This work was supported by Polish National Science Centre through Grant 2014\/13\/B\/ST6\/01807. c \u00a9 Springer Nature Switzerland AG 2018 G. Christodoulou and T. Harks (Eds.): WINE 2018, LNCS 11316, pp. 441\u2013442, 2018. https:\/\/doi.org\/10.1007\/978-3-030-04612-5"]]}
{"id":"207939512","text":["Our solutions build on recent work on updatable encryption but with significant enhancements applicable to the remote KMS setting. "],"abstract":[["We introduce Oblivious Key Management Systems (KMS) as a much more secure alternative to traditional wrapping-based KMS that form the backbone of key management in large-scale data storage deployments. The new system, that builds on Oblivious Pseudorandom Functions (OPRF), hides keys and object identifiers from the KMS, offers unconditional security for key transport, provides key verifiability, reduces storage, and more. Further, we show how to provide all these features in a distributed threshold implementation that enhances protection against server compromise. We extend this system with updatable encryption capability that supports key updates (known as key rotation) so that upon the periodic change of OPRF keys by the KMS server, a very efficient update procedure allows a client of the KMS service to non-interactively update all its encrypted data to be decryptable only by the new key. This enhances security with forward and post-compromise security, namely, security against future and past compromises, respectively, of the client's OPRF keys held by the KMS. Additionally, and in contrast to traditional KMS, our solution supports public key encryption and dispenses with any interaction with the KMS for data encryption (only decryption by the client requires such communication). Our solutions build on recent work on updatable encryption but with significant enhancements applicable to the remote KMS setting. In addition to the critical security improvements, our designs are highly efficient and ready for use in practice. We report on experimental implementation and performance."]]}
{"id":"64278512","text":["However, for a wide range of challenge logics there exists an elegant and uniform solution: By modeling and studying these logics as fragments of classical higher-order logic (HOL) [1, 4] \u2014 a research direction I have recently proposed [3] \u2014 existing results for HOL can often be reused. "],"abstract":[["The development of cut-free calculi for expressive logics, e.g. quantified non-classical logics, is usually a non-trivial task. However, for a wide range of challenge logics there exists an elegant and uniform solution: By modeling and studying these logics as fragments of classical higher-order logic (HOL) [1, 4] \u2014 a research direction I have recently proposed [3] \u2014 existing results for HOL can often be reused. We illustrate the idea with quantified conditional logics [7]."]]}
{"id":"3803029","text":["As a result, they fail to detect attacks using different protocols and, hence, have limited coverage. "],"abstract":[["Cache-based covert channel attacks use highly-tuned shared-cache conflict misses to pass information from a trojan to a spy process. Detecting such attacks is very challenging. State of the art detection mechanisms do not consider the general characteristics of such attacks and, instead, focus on specific communication protocols. As a result, they fail to detect attacks using different protocols and, hence, have limited coverage. In this paper, we make the following observation about these attacks: not only are the malicious accesses highly tuned to the mapping of addresses to the caches; they also follow a distinctive cadence as bits are being received. Changing the mapping of addresses to the caches substantially disrupts the conflict miss patterns, but retains the cadence. This is in contrast to benign programs. Based on this observation, we propose a novel, high-coverage approach to detect cache-based covert channel attacks. It is called ReplayConfusion, and is based on Record and deterministic Replay (RnR). After a program's execution is recorded, it is deterministically replayed using a different mapping of addresses to the caches. We then analyze the difference between the cache miss rate timelines of the two runs. If the difference function is both sizable and exhibits a periodic pattern, it indicates that there is an attack. This paper also introduces a new taxonomy of cache-based covert channel attacks, and shows that ReplayConfusion uncovers examples from all the categories. Finally, ReplayConfusion only needs simple hardware."]]}
{"id":"212673129","text":["To the best of our knowledge, there are no existing RWR algorithms which, at the same time, (1) are index-free, (2) return answers with a theoretical guarantee and (3) are efficient. "],"abstract":[["Due to the prevalence of graph data, graph analysis is very important nowadays. One popular analysis on graph data is Random Walk with Restart (RWR) since it provides a good metric for measuring the proximity of two nodes in a graph. Although RWR is important, it is challenging to design an algorithm for RWR. To the best of our knowledge, there are no existing RWR algorithms which, at the same time, (1) are index-free, (2) return answers with a theoretical guarantee and (3) are efficient. Motivated by this, in this paper, we propose an index-free algorithm called Residue-Accumulated approach (ResAcc) which returns answers with a theoretical guarantee efficiently. Our experimental evaluations on large-scale real graphs show that ResAcc is up to 4 times faster than the best-known previous algorithm, guaranteeing the same accuracy. Under typical settings, the best-known algorithm ran around 1000 seconds on a large dataset containing 41.7 million nodes, which is too time-consuming, while ResAcc finished in 275 seconds with the same accuracy. Moreover, ResAcc is up to 6 orders of magnitude more accurate than the best-known algorithm in practice with the same execution time, which is considered as a substantial improvement."]]}
{"id":"1493941","text":["i) it uses the same features extracted for detection, "],"abstract":[["In this paper we evaluate the quality of the activation layers of a convolutional neural network (CNN) for the generation of object proposals. We generate hypotheses in a sliding-window fashion over different activation layers and show that the final convolutional layers can find the object of interest with high recall but poor localization due to the coarseness of the feature maps. Instead, the first layers of the network can better localize the object of interest but with a reduced recall. Based on this observation we design a method for proposing object locations that is based on CNN features and that combines the best of both worlds. We build an inverse cascade that, going from the final to the initial convolutional layers of the CNN, selects the most promising object locations and refines their boxes in a coarse-to-fine manner. The method is efficient, because i) it uses the same features extracted for detection, ii) it aggregates features using integral images, and iii) it avoids a dense evaluation of the proposals due to the inverse coarse-to-fine cascade. The method is also accurate, it outperforms most of the previously proposed object proposals approaches and when plugged into a CNN-based detector produces state-of-the-art detection performance."]]}
{"id":"252090077","text":["Despite increasing reliance on personalization in digital platforms, many algorithms that curate content or information for users have been met with resistance. "],"abstract":[["Despite increasing reliance on personalization in digital platforms, many algorithms that curate content or information for users have been met with resistance. When users feel dissatisfied or harmed by recommendations, this can lead users to hate, or feel negatively towards these personalized systems. Algorithmic hate detrimentally impacts both users and the system, and can result in various forms of algorithmic harm, or in extreme cases can lead to public protests against \u201cthe algorithm\u201d in question. In this work, we summarize some of the most common causes of algorithmic hate and their negative consequences through various case studies of personalized recommender systems. We explore promising future directions for the RecSys research community that could help alleviate algorithmic hate and improve the relationship between recommender systems and their users."]]}
{"id":"6580782","text":["Clipboards are omnipresent on today's personal computing platforms. "],"abstract":[["Clipboards are omnipresent on today's personal computing platforms. They provide copy-and-paste functionalities that let users easily reorganize information and quickly transfer data across applications. In this work, we introduce personal clipboards to multi-user surfaces. Personal clipboards enable individual and independent copy-and-paste operations, in the presence of multiple users concurrently sharing the same direct-touch interface. As common surface computing platforms do not distinguish touch input of different users, we have developed clipboards that leverage complementary personalization strategies. Specifically, we have built a context menu clipboard based on implicit user identification of every touch, a clipboard based on personal subareas dynamically placed on the surface, and a handheld clipboard based on integration of personal devices for surface interaction. In a user study, we demonstrate the effectiveness of personal clipboards for shared surfaces, and show that different personalization strategies enable clipboards, albeit with different impacts on interaction characteristics."]]}
{"id":"235435822","text":["SUPPLEMENTARY MATERIAL A narrated video illustrating our approach is available at https:\/\/youtu.be\/Nze1wlfmzTQ. "],"abstract":[["Quadrotors are extremely agile, so much in fact, that classic first-principle-models come to their limits. Aerodynamic effects, while insignificant at low speeds, become the dominant model defect during high speeds or agile maneuvers. Accurate modeling is needed to design robust high-performance control systems and enable flying close to the platform\u2019s physical limits. We propose a hybrid approach fusing first principles and learning to model quadrotors and their aerodynamic effects with unprecedented accuracy. First principles fail to capture such aerodynamic effects, rendering traditional approaches inaccurate when used for simulation or controller tuning. Data-driven approaches try to capture aerodynamic effects with blackbox modeling, such as neural networks; however, they struggle to robustly generalize to arbitrary flight conditions. Our hybrid approach unifies and outperforms both first-principles bladeelement momentum theory and learned residual dynamics. It is evaluated in one of the world\u2019s largest motion-capture systems, using autonomous-quadrotor-flight data at speeds up to 65 km\/h. The resulting model captures the aerodynamic thrust, torques, and parasitic effects with astonishing accuracy, outperforming existing models with 50% reduced prediction errors, and shows strong generalization capabilities beyond the training set. SUPPLEMENTARY MATERIAL A narrated video illustrating our approach is available at https:\/\/youtu.be\/Nze1wlfmzTQ. Code and dataset can be found at http:\/\/rpg.ifi.uzh.ch\/NeuroBEM.html."]]}
{"id":"9101403","text":["Complementing these, we obtain a PTAS for a structured class of zero-sum games (where obtaining an FPTAS is still NP-hard) when the payoff matrices obey a Lipschitz condition. "],"abstract":[["We study the optimization problem faced by a perfectly informed principal in a Bayesian game, who reveals information to the players about the state of nature to obtain a desirable equilibrium. This signaling problem is the natural design question motivated by uncertainty in games and has attracted much recent attention. We present new hardness results for signaling problems in (a) Bayesian two-player zero-sum games, and (b) Bayesian network routing games. For Bayesian zero-sum games, when the principal seeks to maximize the equilibrium utility of a player, we show that it is NP-hard to obtain an additive FPTAS. Our hardness proof exploits duality and the equivalence of separation and optimization in a novel way. Further, we rule out an additive PTAS assuming planted clique hardness, which states that no polynomial time algorithm can recover a planted clique from an Erd\\H{o}s-R\\'enyi random graph. Complementing these, we obtain a PTAS for a structured class of zero-sum games (where obtaining an FPTAS is still NP-hard) when the payoff matrices obey a Lipschitz condition. Previous results ruled out an FPTAS assuming planted-clique hardness, and a PTAS only for implicit games with quasi-polynomial-size strategy sets. For Bayesian network routing games, wherein the principal seeks to minimize the average latency of the Nash flow, we show that it is NP-hard to obtain a (multiplicative) (4\/3 - epsilon)-approximation, even for linear latency functions. This is the optimal inapproximability result for linear latencies, since we show that full revelation achieves a (4\/3)-approximation for linear latencies."]]}
{"id":"4960605","text":["Our experiments show that we successfully extend the advantages of the low-fat pointer encoding to stack objects. "],"abstract":[["\u2014Object bounds over\ufb02ow errors are a common source of security vulnerabilities. In principle, bounds check instrumentation eliminates the problem, but this introduces high overheads and is further hampered by limited compatibility against un-instrumented code. On 64-bit systems, low-fat pointers are a recent scheme for implementing ef\ufb01cient and compatible bounds checking by transparently encoding meta information within the native pointer representation itself. However, low-fat pointers are traditionally used for heap objects only, where the allocator has suf\ufb01cient control over object location necessary for the encoding. This is a problem for stack allocation, where there exist strong constraints regarding the location of stack objects that is apparently incompatible with the low-fat pointer approach. To address this problem, we present an extension of low-fat pointers to stack objects by using a collection of techniques, such as pointer mirroring and memory aliasing, thereby allowing stack objects to enjoy bounds error protection from instrumented code. Our extension is compatible with common special uses of the stack, such as alloca , setjmp and longjmp , exceptions, and multi-threading, which rely on direct manipulation of the stack pointer. Our experiments show that we successfully extend the advantages of the low-fat pointer encoding to stack objects. The end result is a competitive bounds checking instrumentation for the stack and heap with low memory and runtime overheads, and high compatibility with un-instrumented legacy code."]]}
{"id":"7684534","text":["Both the map and trajectory updates are shared online with the following vehicles and permit them to derive their absolute location with respect to a common reference trajectory from their current camera image. "],"abstract":[["This paper addresses platooning navigation as part of new transportation services emerging nowadays in urban areas. Platooning formation is ensured using a global decentralized control strategy supported by inter-vehicle communications. A large motion flexibility is achieved according to a manual guidance mode, i.e. the path to follow is inferred online from the motion of the manually driven first vehicle. For this purpose, a visual SLAM algorithm that relies on monocular vision is run on the lead vehicle and coupled with a trajectory creation procedure. Both the map and trajectory updates are shared online with the following vehicles and permit them to derive their absolute location with respect to a common reference trajectory from their current camera image. Full-scale experiments with two urban vehicles demonstrate the performance of the proposed approach."]]}
{"id":"13976092","text":["Our evaluations show that Shandian reduces page load times by more than half for both mobile phones and desktops while incurring modest overheads to data usage."],"abstract":[["Web page loads are slow due to intrinsic inefficiencies in the page load process. Our study shows that the inefficiencies are attributable not only to the contents and structure of the Web pages (e.g., three-fourths of the CSS resources are not used during the initial page load) but also the way that pages are loaded (e.g., 15% of page load times are spent waiting for parsing-blocking resources to be loaded). \n \nTo address these inefficiencies, this paper presents Shandian (which means lightening in Chinese) that restructures the page load process to speed up page loads. Shandian exercises control over what portions of the page gets communicated and in what order so that the initial page load is optimized. Unlike previous techniques, Shandian works on demand without requiring a training period, is compatible with existing latency-reducing techniques (e.g., caching and CDNs), supports security features that enforce same-origin policies, and does not impose additional privacy risks. Our evaluations show that Shandian reduces page load times by more than half for both mobile phones and desktops while incurring modest overheads to data usage."]]}
{"id":"11894908","text":["First, LTE speeds enable subscribers at venues to stream high-quality video and this can be a signficant source of traffic. "],"abstract":[["Large events like the Super Bowl, where almost 75K attendees congegrate for several hours, poses a significant challenge in the planning, design and deployment of wireless networks. This was one of the first events where the LTE cellular network was available widely, in addition to almost 700 WiFi free hotspots. The Super Bowl in 2013 was also unprecedented because of a stadium-wide power outage for over half an hour. This study is the first to look in-depth at the user behaviours and traffic demand of a large ISP's celluar network at such an unique event. The findings of this study can be used to guide the design of the communication networks of large venues in the future. There are several key insights from our study of the data collected. First, LTE speeds enable subscribers at venues to stream high-quality video and this can be a signficant source of traffic. Second, the configuration of the uplink for such events is key, and a thoughtful approach to the design of applications that use the cloud for storing user data can substantially mitigate the congestion on the resource constrained uplink. Further, while it is tempting to take advantage of multicast on the cellular network (e.g., deploying technologies such as eMBMS in a venue), our results indicate that there is a need to combine multicast with caching to remove the strict requirement of overlap of requests from users to derive that benefit."]]}
{"id":"251447764","text":["It is solver-agnostic but extensible to allow for solver-specific testing and supports option fuzzing, cross-checking with other solvers, translation to SMT-LIBv2, and SMT-LIBv2 input fuzzing. "],"abstract":[[". SMT solvers are highly complex pieces of software with performance, robustness, and correctness as key requirements. Complement-ing traditional testing techniques for these solvers with randomized stress testing has been shown to be quite effective. Recent work has showcased the value of input fuzzing for finding issues, but this approach typically does not comprehensively test a solver\u2019s API. Previous work on model-based API fuzzing was tailored to a single solver and a small subset of SMT-LIB. We present Murxla, a comprehensive, modular, and highly extensible model-based API fuzzer for SMT solvers. Murxla randomly generates valid sequences of solver API calls based on a customizable API model, with full support for the semantics and features of SMT-LIB. It is solver-agnostic but extensible to allow for solver-specific testing and supports option fuzzing, cross-checking with other solvers, translation to SMT-LIBv2, and SMT-LIBv2 input fuzzing. Our evaluation confirms its efficacy in finding issues in multiple state-of-the-art SMT solvers."]]}
{"id":"218489781","text":["Due to its simplicity, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT can also be viewed as a way of turning a block cipher into a tweakable block cipher by dividing the block cipher into three chunks, and adding the tweak at the two cutting points only. "],"abstract":[["In this paper, we propose Tweak-aNd-Tweak (\\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT for short) mode, which builds a tweakable block cipher from three independent block ciphers. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT handles the tweak input by simply XOR-ing the unmodified tweak into the internal state of block ciphers twice. Due to its simplicity, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT can also be viewed as a way of turning a block cipher into a tweakable block cipher by dividing the block cipher into three chunks, and adding the tweak at the two cutting points only. \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT is proven to be of beyond-birthday-bound \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$2^{2n\/3}$$\\end{document}22n\/3 security, under the assumption that the three chunks are independent secure n-bit SPRPs. It clearly brings minimum possible overhead to both software and hardware implementations. To demonstrate this, an instantiation named TNT-AES with \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$6 $$\\end{document}6, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$6 $$\\end{document}6, \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$6 $$\\end{document}6 rounds of AES as the underlying block ciphers is proposed. Besides the inherent proven security bound and tweak-independent rekeying feature of the \\documentclass[12pt]{minimal} \\usepackage{amsmath} \\usepackage{wasysym} \\usepackage{amsfonts} \\usepackage{amssymb} \\usepackage{amsbsy} \\usepackage{mathrsfs} \\usepackage{upgreek} \\setlength{\\oddsidemargin}{-69pt} \\begin{document}$$\\mathsf {TNT}$$\\end{document}TNT mode, the performance of TNT-AES is comparable with all existing TBCs designed through modular methods."]]}
{"id":"251846953","text":["It was found that four defect levels may be present in the sample which are shallow donor, shallow acceptor, deep donor, and deep acceptor. "],"abstract":[["Undoped bulk BaSi2 on Si(111) substrate was studied by power dependent photoluminescence (PL). The Ba-to-Si deposition rate ratio of the sample is 3.7. At 10 K, PL spectrum consists of multiple peaks with energies smaller than the band gap. As the excitation power is increased, the low energy peaks become saturated while the high energy peaks become more pronounced. The PL spectrum was best fit with four Gaussian curves with PL peak energies of 1.12, 1.05, 0.992 and 0.878 eV. The power law exponents obtained from the experiment is then compared with the calculated exponents based on the rate equations. It was found that four defect levels may be present in the sample which are shallow donor, shallow acceptor, deep donor, and deep acceptor. The concentration of deep donor and deep acceptor (which responsible for the 0.878 eV PL peak) is relatively low and easily saturated at low excitation. However, the concentration of shallow acceptor is relatively high, remain unsaturated at high excitation and is attributed to the Ba antisite defect."]]}
{"id":"252819643","text":["For example, in single-producer\/single-consumer micro-benchmarks, BBQ yields 11.3x to 42.4x higher throughput than the ringbuffers from Linux kernel, DPDK, Boost, and Folly libraries. "],"abstract":[["Concurrent bounded queues have been widely used for exchanging data and pro\ufb01ling in operating systems, databases, and multithreaded applications. The performance of state-of-the-art queues is limited by the interference between multiple enqueues (enq-enq), multiple dequeues (deq-deq), or enqueues and dequeues (enq-deq), negatively affecting their latency and scalability. Although some existing designs employ optimizations to reduce deq-deq and enq-enq interference, they often neglect the enq-deq case. In fact, such partial optimizations may inadvertently increase interference else-where and result in performance degradation. We present Block-based Bounded Queue ( BBQ ), a novel ringbuffer design that splits the entire buffer into multiple blocks. This eliminates enq-deq interference on concurrency control variables when producers and consumers operate on different blocks. Furthermore, the block-based design is amenable to existing optimizations, e.g. , using the more scalable fetch-and-add instruction. Our evaluation shows that BBQ outperforms several industrial ringbuffers. For example, in single-producer\/single-consumer micro-benchmarks, BBQ yields 11.3x to 42.4x higher throughput than the ringbuffers from Linux kernel, DPDK, Boost, and Folly libraries. In real-world scenarios, BBQ achieves up to 1.5x, 50.5x, and 11.1x performance improvements in benchmarks of DPDK, Linux io_uring, and Disruptor, respectively. We veri\ufb01ed and optimized BBQ on weak memory models with a model-checking-based framework."]]}
{"id":"5537030","text":["We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes. "],"abstract":[["Appositions are adjacent NPs used to add information to a discourse. We propose systems exploiting syntactic and semantic constraints to extract appositions from OntoNotes. Our joint log-linear model outperforms the state-of-the-art Favre and Hakkani-T\u00a8 ur (2009) model by 10% on Broadcast News, and achieves 54.3% Fscore on multiple genres."]]}
{"id":"226975776","text":["Processor design validation and debug is a difficult and complex task, which consumes the lion\u2019s share of the design process. "],"abstract":[["Processor design validation and debug is a difficult and complex task, which consumes the lion\u2019s share of the design process. Design bugs that affect processor performance rather than its functionality are especially difficult to catch, particularly in new microarchitectures. This is because, unlike functional bugs, the correct processor performance of new microarchitectures on complex, long-running benchmarks is typically not deterministically known. Thus, when performance benchmarking new microarchitectures, performance teams may assume that the design is correct when the performance of the new microarchitecture exceeds that of the previous generation, despite significant performance regressions existing in the design. In this work we present a two-stage, machine learning-based methodology that is able to detect the existence of performance bugs in microprocessors. Our results show that our best technique detects 91.5% of microprocessor core performance bugs whose average IPC impact across the studied applications is greater than 1% versus a bug-free design with zero false positives. When evaluated on memory system bugs, our technique achieves 100% detection with zero false positives. Moreover, the detection is automatic, requiring very little performance engineer time."]]}
{"id":"52174144","text":["Learner motivations in each theme were interpreted with the Expectancy-Value-Cost framework. "],"abstract":[["This study proposes a formal multi-step methodology for qualitative assessment of topic modeling results in the context of online learner motivation to purchase Statements of Participation (SoP). We developed Latent Dirichlet Allocation (LDA) based topic models on open-ended responses of three post-course survey questions from 280 open courses offered on the FutureLearn learning platform. For qualitative assessment, we first determined the theme of the topic based on the words that constituted the topic and responses that were most strongly associated with the topic. Then, we verified the theme by comparing the topics assigned by LDA model on a test set with manual annotation. We also performed sentiment analysis to check for alignment with human judgment. Learner motivations in each theme were interpreted with the Expectancy-Value-Cost framework. Our analyses indicated that, primarily, learners were motivated to purchase the SoP based on perceptions of the utility value and financial cost of the certificate. We found that human judgment agreed with the topic model more frequently when LDA topic weights were larger."]]}
{"id":"221516756","text":["A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS."],"abstract":[["Sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. By comprehensively analyzing the literature on graph sampling, we assume that existing algorithms cannot effectively preserve minority structures that are rare and small in a graph but are very important in graph analysis. In this work, we initially conduct a pilot user study to investigate representative minority structures that are most appealing to human viewers. We then perform an experimental study to evaluate the performance of existing graph sampling algorithms regarding minority structure preservation. Results confirm our assumption and suggest key points for designing a new graph sampling approach named mino-centric graph sampling (MCGS). In this approach, a triangle-based algorithm and a cut-point-based algorithm are proposed to efficiently identify minority structures. A set of importance assessment criteria are designed to guide the preservation of important minority structures. Three optimization objectives are introduced into a greedy strategy to balance the preservation between minority and majority structures and suppress the generation of new minority structures. A series of experiments and case studies are conducted to evaluate the effectiveness of the proposed MCGS."]]}
{"id":"222297203","text":["Cloud disk provided to users by cloud providers has been a prevalent form of cloud storage. "],"abstract":[["Cloud disk provided to users by cloud providers has been a prevalent form of cloud storage. Assigning disk storage space for cloud disks among available data warehouses remains a challenging task, as the load characteristics of cloud disks are not known at the time of disk creation. Methods considering only subscribed capacity could be prone to result in resource under-utilization and load imbalance among warehouses. We propose a Smart Cloud Disk Allocation (S-CDA) approach, which uses clustering and classifying to predict the load information for new cloud disks, and then realizes multi-dimensional allocation based on Manhattan distance. Experimental results with realistic cloud workloads show that, compared with the existing one-dimensional allocation scheme, S-CDA increases the overall space\/IOPS\/disk bandwidth utilization, while decreasing the load imbalance."]]}
{"id":"1425977","text":["Prompted by biologists' requests for enhancements, the Beacon Editor includes numerous powerful features for the benefit of creation and presentation."],"abstract":[["The Beacon Editor is a cross-platform desktop application for the creation and modification of signal transduction pathways using the Systems Biology Graphical Notation Activity Flow (SBGN-AF) language. Prompted by biologists' requests for enhancements, the Beacon Editor includes numerous powerful features for the benefit of creation and presentation."]]}
{"id":"245022219","text":["In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively. "],"abstract":[["The burst of applications empowered by massive data have aroused unprecedented privacy concerns in AI society. Currently, data con\ufb01dentiality protection has been one core issue during deep model training. Federated Learning (FL), which enables privacy-preserving training across multiple silos, gained rising popularity for its parameter-only communication. However, previous works have shown that FL revealed a signi\ufb01cant performance drop if the data distributions are heterogeneous among different clients, especially when the clients have cross-domain charac-teristic, such as traf\ufb01c, aerial and in-door. To address this challenging problem, we propose a novel idea, PartialFed , which loads a subset of the global model\u2019s parameters rather than loading the entire model used in most previous works. We \ufb01rst validate our algorithm with manually decided loading strategies inspired by various expert priors, named PartialFed-Fix . Then we develop PartialFed-Adaptive , which automatically selects personalized loading strategy for each client. The superiority of our algorithm is proved by demonstrating the new state-of-the-art results on cross-domain federated classi\ufb01cation and detection. In particular, solely by initializing a small fraction of layers locally, we improve the performance of FedAvg on Of\ufb01ce-Home and UODB by 4.88% and 2.65%, respectively. Further studies show that the adaptive strategy performs signi\ufb01cantly better on domains with large deviation, e.g. improves AP50 by 4.03% and 4.89% on aerial and medical image detection compared to FedAvg."]]}
{"id":"222319298","text":["Secondly, by leveraging the information from the first inference and dropout masks, we predict the zero neurons and skip all their corresponding computations during the following sample inferences. "],"abstract":[["Bayesian Convolutional Neural Networks (BCNNs) have emerged as a robust form of Convolutional Neural Networks (CNNs) with the capability of uncertainty estimation. A BCNN model is implemented by adding a dropout layer after each convolutional layer in the original CNN. By executing the stochastic inferences many times, BCNNs are able to provide an output distribution that reflects the uncertainty of the final prediction. Repeated inferences in this process lead to much longer execution time, which makes it challenging to apply Bayesian technique to CNNs in real-world applications. In this study, we propose Fast-BCNN, an FPGA-based hardware accelerator design that intelligently skips the redundant computations for two types of neurons during repeated BCNN inferences. Firstly, within a sample inference, we aim to skip the dropped neurons that predetermined by dropout masks. Secondly, by leveraging the information from the first inference and dropout masks, we predict the zero neurons and skip all their corresponding computations during the following sample inferences. Particularly, an optimization algorithm is employed to guarantee the accuracy of zero neuron prediction while achieving the maximal computation reduction. To support our neuron skipping strategy at hardware level, we explore an efficient parallelism for CNN convolution to gracefully skip the corresponding computations for both types of neurons, we then propose a novel PE architecture that accommodates the parallel operation of convolution and prediction with negligible overhead. Experimental results demonstrate that our Fast-BCNN achieves 2.1~8.2\u00d7 speedup and 44%~84% energy reduction over the baseline CNN accelerator."]]}
{"id":"16455872","text":["This paper introduces LUDIA, a novel low-rank approximation algorithm that utilizes aggregation constraints in addition to auxiliary information in order to estimate or \"reconstruct\" the original individual-level values from aggregate data. "],"abstract":[["In the past few years, the government and other agencies have publicly released a prodigious amount of data that can be potentially mined to benefit the society at large. However, data such as health records are typically only provided at aggregated levels (e.g. per State, per Hospital Referral Region, etc.) to protect privacy. Unfortunately aggregation can severely diminish the utility of such data when modeling or analysis is desired at a per-individual basis. So, not surprisingly, despite the increasing abundance of aggregate data, there have been very few successful attempts in exploiting them for individual-level analyses. This paper introduces LUDIA, a novel low-rank approximation algorithm that utilizes aggregation constraints in addition to auxiliary information in order to estimate or \"reconstruct\" the original individual-level values from aggregate data. If the reconstructed data are statistically similar to the original individual-level data, off-the-shelf individual-level models can be readily and reliably applied for subsequent predictive or descriptive analytics. LUDIA is more robust to nonlinear estimates and random effects than other reconstruction algorithms. It solves a Sylvester equation and leverages multi-level (also known as hierarchical or mixed-effect) modeling approaches efficiently. A novel graphical model is also introduced to provide a probabilistic viewpoint of LUDIA. Experimental results using a Texas inpatient dataset show that individual-level data can be reasonably reconstructed from county-, hospital-, and zip code-level aggregate data. Several factors affecting the reconstruction quality are discussed, along with the implications of this work for current aggregation guidelines."]]}
{"id":"220364743","text":["Cautioning against \u201cparticipation washing\u201d, it argues that the notion of \u201cparticipation\u201d should be expanded to acknowledge more subtle, and possibly exploitative, forms of community involvement in participatory machine learning design. "],"abstract":[["This paper critiques popular modes of participation in design practice and machine learning. It examines three existing kinds of participation in design practice and machine learning participation as work, participation as consultation, and as participation as justice \u2013 to argue that the machine learning community must become attuned to possibly exploitative and extractive forms of community involvement and shift away from the prerogatives of context independent scalability. Cautioning against \u201cparticipation washing\u201d, it argues that the notion of \u201cparticipation\u201d should be expanded to acknowledge more subtle, and possibly exploitative, forms of community involvement in participatory machine learning design. Specifically, it suggests that it is imperative to recognize design participation as work; to ensure that participation as consultation is context-specific; and that participation as justice must be genuine and long term. The paper argues that such a development can only be scaffolded by a new epistemology around design harms, including, but not limited to, in machine learning. To facilitate such a development, the paper suggests developing we argue that developing a cross-sectoral database of design participation failures that is cross-referenced with socio-structural dimensions and highlights \u201cedge cases\u201d that can and must be learned from."]]}
{"id":"4167933","text":["Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs."],"abstract":[["Large CNNs have delivered impressive performance in various computer vision applications. But the storage and computation requirements make it problematic for deploying these models on mobile devices. Recently, tensor decompositions have been used for speeding up CNNs. In this paper, we further develop the tensor decomposition technique. We propose a new algorithm for computing the low-rank tensor decomposition for removing the redundancy in the convolution kernels. The algorithm finds the exact global optimizer of the decomposition and is more effective than iterative methods. Based on the decomposition, we further propose a new method for training low-rank constrained CNNs from scratch. Interestingly, while achieving a significant speedup, sometimes the low-rank constrained CNNs delivers significantly better performance than their non-constrained counterparts. On the CIFAR-10 dataset, the proposed low-rank NIN model achieves $91.31\\%$ accuracy (without data augmentation), which also improves upon state-of-the-art result. We evaluated the proposed method on CIFAR-10 and ILSVRC12 datasets for a variety of modern CNNs, including AlexNet, NIN, VGG and GoogleNet with success. For example, the forward time of VGG-16 is reduced by half while the performance is still comparable. Empirical success suggests that low-rank tensor decompositions can be a very useful tool for speeding up large CNNs."]]}
{"id":"248239751","text":["Different from traditional contrastive learning methods which generate two graph views by uniform data augmentation schemes such as corruption or dropping, we comprehensively consider three different graph views for KG-aware recommendation, including global-level structural view, local-level collaborative and semantic views. "],"abstract":[["Knowledge graph (KG) plays an increasingly important role in recommender systems. Recently, graph neural networks (GNNs) based model has gradually become the theme of knowledge-aware recommendation (KGR). However, there is a natural deficiency for GNN-based KGR models, that is, the sparse supervised signal problem, which may make their actual performance drop to some extent. Inspired by the recent success of contrastive learning in mining supervised signals from data itself, in this paper, we focus on exploring the contrastive learning in KG-aware recommendation and propose a novel multi-level cross-view contrastive learning mechanism, named MCCLK. Different from traditional contrastive learning methods which generate two graph views by uniform data augmentation schemes such as corruption or dropping, we comprehensively consider three different graph views for KG-aware recommendation, including global-level structural view, local-level collaborative and semantic views. Specifically, we consider the user-item graph as a collaborative view, the item-entity graph as a semantic view, and the user-item-entity graph as a structural view. MCCLK hence performs contrastive learning across three views on both local and global levels, mining comprehensive graph feature and structure information in a self-supervised manner. Besides, in semantic view, a k-Nearest-Neighbor (k NN) item-item semantic graph construction module is proposed, to capture the important item-item semantic relation which is usually ignored by previous work. Extensive experiments conducted on three benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. The implementations are available at: https:\/\/github.com\/CCIIPLab\/MCCLK."]]}
{"id":"231684321","text":["Concretely, our algorithm takes input an $\\epsilon$-corrupted sample from a $k$-GMM and outputs an approximate clustering that misclassifies at most $k^{O(k)}(\\epsilon+\\eta)$ fraction of the points whenever every pair of mixture components are separated by $1-\\exp(-poly(k\/\\eta))$ in total variation distance. "],"abstract":[["We give the first outlier-robust efficient algorithm for clustering a mixture of $k$ statistically separated $d$ - dimensional Gaussians ($k$-GMMs). Concretely, our algorithm takes input an $\\epsilon$-corrupted sample from a $k$-GMM and outputs an approximate clustering that misclassifies at most $k^{O(k)}(\\epsilon+\\eta)$ fraction of the points whenever every pair of mixture components are separated by $1-\\exp(-poly(k\/\\eta))$ in total variation distance. This is the statistically weakest possible notion of separation and allows, for e.g., clustering of mixtures with components with the same mean with covariances differing in a single unknown direction or separated in Frobenius distance. The running time of our algorithm is $d^{poly(k\/\\eta)}$. Such results were not known prior to our work, even for $k=2$. More generally, our algorithms succeed for mixtures of any distribution that satisfies two well-studied analytic assumptions - sum-of-squares certifiable hypercontractivity and anti-concentration. As an immediate corollary, they extend to clustering mixtures of arbitrary affine transforms of the uniform distribution on the $d$-dimensional unit sphere. Even the information theoretic clusterability of separated distributions satisfying our analytic assumptions was not known and is likely to be of independent interest. Our algorithms build on the recent flurry of work relying on certifiable anti-concentration first introduced in [1], [2]. Our techniques expand the sum-of-squares toolkit to show robust certifiability of TV-separated Gaussian clusters in data. This involves giving a low-degree sum-of-squares proof of statements that relate parameter (i.e. mean and covariances) distance to total variation distance by relying only on hypercontractivity and anti-concentration."]]}
{"id":"255188131","text":["This solves a conjecture of Gupta in degree 3 and generalizes the result from Shpilka, who proved that quadratic radical Sylvester-Gallai configurations are constant dimensional. "],"abstract":[["We prove that any cubic radical Sylvester-Gallai configuration is constant dimensional. This solves a conjecture of Gupta in degree 3 and generalizes the result from Shpilka, who proved that quadratic radical Sylvester-Gallai configurations are constant dimensional. To prove our Sylvester-Gallai theorem, we develop several new tools combining techniques from algebraic geometry and elimination theory. Among our technical contributions, we prove a structure theorem characterizing non-radical ideals generated by two cubic forms, generalizing previous structure theorems for intersections of two quadrics. Moreover, building upon the groundbreaking work Ananyan and Hochster, we introduce the notion of wide Ananyan-Hochster algebras and show that these algebras allow us to transfer the local conditions of Sylvester-Gallai configurations into global conditions."]]}
{"id":"7810571","text":["In this paper we apply a first-order perturbation analysis of the SVD to derive analytic expressions of the signal to interference plus noise ratio (SINR) for each subchannel of each UT using BD in presence of imperfect CSI. "],"abstract":[["Block diagonalization (BD) is a low-complexity linear precoding technique for multi-user MIMO (MU-MIMO) downlink systems, which can provide a performance that is close to theMU-MIMO capacity. However, imperfect channel state information (CSI) will result in a degraded performance of the BD scheme. Thus, studying the performance of BD under imperfect CSI is crucial for a practical system design since the robustness of BD to real-world imperfections should be verified. In this paper we apply a first-order perturbation analysis of the SVD to derive analytic expressions of the signal to interference plus noise ratio (SINR) for each subchannel of each UT using BD in presence of imperfect CSI. To demonstrate the usefulness of these expressions, a robust BD technique via worst SINR maximization is developed. Numerical simulations show the accuracy and the usefulness of the derived analytical results."]]}
{"id":"220647038","text":["However, questionnaire based psychometric tests have some known shortcomings. "],"abstract":[["A number of questionnaire based psychometric testing frameworks are globally for example OCEAN (Five factor) indicator, MBTI (Myers Brigg Type Indicator) etc. However, questionnaire based psychometric tests have some known shortcomings. This work explores whether these shortcomings can be mitigated through computer-based gaming platforms for evaluating psychometric parameters. A computer based psychometric game framework called Antarjami has been developed for evaluating OCEAN (Five factor) indicators. It investigates the feasibility of extracting psychometric parameters through computer-based games, utilizing underlying improvements in the area of modern artificial intelligence. The candidates for the test are subjected to a number scenarios as part of the computer based game and their reactions\/responses are used to evaluate their psychometric parameters. As part of the study, the parameters obtained from the game were compared with those evaluated using paper based tests and scores given by a panel of psychologists. The achieved results were very promising."]]}
{"id":"249192395","text":["The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. "],"abstract":[["We propose a differentiable rendering algorithm for efficient novel view synthesis. By departing from volume-based representations in favor of a learned point representation, we improve on existing methods more than an order of magnitude in memory and runtime, both in training and inference. The method begins with a uniformly-sampled random point cloud and learns per-point position and view-dependent appearance, using a differentiable splat-based renderer to train the model to reproduce a set of input training images with the given pose. Our method is up to 300 \u00d7 faster than NeRF in both training and inference, with only a marginal sacrifice in quality, while using less than 10 MB of memory for a static scene. For dynamic scenes, our method trains two orders of magnitude faster than STNeRF and renders at a near interactive rate, while maintaining high image quality and temporal coherence even without imposing any temporal-coherency regularizers."]]}
{"id":"78094946","text":["White space separators and\/or spatially grouped color coding led to significantly stronger understanding of the underlying topics compared to a standard Wordle layout, while simultaneously scoring higher on measures of aesthetic appeal. "],"abstract":[["Word clouds continue to be a popular tool for summarizing textual information, despite their well-documented deficiencies for analytic tasks. Much of their popularity rests on their playful visual appeal. In this paper, we present the results of a series of controlled experiments that show that layouts in which words are arranged into semantically and visually distinct zones are more effective for understanding the underlying topics than standard word cloud layouts. White space separators and\/or spatially grouped color coding led to significantly stronger understanding of the underlying topics compared to a standard Wordle layout, while simultaneously scoring higher on measures of aesthetic appeal. This work is an advance on prior research on semantic layouts for word clouds because that prior work has either not ensured that the different semantic groupings are visually or semantically distinct, or has not performed usability studies. An additional contribution of this work is the development of a dataset for a semantic category identification task that can be used for replication of these results or future evaluations of word cloud designs."]]}
{"id":"202583773","text":["Since booters are a serious threat to Internet operations and can cause significant financial and reputational damage, they also draw the attention of law enforcement agencies and related counter activities. "],"abstract":[["Booter services continue to provide popular DDoS-as-a-service platforms and enable anyone irrespective of their technical ability, to execute DDoS attacks with devastating impact. Since booters are a serious threat to Internet operations and can cause significant financial and reputational damage, they also draw the attention of law enforcement agencies and related counter activities. In this paper, we investigate booter-based DDoS attacks in the wild and the impact of an FBI takedown targeting 15 booter websites in December 2018 from the perspective of a major IXP and two ISPs. We study and compare attack properties of multiple booter services by launching Gbps-level attacks against our own infrastructure. To understand spatial and temporal trends of the DDoS traffic originating from booters we scrutinize 5 months, worth of inter-domain traffic. We observe that the takedown only leads to a temporary reduction in attack traffic. Additionally, one booter was found to quickly continue operation by using a new domain for its website."]]}
{"id":"236961648","text":["mmTag addresses the key challenges that prevent existing backscatter networks from operating at mmWave bands. "],"abstract":[["Recent advances in IoT, machine learning and cloud computing have placed a huge strain on wireless networks. In particular, many emerging applications require streaming rich content (such as videos) in real time, while they are constrained by energy sources. A wireless network which supports high data-rate while consuming low-power would be very attractive for these applications. Unfortunately, existing wireless networks do not satisfy this requirement. For example, WiFi backscatter and Bluetooth networks have very low power consumption, but their data-rate is very limited (less than a Mbps). On the other hand, modern WiFi and mmWave networks support high throughput, but have a high power consumption (more than a watt). To address this problem, we present mmTag, a novel mmWave backscatter network which enables low-power high-throughput wireless links for emerging applications. mmTag is a backscatter system which operates in the mmWave frequency bands. mmTag addresses the key challenges that prevent existing backscatter networks from operating at mmWave bands. We implemented mmTag and evaluated its performance empirically. Our results show that mmTag is capable of achieving 1 Gbps and 100 Mbps at 4.6 m and 8 m, respectively, while consuming only 2.4 nJ\/bit."]]}
{"id":"102347327","text":["With frequent churn, systems quickly become impractically difficult to penetrate. "],"abstract":[["Attacks often succeed by abusing the gap between program and machine-level semantics-- for example, by locating a sensitive pointer, exploiting a bug to overwrite this sensitive data, and hijacking the victim program's execution. In this work, we take secure system design on the offensive by continuously obfuscating information that attackers need but normal programs do not use, such as representation of code and pointers or the exact location of code and data. Our secure hardware architecture, Morpheus, combines two powerful protections: ensembles of moving target defenses and churn. Ensembles of moving target defenses randomize key program values (e.g., relocating pointers and encrypting code and pointers) which forces attackers to extensively probe the system prior to an attack. To ensure attack probes fail, the architecture incorporates churn to transparently re-randomize program values underneath the running system. With frequent churn, systems quickly become impractically difficult to penetrate. We demonstrate Morpheus through a RISC-V-based prototype designed to stop control-flow attacks. Each moving target defense in Morpheus uses hardware support to individually offer more randomness at a lower cost than previous techniques. When ensembled with churn, Morpheus defenses offer strong protection against control-flow attacks, with our security testing and performance studies revealing: i) high-coverage protection for a broad array of control-flow attacks, including protections for advanced attacks and an attack disclosed after the design of Morpheus, and ii) negligible performance impacts (1%) with churn periods up to 50 ms, which our study estimates to be at least 5000x faster than the time necessary to possibly penetrate Morpheus."]]}
{"id":"235436386","text":["As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. "],"abstract":[["As machine learning (ML) increasingly affects people and society, awareness of its potential unwanted consequences has also grown. To anticipate, prevent, and mitigate undesirable downstream consequences, it is critical that we understand when and how harm might be introduced throughout the ML life cycle. In this paper, we provide a framework that identifies seven distinct potential sources of downstream harm in machine learning, spanning data collection, development, and deployment. In doing so, we aim to facilitate more productive and precise communication around these issues, as well as more direct, application-grounded ways to mitigate them."]]}
{"id":"195353511","text":["Based on the phenomenon of entrainment, multi-feature adaptation on tone and loudness has been found in human-human interactions to be highly correlated to learning and social engagement. "],"abstract":[["Teachable agents are pedagogical agents that employ the \u2018learning-by-teaching\u2019 strategy, which facilitates learning by encouraging students to construct explanations, reflect on misconceptions, and elaborate on what they know. Teachable agents present unique opportunities to maximize the benefits of a \u2018learning-by-teaching\u2019 experience. For example, teachable agents can provide socio-emotional support to learners, influencing learner self-efficacy and motivation, and increasing learning. Prior work has found that a teachable agent which engages learners socially through social dialogue and paraverbal adaptation on pitch can have positive effects on rapport and learning. In this work, we introduce Emma, a teachable robotic agent that can speak socially and adapt on both pitch and loudness. Based on the phenomenon of entrainment, multi-feature adaptation on tone and loudness has been found in human-human interactions to be highly correlated to learning and social engagement. In a study with 48 middle school participants, we performed a novel exploration of how multi-feature adaptation can influence learner rapport and learning as an independent social behavior and combined with social dialogue. We found significantly more rapport for Emma when the robot both adapted and spoke socially than when Emma only adapted and indications of a similar trend for learning. Additionally, it appears that an individual\u2019s initial comfort level with robots may influence how they respond to such behavior, suggesting that for individuals who are more comfortable interacting with robots, social behavior may have a more positive influence."]]}
{"id":"206594509","text":["(i) the image locations of the human joints are provided and "],"abstract":[["This paper addresses the challenge of 3D full-body human pose estimation from a monocular image sequence. Here, two cases are considered: (i) the image locations of the human joints are provided and (ii) the image locations of joints are unknown. In the former case, a novel approach is introduced that integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the latter case, the former case is extended by treating the image locations of the joints as latent variables to take into account considerable uncertainties in 2D joint locations. A deep fully convolutional network is trained to predict the uncertainty maps of the 2D joint locations. The 3D pose estimates are realized via an Expectation-Maximization algorithm over the entire sequence, where it is shown that the 2D joint location uncertainties can be conveniently marginalized out during inference. Empirical evaluation on the Human3.6M dataset shows that the proposed approaches achieve greater 3D pose estimation accuracy over state-of-the-art baselines. Further, the proposed approach outperforms a publicly available 2D pose estimation baseline on the challenging PennAction dataset."]]}
{"id":"201717867","text":["Using our approach to Japanese textual data, we have implemented a prototype system that can display causal chains for user-entered words. "],"abstract":[["In this research, we extract causal information from textual data and construct a causality database in the economic field. We develop a method to produce causal chains starting from phrases representing specific events. The proposed method can offer possible ripple effects and factors of particular events or situations. Using our approach to Japanese textual data, we have implemented a prototype system that can display causal chains for user-entered words. A user can interactively edit the causal chains by selecting appropriate causalities and deleting inappropriate causalities. The economic causal-chain search algorithm can be applied to various financial information services."]]}
{"id":"7112188","text":["Using non-overlapping hardware allows simultaneous execution, and up to 2.58-fold performance gain, when compared with any other algorithm to search sequence databases. "],"abstract":[["The Smith-Waterman algorithm has a great sensitivity when used for biological sequence-database searches, but at the expense of high computing-power requirements. To overcome this problem, there are implementations in literature that exploit the different hardware-architectures available in a standard PC, such as GPU, CPU, and coprocessors. We introduce an application that splits the original database-search problem into smaller parts, resolves each of them by executing the most efficient implementations of the Smith-Waterman algorithms in different hardware architectures, and finally unifies the generated results. Using non-overlapping hardware allows simultaneous execution, and up to 2.58-fold performance gain, when compared with any other algorithm to search sequence databases. Even the performance of the popular BLAST heuristic is exceeded in 78% of the tests. The application has been tested with standard hardware: Intel i7-4820K CPU, Intel Xeon Phi 31S1P coprocessors, and nVidia GeForce GTX 960 graphics cards. An important increase in performance has been obtained in a wide range of situations, effectively exploiting the available hardware."]]}
{"id":"234478736","text":["The users are guided to move in the direction of the resultant force vector of density force, gravitational force and APF force. "],"abstract":[["With more attention being paid to large scale virtual environments, the demand for more users to collaborate in the same physical space is growing rapidly. Due to the complex collision problem caused by the limitation of physical space, the multi-user redirected walking methods are devoted to improving the ability of multi-user navigation in large-scale virtual environments by reducing the disturbance of resets. Because the existing multi-user redirected walking methods do not consider the density of users in the physical space, there are more boundary conflicts in the real walking for the multi-user virtual environment. In order to decrease the boundary conflicts, this paper presents a novel method of dynamic density-based redirected walking towards multi-user virtual environments. This method dynamically adjusts the user distribution to a state with high center density and low boundary density through the density force, which is generated by the density difference between standard density and actual density. In our method, the users in high-density areas are guided by a repulsive force away from the central area while the users in low-density areas are guided by the gravitational forces towards the central area. Our method can select a double-density optimal gravitational point as the turning target, so all users can move to the area of minimum density to make better use of the whole physical space. Our method also adopts the artificial potential field (APF) forces to prevent user collisions caused by usergathering. The users are guided to move in the direction of the resultant force vector of density force, gravitational force and APF force. In addition, this paper introduces a matching resetting method to further adjust the density distribution while dealing with user conflicts. The results of experiments show that our method successfully reduces the potential conflicts about 30% on average compared with the existing reactive multi-user redirection algorithms. Especially as the number of users increases, our method can avoid more boundary conflicts by using the adjustment of density."]]}
{"id":"220872177","text":["We design new vectoring protocols that compute on partial orders, with every node capable of electing multiple attributes per destination rather than a single attribute as in standard vectoring protocols. "],"abstract":[["Standard vectoring protocols, such as EIGRP, BGP, DSDV, or Babel, only route on optimal paths when the total order on path attributes that substantiates optimality is consistent with the extension operation that calculates path attributes from link attributes, leaving out many optimality criteria of practical interest. We present a solution to this problem and, more generally, to the problem of routing on multiple optimality criteria. A key idea is the derivation of a partial order on path attributes that is consistent with the extension operation and respects every optimality criterion of a designated collection of such criteria. We design new vectoring protocols that compute on partial orders, with every node capable of electing multiple attributes per destination rather than a single attribute as in standard vectoring protocols. Our evaluation over publicly available network topologies and attributes shows that the proposed protocols converge fast and enable optimal path routing concurrently for many optimality criteria with only a few elected attributes at each node per destination. We further show how predicating computations on partial orders allows incorporation of service chain constraints on optimal path routing."]]}
{"id":"212556126","text":["Also, three dementia-related symptoms: delusions, agitation\/aggression, and euphoria\/exaltation, show a statistically significant decrease after the intervention. "],"abstract":[["Several studies have been reported on the use of social robots for dementia care. These robots have been used for diverse tasks such as for companionship, as an exercise coach, and as daily life assistant. However, most of these studies have assessed impact on participants only at the time when the interaction takes place rather than their medium or long-term effects. In this work, we report on a nine-week study conducted in a nursing home in which a autonomous social robot, called Eva, acts as facilitator of a cognitive stimulation therapy (CST). During the study, eight persons with dementia interacted with the robot in a group session which included elements of music therapy, reminiscence, cognitive games, and relaxation. Using the Neuropsychiatric Inventory - Nursing Home version (NPI-NH), we analyzed the impact of the therapy guided by the robot. The results show a statistically significant decrease in the total score of NPI-NH. Also, three dementia-related symptoms: delusions, agitation\/aggression, and euphoria\/exaltation, show a statistically significant decrease after the intervention. In addition, a qualitative analysis on interviews conducted with caregivers shows that all participants exhibits positive short-term effects after the session and provides insights on why some changes in behavior prevailed beyond the therapy sessions. Our results provide evidence that a social robot could play a role in improving the quality of life of persons with dementia. CCS CONCEPTS \u2022 Computer systems organization \u2192 Robotics; \u2022 Social and professional topics \u2192 Medical technologies; People with disabilities; \u2022 Human-centered computing \u2192 Interactive systems and tools; User studies. ACM Reference Format: Dagoberto Cruz-Sandoval, Arturo Morales-Tellez, Eduardo Benitez Sandoval, and Jesus Favela. 2020. A Social Robot as Therapy Facilitator in Interventions to Deal with Dementia-related Behavioral Symptoms. In Proceedings of the 2020 ACM\/IEEE International Conference on Human-Robot Interaction (HRI \u201920), March 23\u201326, 2020, Cambridge, United Kingdom. ACM, New York, NY, USA, 9 pages. https:\/\/doi.org\/10.1145\/3319502.3374840"]]}
{"id":"234767630","text":["Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. "],"abstract":[["Watermarking has become the tendency in protecting the intellectual property of DNN models. Recent works, from the adversary's perspective, attempted to subvert watermarking mechanisms by designing watermark removal attacks. However, these attacks mainly adopted sophisticated fine-tuning techniques, which have certain fatal drawbacks or unrealistic assumptions. In this paper, we propose a novel watermark removal attack from a different perspective. Instead of just fine-tuning the watermarked models, we design a simple yet powerful transformation algorithm by combining imperceptible pattern embedding and spatial-level transformations, which can effectively and blindly destroy the memorization of watermarked models to the watermark samples. We also introduce a lightweight fine-tuning strategy to preserve the model performance. Our solution requires much less resource or knowledge about the watermarking scheme than prior works. Extensive experimental results indicate that our attack can bypass state-of-the-art watermarking solutions with very high success rates. Based on our attack, we propose watermark augmentation techniques to enhance the robustness of existing watermarks."]]}
{"id":"15659468","text":["Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. "],"abstract":[["Partition functions arise in a variety of settings, including conditional random fields, logistic regression, and latent gaussian models. In this paper, we consider semistochastic quadratic bound (SQB) methods for maximum likelihood inference based on partition function optimization. Batch methods based on the quadratic bound were recently proposed for this class of problems, and performed favorably in comparison to state-of-the-art techniques. Semistochastic methods fall in between batch algorithms, which use all the data, and stochastic gradient type methods, which use small random selections at each iteration. We build semistochastic quadratic bound-based methods, and prove both global convergence (to a stationary point) under very weak assumptions, and linear convergence rate under stronger assumptions on the objective. To make the proposed methods faster and more stable, we consider inexact subproblem minimization and batch-size selection schemes. The efficacy of SQB methods is demonstrated via comparison with several state-of-the-art techniques on commonly used datasets."]]}
{"id":"6957775","text":["Our goal is to design a persistent memory system with performance very close to that of a native system. "],"abstract":[["Persistent memory is an emerging technology which allows in-memory persistent data objects to be updated at much higher throughput than when using disks as persistent storage. Previous persistent memory designs use logging or copy-on-write mechanisms to update persistent data, which unfortunately reduces the system performance to roughly half that of a native system with no persistence support. One of the great challenges in this application class is therefore how to efficiently enable atomic, consistent, and durable updates to ensure data persistence that survives application and\/or system failures. Our goal is to design a persistent memory system with performance very close to that of a native system. We propose Kiln, a persistent memory design that adopts a nonvolatile cache and a nonvolatile main memory to enable atomic in-place updates without logging or copy-on-write. Our evaluation shows that Kiln can achieve 2\u00d7 performance improvement compared with NVRAM-based persistent memory with write-ahead logging. In addition, our design has numerous practical advantages: a simple and intuitive abstract interface, microarchitecture-level optimizations, fast recovery from failures, and eliminating redundant writes to nonvolatile storage media."]]}
