user	instance_id	displayed_text	textbox_input:::text_box	span_annotation:::Values	span_annotation:::Research Values
1	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	"Universality
Generalization
Efficiency
Rich annotations
Effectiveness
"	[]	[{'start': 195, 'end': 255, 'span': 'desire a single machine that can handle diverse new problems', 'annotation': 'Research Values'}, {'start': 340, 'end': 356, 'span': 'a unified solver', 'annotation': 'Research Values'}, {'start': 538, 'end': 611, 'span': 'generalize across domains with only a handful of domain-specific examples', 'annotation': 'Research Values'}, {'start': 707, 'end': 768, 'span': 'transforming is unnecessary if we can obtain rich annotations', 'annotation': 'Research Values'}, {'start': 860, 'end': 884, 'span': 'insufficient annotations', 'annotation': 'Research Values'}, {'start': 886, 'end': 899, 'span': 'Universal NLP', 'annotation': 'Research Values'}, {'start': 980, 'end': 989, 'span': 'Universal', 'annotation': 'Research Values'}, {'start': 1108, 'end': 1165, 'span': 'work well on new entailment domains in a few-shot setting', 'annotation': 'Research Values'}, {'start': 1171, 'end': 1193, 'span': 'show its effectiveness', 'annotation': 'Research Values'}, {'start': 1197, 'end': 1213, 'span': 'a unified solver', 'annotation': 'Research Values'}]
1	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	"Performance
Reproducibility
Understanding phenonemon
"	[]	[{'start': 379, 'end': 410, 'span': 'the highest-scoring translation', 'annotation': 'Research Values'}, {'start': 471, 'end': 481, 'span': 'inadequacy', 'annotation': 'Research Values'}, {'start': 617, 'end': 626, 'span': 'reproduce', 'annotation': 'Research Values'}, {'start': 1028, 'end': 1131, 'span': 'advocate for the use of decision rules that take into account the translation distribution holistically', 'annotation': 'Research Values'}, {'start': 1200, 'end': 1219, 'span': 'competitive results', 'annotation': 'Research Values'}, {'start': 1220, 'end': 1310, 'span': 'confirming that NMT models do capture important aspects of translation well in expectation', 'annotation': 'Research Values'}]
1	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	"performance
universality
quantitative evidence"	[]	[{'start': 136, 'end': 146, 'span': 'end-to-end', 'annotation': 'Research Values'}, {'start': 161, 'end': 177, 'span': 'jointly performs', 'annotation': 'Research Values'}, {'start': 218, 'end': 266, 'span': 'outperforms the previously reported best results', 'annotation': 'Research Values'}, {'start': 419, 'end': 456, 'span': 'alleviating the fundamental trade-off', 'annotation': 'Research Values'}, {'start': 560, 'end': 597, 'span': 'achieves state-of-the-art performance', 'annotation': 'Research Values'}, {'start': 717, 'end': 755, 'span': 'provides further support for the claim', 'annotation': 'Research Values'}]
1	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	"efficiency
speed
generalization
nov
performance"	[]	[{'start': 271, 'end': 317, 'span': 'suffer from the inefficiency of data synthesis', 'annotation': 'Research Values'}, {'start': 330, 'end': 356, 'span': 'data-free training process', 'annotation': 'Research Values'}, {'start': 367, 'end': 381, 'span': 'time-consuming', 'annotation': 'Research Values'}, {'start': 391, 'end': 425, 'span': 'inapplicable for large-scale tasks', 'annotation': 'Research Values'}, {'start': 457, 'end': 468, 'span': 'efficacious', 'annotation': 'Research Values'}, {'start': 515, 'end': 565, 'span': 'accelerate DFKD by a factor of orders of magnitude', 'annotation': 'Research Values'}, {'start': 601, 'end': 606, 'span': 'novel', 'annotation': 'Research Values'}, {'start': 619, 'end': 651, 'span': 'reuse the shared common features', 'annotation': 'Research Values'}, {'start': 741, 'end': 777, 'span': 'optimize a set of data independently', 'annotation': 'Research Values'}, {'start': 823, 'end': 894, 'span': 'seeks common features as the initialization for the fast data synthesis', 'annotation': 'Research Values'}, {'start': 942, 'end': 965, 'span': 'within only a few steps', 'annotation': 'Research Values'}, {'start': 967, 'end': 1027, 'span': 'significantly enhancing the efficiency of data-free training', 'annotation': 'Research Values'}, {'start': 1122, 'end': 1152, 'span': '0x and even 100x acceleration ', 'annotation': 'Research Values'}, {'start': 1158, 'end': 1210, 'span': 'preserving performances on par with state of the art', 'annotation': 'Research Values'}]
1	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	"understand social phenomenon
discover phinonenon
real-world impact
policy recommendations"	[]	[{'start': 505, 'end': 544, 'span': 'polarized and siloed into echo chambers', 'annotation': 'Research Values'}, {'start': 571, 'end': 610, 'span': 'understand the nature of this discourse', 'annotation': 'Research Values'}, {'start': 681, 'end': 755, 'span': 'healthcare decisions may affect their communities and the country at large', 'annotation': 'Research Values'}, {'start': 1037, 'end': 1089, 'span': 'examine the state of polarization around vaccination', 'annotation': 'Research Values'}, {'start': 1228, 'end': 1247, 'span': 'manually illustrate', 'annotation': 'Research Values'}, {'start': 1400, 'end': 1423, 'span': 'find the stark division', 'annotation': 'Research Values'}, {'start': 1710, 'end': 1739, 'span': 'discover a series of concerns', 'annotation': 'Research Values'}, {'start': 1905, 'end': 1953, 'span': 'recommend an ongoing surveillance of this debate', 'annotation': 'Research Values'}]
132	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	N/A	[]	[]
132	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	N/A	[]	[]
132	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	N/A	[]	[]
132	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	N/A	[]	[]
132	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	N/A	[]	[]
62b20662ca9e4e8378d8d7a3	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	limited, annotations, unified, generalisation	[]	[{'start': 36, 'end': 40, 'span': 'NLP ', 'annotation': 'Research Values'}, {'start': 61, 'end': 76, 'span': 'constructing a ', 'annotation': 'Research Values'}, {'start': 76, 'end': 100, 'span': 'problem-specific dataset', 'annotation': 'Research Values'}, {'start': 107, 'end': 143, 'span': 'building a model to fit this dataset', 'annotation': 'Research Values'}, {'start': 202, 'end': 255, 'span': 'a single machine that can handle diverse new problems', 'annotation': 'Research Values'}, {'start': 267, 'end': 304, 'span': 'task-specific annotations are limited', 'annotation': 'Research Values'}, {'start': 306, 'end': 356, 'span': 'We bring up textual entailment as a unified solver', 'annotation': 'Research Values'}, {'start': 707, 'end': 782, 'span': 'transforming is unnecessary if we can obtain rich annotations for this task', 'annotation': 'Research Values'}, {'start': 1067, 'end': 1075, 'span': 'enables ', 'annotation': 'Research Values'}, {'start': 1180, 'end': 1194, 'span': 'effectiveness ', 'annotation': 'Research Values'}, {'start': 1199, 'end': 1342, 'span': 'unified solver for several downstream NLP tasks such as question answering and coreference resolution when the end-task annotations are limited', 'annotation': 'Research Values'}]
62b20662ca9e4e8378d8d7a3	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	NMT, decision rule, MAP decoding, MLE	[]	[{'start': 56, 'end': 88, 'span': 'neural machine translation (NMT)', 'annotation': 'Research Values'}, {'start': 185, 'end': 188, 'span': 'NMT', 'annotation': 'Research Values'}, {'start': 194, 'end': 200, 'span': 'model ', 'annotation': 'Research Values'}, {'start': 207, 'end': 225, 'span': 'training algorithm', 'annotation': 'Research Values'}, {'start': 227, 'end': 262, 'span': 'maximum likelihood estimation (MLE)', 'annotation': 'Research Values'}, {'start': 305, 'end': 340, 'span': 'maximum a posteriori (MAP) decoding', 'annotation': 'Research Values'}, {'start': 344, 'end': 357, 'span': 'decision rule', 'annotation': 'Research Values'}, {'start': 383, 'end': 410, 'span': 'highest-scoring translation', 'annotation': 'Research Values'}, {'start': 421, 'end': 425, 'span': 'mode', 'annotation': 'Research Values'}, {'start': 471, 'end': 481, 'span': 'inadequacy', 'annotation': 'Research Values'}, {'start': 527, 'end': 533, 'span': 'model ', 'annotation': 'Research Values'}, {'start': 541, 'end': 559, 'span': 'training algorithm', 'annotation': 'Research Values'}, {'start': 617, 'end': 627, 'span': 'reproduce ', 'annotation': 'Research Values'}, {'start': 673, 'end': 684, 'span': 'beam search', 'annotation': 'Research Values'}, {'start': 771, 'end': 775, 'span': 'NMT ', 'annotation': 'Research Values'}, {'start': 786, 'end': 798, 'span': 'MAP decoding', 'annotation': 'Research Values'}, {'start': 810, 'end': 816, 'span': 'NMT’s ', 'annotation': 'Research Values'}, {'start': 844, 'end': 847, 'span': 'MLE', 'annotation': 'Research Values'}, {'start': 943, 'end': 959, 'span': 'probability mass', 'annotation': 'Research Values'}, {'start': 1004, 'end': 1013, 'span': 'arbitrary', 'annotation': 'Research Values'}, {'start': 1052, 'end': 1066, 'span': 'decision rules', 'annotation': 'Research Values'}, {'start': 1094, 'end': 1118, 'span': 'translation distribution', 'annotation': 'Research Values'}, {'start': 1119, 'end': 1131, 'span': 'holistically', 'annotation': 'Research Values'}, {'start': 1166, 'end': 1193, 'span': 'minimum Bayes risk decoding', 'annotation': 'Research Values'}, {'start': 1200, 'end': 1212, 'span': 'competitive ', 'annotation': 'Research Values'}, {'start': 1236, 'end': 1240, 'span': 'NMT ', 'annotation': 'Research Values'}, {'start': 1249, 'end': 1267, 'span': ' capture important', 'annotation': 'Research Values'}, {'start': 1279, 'end': 1291, 'span': 'translation ', 'annotation': 'Research Values'}]
62b20662ca9e4e8378d8d7a3	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	parsing, TAG, transition, graph, sota, analysis	[]	[{'start': 218, 'end': 302, 'span': 'outperforms the previously reported best results by more than 2.2 LAS and UAS points', 'annotation': 'Research Values'}, {'start': 419, 'end': 513, 'span': 'alleviating the fundamental trade-off between transition-based and graph-based parsing systems', 'annotation': 'Research Values'}, {'start': 560, 'end': 710, 'span': 'achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery', 'annotation': 'Research Values'}, {'start': 761, 'end': 850, 'span': 'TAG is a viable formalism for problems that require rich structural analysis of sentences', 'annotation': 'Research Values'}]
62b20662ca9e4e8378d8d7a3	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	DFKD, scalability, acceleration, synthesis	[]	[{'start': 205, 'end': 217, 'span': 'encouraging ', 'annotation': 'Research Values'}, {'start': 287, 'end': 300, 'span': 'inefficiency ', 'annotation': 'Research Values'}, {'start': 303, 'end': 317, 'span': 'data synthesis', 'annotation': 'Research Values'}, {'start': 367, 'end': 381, 'span': 'time-consuming', 'annotation': 'Research Values'}, {'start': 391, 'end': 403, 'span': 'inapplicable', 'annotation': 'Research Values'}, {'start': 408, 'end': 425, 'span': 'large-scale tasks', 'annotation': 'Research Values'}, {'start': 457, 'end': 469, 'span': 'efficacious ', 'annotation': 'Research Values'}, {'start': 502, 'end': 565, 'span': 'allows us to accelerate DFKD by a factor of orders of magnitude', 'annotation': 'Research Values'}, {'start': 601, 'end': 607, 'span': 'novel ', 'annotation': 'Research Values'}, {'start': 619, 'end': 713, 'span': 'reuse the shared common features in training data so as to synthesize different data instances', 'annotation': 'Research Values'}, {'start': 918, 'end': 927, 'span': 'achieves ', 'annotation': 'Research Values'}, {'start': 942, 'end': 965, 'span': 'within only a few steps', 'annotation': 'Research Values'}, {'start': 967, 'end': 1027, 'span': 'significantly enhancing the efficiency of data-free training', 'annotation': 'Research Values'}, {'start': 1112, 'end': 1210, 'span': 'achieves 10x and even 100x acceleration while preserving performances on par with state of the art', 'annotation': 'Research Values'}]
62b20662ca9e4e8378d8d7a3	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	polarization, vaccination, supporters, hesitants	[]	[{'start': 331, 'end': 339, 'span': 'urgency ', 'annotation': 'Research Values'}, {'start': 356, 'end': 372, 'span': 'negative effects', 'annotation': 'Research Values'}, {'start': 1034, 'end': 1089, 'span': 'to examine the state of polarization around vaccination', 'annotation': 'Research Values'}, {'start': 1408, 'end': 1515, 'span': ' stark division between supporters and hesitant individuals to continue throughout the vaccination campaign', 'annotation': 'Research Values'}, {'start': 1537, 'end': 1627, 'span': 'increasing commonality in the topical focus of the vaccine supporters and vaccine hesitant', 'annotation': 'Research Values'}, {'start': 1969, 'end': 2078, 'span': 'uncover concerns around vaccination before the public health decisions and official messaging are made public', 'annotation': 'Research Values'}]
2	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	"generalization
data efficiency
research window
AI
"	[]	[{'start': 158, 'end': 190, 'span': 'ultimate artificial intelligence', 'annotation': 'Research Values'}, {'start': 192, 'end': 255, 'span': 'we desire a single machine that can handle diverse new problems', 'annotation': 'Research Values'}, {'start': 267, 'end': 304, 'span': 'task-specific annotations are limited', 'annotation': 'Research Values'}, {'start': 342, 'end': 356, 'span': 'unified solver', 'annotation': 'Research Values'}, {'start': 428, 'end': 479, 'span': 'has not spilled much ink on the following questions', 'annotation': 'Research Values'}, {'start': 538, 'end': 563, 'span': 'generalize across domains', 'annotation': 'Research Values'}, {'start': 803, 'end': 884, 'span': 'really matters particularly when the target NLP task has insufficient annotations', 'annotation': 'Research Values'}, {'start': 886, 'end': 899, 'span': 'Universal NLP', 'annotation': 'Research Values'}, {'start': 980, 'end': 989, 'span': 'Universal', 'annotation': 'Research Values'}, {'start': 990, 'end': 998, 'span': 'Few-shot', 'annotation': 'Research Values'}, {'start': 1108, 'end': 1143, 'span': 'work well on new entailment domains', 'annotation': 'Research Values'}, {'start': 1144, 'end': 1165, 'span': 'in a few-shot setting', 'annotation': 'Research Values'}, {'start': 1199, 'end': 1246, 'span': 'unified solver for several downstream NLP tasks', 'annotation': 'Research Values'}, {'start': 1310, 'end': 1342, 'span': 'end-task annotations are limited', 'annotation': 'Research Values'}]
2	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	"performance 
model
bias
abritrariness
human fidelity"	[]	[{'start': 150, 'end': 225, 'span': 'something fundamentally wrong with NMT as a model or its training algorithm', 'annotation': 'Research Values'}, {'start': 471, 'end': 497, 'span': 'inadequacy of MAP decoding', 'annotation': 'Research Values'}, {'start': 745, 'end': 774, 'span': 'pathologies and biases of NMT', 'annotation': 'Research Values'}, {'start': 964, 'end': 1013, 'span': ' the mode can be considered essentially arbitrary', 'annotation': 'Research Values'}, {'start': 1200, 'end': 1219, 'span': 'competitive results', 'annotation': 'Research Values'}, {'start': 1250, 'end': 1311, 'span': 'capture important aspects of translation well in expectation.', 'annotation': 'Research Values'}]
2	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	"performance
flexibility
understanding"	[]	[{'start': 217, 'end': 266, 'span': ' outperforms the previously reported best results', 'annotation': 'Research Values'}, {'start': 352, 'end': 368, 'span': 'global inference', 'annotation': 'Research Values'}, {'start': 373, 'end': 401, 'span': 'rich feature representations', 'annotation': 'Research Values'}, {'start': 419, 'end': 513, 'span': 'alleviating the fundamental trade-off between transition-based and graph-based parsing systems', 'annotation': 'Research Values'}, {'start': 569, 'end': 597, 'span': 'state-of-the-art performance', 'annotation': 'Research Values'}, {'start': 770, 'end': 850, 'span': 'viable formalism for problems that require rich structural analysis of sentences', 'annotation': 'Research Values'}]
2	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	"performance
speed
efficiency
research attention
novelty"	[]	[{'start': 69, 'end': 115, 'span': 'increasing attention from research communities', 'annotation': 'Research Values'}, {'start': 135, 'end': 191, 'span': 'capability to compress a model only using synthetic data', 'annotation': 'Research Values'}, {'start': 235, 'end': 264, 'span': 'state-of-the-art DFKD methods', 'annotation': 'Research Values'}, {'start': 287, 'end': 317, 'span': 'inefficiency of data synthesis', 'annotation': 'Research Values'}, {'start': 367, 'end': 381, 'span': 'time-consuming', 'annotation': 'Research Values'}, {'start': 390, 'end': 425, 'span': ' inapplicable for large-scale tasks', 'annotation': 'Research Values'}, {'start': 457, 'end': 475, 'span': 'efficacious scheme', 'annotation': 'Research Values'}, {'start': 515, 'end': 565, 'span': 'accelerate DFKD by a factor of orders of magnitude', 'annotation': 'Research Values'}, {'start': 601, 'end': 615, 'span': 'novel strategy', 'annotation': 'Research Values'}, {'start': 669, 'end': 713, 'span': 'so as to synthesize different data instances', 'annotation': 'Research Values'}, {'start': 741, 'end': 777, 'span': 'optimize a set of data independently', 'annotation': 'Research Values'}, {'start': 875, 'end': 894, 'span': 'fast data synthesis', 'annotation': 'Research Values'}, {'start': 942, 'end': 965, 'span': 'within only a few steps', 'annotation': 'Research Values'}, {'start': 968, 'end': 1027, 'span': 'ignificantly enhancing the efficiency of data-free training', 'annotation': 'Research Values'}, {'start': 1112, 'end': 1151, 'span': 'achieves 10x and even 100x acceleration', 'annotation': 'Research Values'}, {'start': 1158, 'end': 1210, 'span': 'preserving performances on par with state of the art', 'annotation': 'Research Values'}]
2	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	"understanding
real-world impact
"	[]	[{'start': 557, 'end': 610, 'span': 'imperative to understand the nature of this discourse', 'annotation': 'Research Values'}, {'start': 681, 'end': 755, 'span': 'healthcare decisions may affect their communities and the country at large', 'annotation': 'Research Values'}]
62d59a3805804b94c5fd526f	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	"domains
transformation
annotations"	[]	[{'start': 76, 'end': 101, 'span': 'problem-specific dataset,', 'annotation': 'Research Values'}, {'start': 267, 'end': 292, 'span': 'task-specific annotations', 'annotation': 'Research Values'}, {'start': 318, 'end': 337, 'span': 'textual entailment ', 'annotation': 'Research Values'}, {'start': 538, 'end': 563, 'span': 'generalize across domains', 'annotation': 'Research Values'}, {'start': 640, 'end': 663, 'span': 'ransforming an NLP task', 'annotation': 'Research Values'}, {'start': 860, 'end': 884, 'span': 'insufficient annotations', 'annotation': 'Research Values'}, {'start': 1076, 'end': 1104, 'span': ' pretrained entailment model', 'annotation': 'Research Values'}]
62d59a3805804b94c5fd526f	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	"translation
decoding
decisions
assumptions
inadequacy"	[]	[{'start': 382, 'end': 411, 'span': ' highest-scoring translation,', 'annotation': 'Research Values'}, {'start': 471, 'end': 481, 'span': 'inadequacy', 'annotation': 'Research Values'}, {'start': 617, 'end': 645, 'span': 'reproduce various statistics', 'annotation': 'Research Values'}, {'start': 786, 'end': 798, 'span': 'MAP decoding', 'annotation': 'Research Values'}, {'start': 816, 'end': 839, 'span': 'statistical assumptions', 'annotation': 'Research Values'}, {'start': 936, 'end': 959, 'span': 'little probability mass', 'annotation': 'Research Values'}, {'start': 1052, 'end': 1066, 'span': 'decision rules', 'annotation': 'Research Values'}, {'start': 1165, 'end': 1193, 'span': ' minimum Bayes risk decoding', 'annotation': 'Research Values'}]
62d59a3805804b94c5fd526f	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	"representation
global
viable
downstream"	[]	[{'start': 161, 'end': 168, 'span': 'jointly', 'annotation': 'Research Values'}, {'start': 218, 'end': 229, 'span': 'outperforms', 'annotation': 'Research Values'}, {'start': 352, 'end': 368, 'span': 'global inference', 'annotation': 'Research Values'}, {'start': 378, 'end': 401, 'span': 'feature representations', 'annotation': 'Research Values'}, {'start': 447, 'end': 457, 'span': 'trade-off ', 'annotation': 'Research Values'}, {'start': 605, 'end': 621, 'span': 'downstream tasks', 'annotation': 'Research Values'}, {'start': 770, 'end': 787, 'span': 'viable formalism ', 'annotation': 'Research Values'}]
62d59a3805804b94c5fd526f	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	"accelerate
inapplicable
enhancing
efficiency"	[]	[{'start': 69, 'end': 89, 'span': 'increasing attention', 'annotation': 'Research Values'}, {'start': 135, 'end': 145, 'span': 'capability', 'annotation': 'Research Values'}, {'start': 287, 'end': 300, 'span': 'inefficiency ', 'annotation': 'Research Values'}, {'start': 390, 'end': 403, 'span': ' inapplicable', 'annotation': 'Research Values'}, {'start': 457, 'end': 468, 'span': 'efficacious', 'annotation': 'Research Values'}, {'start': 515, 'end': 530, 'span': 'accelerate DFKD', 'annotation': 'Research Values'}, {'start': 678, 'end': 703, 'span': 'synthesize different data', 'annotation': 'Research Values'}, {'start': 764, 'end': 777, 'span': 'independently', 'annotation': 'Research Values'}, {'start': 981, 'end': 990, 'span': 'enhancing', 'annotation': 'Research Values'}, {'start': 995, 'end': 1027, 'span': 'efficiency of data-free training', 'annotation': 'Research Values'}]
62d59a3805804b94c5fd526f	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	"hesitation
dataset
debate
textual
unfounded
concerns"	[]	[{'start': 150, 'end': 170, 'span': 'vaccination campaign', 'annotation': 'Research Values'}, {'start': 277, 'end': 296, 'span': 'igorous discussion ', 'annotation': 'Research Values'}, {'start': 452, 'end': 484, 'span': 'pre-pandemic vaccination debate ', 'annotation': 'Research Values'}, {'start': 505, 'end': 514, 'span': 'polarized', 'annotation': 'Research Values'}, {'start': 653, 'end': 673, 'span': 'hesitant individuals', 'annotation': 'Research Values'}, {'start': 978, 'end': 992, 'span': 'Twitter datase', 'annotation': 'Research Values'}, {'start': 1104, 'end': 1128, 'span': 'hierarchical clustering ', 'annotation': 'Research Values'}, {'start': 1176, 'end': 1188, 'span': 'endorsement ', 'annotation': 'Research Values'}, {'start': 1318, 'end': 1327, 'span': 'structure', 'annotation': 'Research Values'}, {'start': 1358, 'end': 1374, 'span': 'textual content ', 'annotation': 'Research Values'}, {'start': 1548, 'end': 1559, 'span': 'commonality', 'annotation': 'Research Values'}, {'start': 1731, 'end': 1740, 'span': 'concerns ', 'annotation': 'Research Values'}, {'start': 1787, 'end': 1809, 'span': 'unfounded conspiracies', 'annotation': 'Research Values'}, {'start': 1926, 'end': 1938, 'span': 'surveillance', 'annotation': 'Research Values'}]
121	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	N/A	[{'start': 443, 'end': 494, 'span': 'In this paper, we re-examine the role of TD in mode', 'annotation': 'Values'}]	[]
6318b92c7b39fed36c4514a8	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.	"Dataset
Textual Entailment
UFO-Entail"	[]	[{'start': 61, 'end': 143, 'span': 'constructing a problem-specific dataset, then building a model to fit this dataset', 'annotation': 'Research Values'}, {'start': 318, 'end': 356, 'span': 'textual entailment as a unified solver', 'annotation': 'Research Values'}, {'start': 980, 'end': 1030, 'span': 'Universal Few-shot textual Entailment (UFO-Entail)', 'annotation': 'Research Values'}]
6318b92c7b39fed36c4514a8	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.	"NMT
MAP decoding"	[]	[{'start': 185, 'end': 225, 'span': 'NMT as a model or its training algorithm', 'annotation': 'Research Values'}, {'start': 471, 'end': 497, 'span': 'inadequacy of MAP decoding', 'annotation': 'Research Values'}, {'start': 745, 'end': 798, 'span': 'pathologies and biases of NMT are due to MAP decoding', 'annotation': 'Research Values'}]
6318b92c7b39fed36c4514a8	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.	"Graph based TAG
BiLSTM
tagging
PETE
UDR"	[]	[{'start': 13, 'end': 53, 'span': 'graph-based Tree Adjoining Grammar (TAG)', 'annotation': 'Research Values'}, {'start': 71, 'end': 77, 'span': 'BiLSTM', 'annotation': 'Research Values'}, {'start': 169, 'end': 216, 'span': 'performs supertagging, POS tagging, and parsing', 'annotation': 'Research Values'}, {'start': 625, 'end': 710, 'span': 'Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery', 'annotation': 'Research Values'}]
6318b92c7b39fed36c4514a8	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.	"DFKD
Synthetic data"	[]	[{'start': 0, 'end': 39, 'span': 'Data-free knowledge distillation (DFKD)', 'annotation': 'Research Values'}, {'start': 149, 'end': 191, 'span': 'compress a model only using synthetic data', 'annotation': 'Research Values'}]
6318b92c7b39fed36c4514a8	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.	Hierarchical clustering approach	[]	[{'start': 1104, 'end': 1136, 'span': 'hierarchical clustering approach', 'annotation': 'Research Values'}]
1001	14	We introduce a novel dataset of real multi-destination trips booked through Booking.com's online travel platform. The dataset consists of 1.5 million reservations representing 359,000 unique journeys made across 39,000 destinations. As such, the data is particularly well suited to model sequential recommendation and retrieval problems in a high cardinality target space. To preserve user privacy and protect business-sensitive statistics, the data is fully anonymized, sampled and limited to five user origin markets. Even so, the dataset is representative of the general travel purchase behavior and therefore presents a uniquely valuable resource for Machine Learning and information retrieval researchers. This work provides an overview of the dataset. It reports several benchmark results for relevant recommendation problems obtained as part of the recently held Booking.com data challenge during the WSDM WebTour workshop.		[]	[{'start': 15, 'end': 20, 'span': 'novel', 'annotation': 'Research Values'}, {'start': 267, 'end': 278, 'span': 'well suited', 'annotation': 'Research Values'}, {'start': 385, 'end': 397, 'span': 'user privacy', 'annotation': 'Research Values'}, {'start': 402, 'end': 439, 'span': 'protect business-sensitive statistics', 'annotation': 'Research Values'}, {'start': 445, 'end': 469, 'span': 'data is fully anonymized', 'annotation': 'Research Values'}, {'start': 483, 'end': 518, 'span': 'limited to five user origin markets', 'annotation': 'Research Values'}, {'start': 544, 'end': 598, 'span': 'representative of the general travel purchase behavior', 'annotation': 'Research Values'}, {'start': 624, 'end': 632, 'span': 'uniquely', 'annotation': 'Research Values'}, {'start': 633, 'end': 650, 'span': 'valuable resource', 'annotation': 'Research Values'}, {'start': 799, 'end': 807, 'span': 'relevant', 'annotation': 'Research Values'}]
1001	7	Government documents must be reviewed to identify and protect any sensitive information, such as personal information, before the documents can be released to the public. However, in the era of digital government documents, such as e-mail, traditional sensitivity review procedures are no longer practical, for example due to the volume of documents to be reviewed. Therefore, there is a need for new technology assisted review protocols to integrate automatic sensitivity classification into the sensitivity review process. Moreover, to effectively assist sensitivity review, such assistive technologies must incorporate reviewer feedback to enable sensitivity classifiers to quickly learn and adapt to the sensitivities within a collection, when the types of sensitivity are not known a priori. In this work, we present a thorough evaluation of active learning strategies for sensitivity review. Moreover, we present an active learning strategy that integrates reviewer feedback, from sensitive text annotations, to identify features of sensitivity that enable us to learn an effective sensitivity classifier (0.7 Balanced Accuracy) using significantly less reviewer effort, according to the sign test (\(p<0.01\)). Moreover, this approach results in a 51% reduction in the number of documents required to be reviewed to achieve the same level of classification accuracy, compared to when the approach is deployed without annotation features.		[]	[{'start': 54, 'end': 87, 'span': 'protect any sensitive information', 'annotation': 'Research Values'}, {'start': 286, 'end': 305, 'span': 'no longer practical', 'annotation': 'Research Values'}, {'start': 451, 'end': 487, 'span': 'automatic sensitivity classification', 'annotation': 'Research Values'}, {'start': 538, 'end': 549, 'span': 'effectively', 'annotation': 'Research Values'}, {'start': 605, 'end': 639, 'span': 'must incorporate reviewer feedback', 'annotation': 'Research Values'}, {'start': 677, 'end': 684, 'span': 'quickly', 'annotation': 'Research Values'}, {'start': 695, 'end': 700, 'span': 'adapt', 'annotation': 'Research Values'}, {'start': 824, 'end': 833, 'span': 'thorough ', 'annotation': 'Research Values'}, {'start': 952, 'end': 980, 'span': 'integrates reviewer feedback', 'annotation': 'Research Values'}, {'start': 1078, 'end': 1087, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 1135, 'end': 1175, 'span': 'using significantly less reviewer effort', 'annotation': 'Research Values'}, {'start': 1268, 'end': 1328, 'span': 'reduction in the number of documents required to be reviewed', 'annotation': 'Research Values'}, {'start': 1344, 'end': 1381, 'span': 'same level of classification accuracy', 'annotation': 'Research Values'}]
1001	9	"ntroductory computer networks courses often include descriptive coverage of the network protocol headers. A straightforward listing of the headers and their meanings can lead to questions like ""Will this be on the test""? Programming exercises may involve selecting values for some aspects of a protocol but tend to abstract away the details, and depend on prior programming skills. In addition, campuses without dedicated network lab facilities may have limited ability to experiment with protocols on an existing institutional network. The Python-based Scapy package provides explicit, detailed control of the contents of header fields, and includes graphical visualization features that offer easy feedback. Programming ability is helpful but not necessary; the interactive Python environment permits step-by-step and guided exploration of the various protocols. Effective use of scapy requires root (administrator) privileges; a virtual machine environment such as that provided by Oracle VirtualBox allows complete control and access to the operating system. This talk is about scapy-based lab modules that the author is developing, which provide active, hands-on exposure to and manipulation of network headers. So far, a Transport-layer activity and a preliminary Link-layer activity have been written. The current activities will be discussed, along with ideas for additional modules."		[]	[{'start': 52, 'end': 63, 'span': 'descriptive', 'annotation': 'Research Values'}, {'start': 108, 'end': 123, 'span': 'straightforward', 'annotation': 'Research Values'}, {'start': 315, 'end': 340, 'span': 'abstract away the details', 'annotation': 'Research Values'}, {'start': 346, 'end': 380, 'span': 'depend on prior programming skills', 'annotation': 'Research Values'}, {'start': 454, 'end': 498, 'span': 'limited ability to experiment with protocols', 'annotation': 'Research Values'}, {'start': 577, 'end': 585, 'span': 'explicit', 'annotation': 'Research Values'}, {'start': 587, 'end': 595, 'span': 'detailed', 'annotation': 'Research Values'}, {'start': 596, 'end': 603, 'span': 'control', 'annotation': 'Research Values'}, {'start': 651, 'end': 674, 'span': 'graphical visualization', 'annotation': 'Research Values'}, {'start': 695, 'end': 708, 'span': 'easy feedback', 'annotation': 'Research Values'}, {'start': 764, 'end': 775, 'span': 'interactive', 'annotation': 'Research Values'}, {'start': 803, 'end': 815, 'span': 'step-by-step', 'annotation': 'Research Values'}, {'start': 820, 'end': 826, 'span': 'guided', 'annotation': 'Research Values'}, {'start': 865, 'end': 874, 'span': 'Effective', 'annotation': 'Research Values'}, {'start': 1010, 'end': 1026, 'span': 'complete control', 'annotation': 'Research Values'}, {'start': 1031, 'end': 1037, 'span': 'access', 'annotation': 'Research Values'}, {'start': 1151, 'end': 1157, 'span': 'active', 'annotation': 'Research Values'}, {'start': 1159, 'end': 1167, 'span': 'hands-on', 'annotation': 'Research Values'}]
1001	11	The paper addresses the problem of forecasting consumer expenditure from social media data. Previous research of the topic exploited the intuition that search engine traffic reflects purchase intentions and constructed predictive models of consumer behaviour from search query volumes. In contrast, we derive predictors from explicit expressions of purchase intentions found in social media posts. Two types of predictors created from these expressions are explored: those based on word embeddings and those based on topical word clusters. We introduce a new clustering method, which takes into account temporal co-occurrence of words, in addition to their semantic similarity, in order to create predictors relevant to the forecasting problem. The predictors are evaluated against baselines that use only macroeconomic variables, and against models trained on search traffic data. Conducting experiments with three different regression methods on Facebook and Twitter data, we find that both word embeddings and word clusters help to reduce forecasting errors in comparison to purely macroeconomic models. In most experimental settings, the error reduction is statistically significant, and is comparable to error reduction achieved with search traffic variables.		[]	[{'start': 325, 'end': 334, 'span': 'explicit ', 'annotation': 'Research Values'}, {'start': 555, 'end': 558, 'span': 'new', 'annotation': 'Research Values'}, {'start': 708, 'end': 716, 'span': 'relevant', 'annotation': 'Research Values'}, {'start': 1035, 'end': 1060, 'span': 'reduce forecasting errors', 'annotation': 'Research Values'}, {'start': 1142, 'end': 1157, 'span': 'error reduction', 'annotation': 'Research Values'}, {'start': 1161, 'end': 1186, 'span': 'statistically significant', 'annotation': 'Research Values'}, {'start': 1195, 'end': 1263, 'span': 'comparable to error reduction achieved with search traffic variables', 'annotation': 'Research Values'}]
1001	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.		[]	[{'start': 299, 'end': 308, 'span': 'different', 'annotation': 'Research Values'}, {'start': 313, 'end': 346, 'span': 'potentially conflicting utilities', 'annotation': 'Research Values'}, {'start': 409, 'end': 427, 'span': 'jointly optimizing', 'annotation': 'Research Values'}, {'start': 521, 'end': 535, 'span': 'not applicable', 'annotation': 'Research Values'}, {'start': 682, 'end': 689, 'span': 'general', 'annotation': 'Research Values'}, {'start': 758, 'end': 771, 'span': 'heterogeneous', 'annotation': 'Research Values'}, {'start': 776, 'end': 797, 'span': 'hierarchical contents', 'annotation': 'Research Values'}, {'start': 812, 'end': 823, 'span': 'constrained', 'annotation': 'Research Values'}, {'start': 1051, 'end': 1078, 'span': 'consistent user experience ', 'annotation': 'Research Values'}, {'start': 1157, 'end': 1169, 'span': 'transparency', 'annotation': 'Research Values'}, {'start': 1232, 'end': 1241, 'span': 'efficient', 'annotation': 'Research Values'}, {'start': 1573, 'end': 1586, 'span': 'effectiveness', 'annotation': 'Research Values'}, {'start': 1642, 'end': 1652, 'span': 'optimizing', 'annotation': 'Research Values'}, {'start': 1716, 'end': 1733, 'span': 'deployed globally', 'annotation': 'Research Values'}]
1001	10	This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.		[]	[{'start': 252, 'end': 274, 'span': 'coreference resolution', 'annotation': 'Research Values'}, {'start': 499, 'end': 511, 'span': 'new resource', 'annotation': 'Research Values'}, {'start': 590, 'end': 603, 'span': 'balanced data', 'annotation': 'Research Values'}]
1001	12	Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.		[]	[{'start': 248, 'end': 264, 'span': 'not reproducible', 'annotation': 'Research Values'}, {'start': 300, 'end': 375, 'span': 'uses components (such as document popularity) that are not always available', 'annotation': 'Research Values'}, {'start': 409, 'end': 416, 'span': 'unclear', 'annotation': 'Research Values'}, {'start': 532, 'end': 554, 'span': 'event-specific corpora', 'annotation': 'Research Values'}, {'start': 622, 'end': 631, 'span': 'differing', 'annotation': 'Research Values'}, {'start': 633, 'end': 650, 'span': 'relatively simple', 'annotation': 'Research Values'}, {'start': 752, 'end': 763, 'span': 'performance', 'annotation': 'Research Values'}, {'start': 793, 'end': 797, 'span': 'weak', 'annotation': 'Research Values'}, {'start': 956, 'end': 972, 'span': 'highly sensitive', 'annotation': 'Research Values'}, {'start': 1100, 'end': 1131, 'span': 'common static background corpus', 'annotation': 'Research Values'}]
1001	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.		[]	[{'start': 30, 'end': 73, 'span': 'improves upon the current state of the art ', 'annotation': 'Research Values'}, {'start': 76, 'end': 100, 'span': 'increasing the usability', 'annotation': 'Research Values'}, {'start': 105, 'end': 123, 'span': 'practical security', 'annotation': 'Research Values'}, {'start': 160, 'end': 170, 'span': 'More users', 'annotation': 'Research Values'}, {'start': 217, 'end': 267, 'span': 'without mistakenly revealing sensitive information', 'annotation': 'Research Values'}, {'start': 327, 'end': 359, 'span': 'positively affect the usability ', 'annotation': 'Research Values'}, {'start': 362, 'end': 371, 'span': ' security', 'annotation': 'Research Values'}, {'start': 483, 'end': 497, 'span': 'high usability', 'annotation': 'Research Values'}, {'start': 498, 'end': 511, 'span': ' few mistakes', 'annotation': 'Research Values'}, {'start': 519, 'end': 591, 'span': 'strong understanding of the protection provided to secure email messages', 'annotation': 'Research Values'}, {'start': 657, 'end': 667, 'span': 'usability ', 'annotation': 'Research Values'}, {'start': 670, 'end': 678, 'span': 'security', 'annotation': 'Research Values'}]
1001	13	Trust prediction, aiming to predict the trust relations between users in a social network, is a key to helping users discover the reliable information. Many trust prediction methods are proposed based on the low-rank assumption of a trust network. However, one typical property of the trust network is that the trust relations follow the power-law distribution, i.e., few users are trusted by many other users, while most tail users have few trustors. Due to these tail users, the fundamental low-rank assumption made by existing methods is seriously violated and becomes unrealistic. In this paper, we propose a simple yet effective method to address the problem of the violated low-rank assumption. Instead of discovering the low-rank component of the trust network alone, we learn a sparse component of the trust network to describe the tail users simultaneously. With both of the learned low-rank and sparse components, the trust relations in the whole network can be better captured. Moreover, the transitive closure structure of the trust relations is also integrated into our model. We then derive an effective iterative algorithm to infer the parameters of our model, along with the proof of correctness. Extensive experimental results on real-world trust networks demonstrate the superior performance of our proposed method over the state-of-the-arts.		[]	[{'start': 117, 'end': 150, 'span': 'discover the reliable information', 'annotation': 'Research Values'}, {'start': 368, 'end': 409, 'span': 'few users are trusted by many other users', 'annotation': 'Research Values'}, {'start': 481, 'end': 560, 'span': 'fundamental low-rank assumption made by existing methods is seriously violated ', 'annotation': 'Research Values'}, {'start': 572, 'end': 583, 'span': 'unrealistic', 'annotation': 'Research Values'}, {'start': 613, 'end': 619, 'span': 'simple', 'annotation': 'Research Values'}, {'start': 623, 'end': 633, 'span': ' effective', 'annotation': 'Research Values'}, {'start': 671, 'end': 699, 'span': 'violated low-rank assumption', 'annotation': 'Research Values'}, {'start': 972, 'end': 987, 'span': 'better captured', 'annotation': 'Research Values'}, {'start': 1003, 'end': 1031, 'span': 'transitive closure structure', 'annotation': 'Research Values'}, {'start': 1108, 'end': 1117, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 1118, 'end': 1127, 'span': 'iterative', 'annotation': 'Research Values'}, {'start': 1191, 'end': 1211, 'span': 'proof of correctness', 'annotation': 'Research Values'}, {'start': 1289, 'end': 1309, 'span': 'superior performance', 'annotation': 'Research Values'}]
1001	8	We study the problem of multi-dimensional revenue maximization when selling m items to a buyer that has additive valuations for them, drawn from a (possibly correlated) prior distribution. Unlike traditional Bayesian auction design, we assume that the seller has a very restricted knowledge of this prior: they only know the mean μj and an upper bound σj on the standard deviation of each item’s marginal distribution. Our goal is to design mechanisms that achieve good revenue against an ideal optimal auction that has full knowledge of the distribution in advance. Informally, our main contribution is a tight quantification of the interplay between the dispersity of the priors and the aforementioned robust approximation ratio. Furthermore, this can be achieved by very simple selling mechanisms. More precisely, we show that selling the items via separate price lotteries achieves an O(log r) approximation ratio where r = maxj(σj/μj) is the maximum coefficient of variation across the items. To prove the result, we leverage a price lottery for the single-item case. If forced to restrict ourselves to deterministic mechanisms, this guarantee degrades to O(r2). Assuming independence of the item valuations, these ratios can be further improved by pricing the full bundle. For the case of identical means and variances, in particular, we get a guarantee of O(log (r/m)) that converges to optimality as the number of items grows large. We demonstrate the optimality of the preceding mechanisms by providing matching lower bounds. Our tight analysis for the single-item deterministic case resolves an open gap from the work of Azar and Micali (ITCS’13). As a by-product, we also show how one can directly use our upper bounds to improve and extend previous results related to the parametric auctions of Azar et al. (SODA’13).		[]	[{'start': 457, 'end': 478, 'span': 'achieve good revenue ', 'annotation': 'Research Values'}, {'start': 606, 'end': 627, 'span': 'tight quantification ', 'annotation': 'Research Values'}, {'start': 704, 'end': 710, 'span': 'robust', 'annotation': 'Research Values'}, {'start': 769, 'end': 780, 'span': 'very simple', 'annotation': 'Research Values'}, {'start': 877, 'end': 917, 'span': 'achieves an O(log r) approximation ratio', 'annotation': 'Research Values'}, {'start': 1220, 'end': 1250, 'span': 'ratios can be further improved', 'annotation': 'Research Values'}, {'start': 1381, 'end': 1404, 'span': 'converges to optimality', 'annotation': 'Research Values'}, {'start': 1460, 'end': 1470, 'span': 'optimality', 'annotation': 'Research Values'}, {'start': 1539, 'end': 1544, 'span': 'tight', 'annotation': 'Research Values'}, {'start': 1593, 'end': 1613, 'span': 'resolves an open gap', 'annotation': 'Research Values'}, {'start': 1700, 'end': 1729, 'span': 'directly use our upper bounds', 'annotation': 'Research Values'}, {'start': 1733, 'end': 1740, 'span': 'improve', 'annotation': 'Research Values'}, {'start': 1745, 'end': 1769, 'span': 'extend previous results ', 'annotation': 'Research Values'}]
1001	6	"Subspace clustering has been widely applied to detect meaningful clusters in high-dimensional data spaces. A main challenge in subspace clustering is to quickly calculate a ""good"" affinity matrix. ℓ0, ℓ1, ℓ2 or nuclear norm regularization is used to construct the affinity matrix in many subspace clustering methods because of their theoretical guarantees and empirical success. However, they suffer from the following problems: (1) ℓ2 and nuclear norm regularization require very strong assumptions to guarantee a subspace-preserving affinity; (2) although ℓ1 regularization can be guaranteed to give a subspace-preserving affinity under certain conditions, it needs more time to solve a large-scale convex optimization problem; (3) ℓ0 regularization can yield a tradeoff between computationally efficient and subspace-preserving affinity by using the orthogonal matching pursuit (OMP) algorithm, but this still takes more time to search the solution in OMP when the number of data points is large. In order to overcome these problems, we first propose a learned OMP (LOMP) algorithm to learn a single hidden neural network (SHNN) to fast approximate the ℓ0code. We then exploit a sparse subspace clustering method based on ℓ0 code which is fast computed by SHNN. Two sufficient conditions are presented to guarantee that our method can give a subspace-preserving affinity. Experiments on handwritten digit and face clustering show that our method not only quickly computes the ℓ0 code, but also outperforms the relevant subspace clustering methods in clustering results. In particular, our method achieves the state-of-the-art clustering accuracy (94.32%) on MNIST."		[]	[{'start': 54, 'end': 65, 'span': 'meaningful ', 'annotation': 'Research Values'}, {'start': 153, 'end': 161, 'span': 'quickly ', 'annotation': 'Research Values'}, {'start': 333, 'end': 356, 'span': 'theoretical guarantees ', 'annotation': 'Research Values'}, {'start': 360, 'end': 377, 'span': 'empirical success', 'annotation': 'Research Values'}, {'start': 468, 'end': 499, 'span': 'require very strong assumptions', 'annotation': 'Research Values'}, {'start': 662, 'end': 728, 'span': 'needs more time to solve a large-scale convex optimization problem', 'annotation': 'Research Values'}, {'start': 781, 'end': 806, 'span': 'computationally efficient', 'annotation': 'Research Values'}, {'start': 811, 'end': 839, 'span': 'subspace-preserving affinity', 'annotation': 'Research Values'}, {'start': 913, 'end': 929, 'span': 'takes more time ', 'annotation': 'Research Values'}, {'start': 1135, 'end': 1139, 'span': 'fast', 'annotation': 'Research Values'}, {'start': 1242, 'end': 1255, 'span': 'fast computed', 'annotation': 'Research Values'}, {'start': 1308, 'end': 1317, 'span': 'guarantee', 'annotation': 'Research Values'}, {'start': 1345, 'end': 1373, 'span': 'subspace-preserving affinity', 'annotation': 'Research Values'}, {'start': 1458, 'end': 1465, 'span': 'quickly', 'annotation': 'Research Values'}, {'start': 1497, 'end': 1508, 'span': 'outperforms', 'annotation': 'Research Values'}, {'start': 1599, 'end': 1648, 'span': 'achieves the state-of-the-art clustering accuracy', 'annotation': 'Research Values'}]
1001	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.		[]	[{'start': 198, 'end': 220, 'span': 'global characteristics', 'annotation': 'Research Values'}, {'start': 322, 'end': 384, 'span': 'only considered the representation of a single image-text post', 'annotation': 'Research Values'}, {'start': 389, 'end': 462, 'span': 'failed to capture the global co-occurrence characteristics of the dataset', 'annotation': 'Research Values'}, {'start': 785, 'end': 807, 'span': 'global characteristics', 'annotation': 'Research Values'}, {'start': 1044, 'end': 1057, 'span': 'effectiveness', 'annotation': 'Research Values'}]
1001	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.		[]	[{'start': 276, 'end': 322, 'span': 'superior to direct Monte Carlo estimation (MC)', 'annotation': 'Research Values'}, {'start': 337, 'end': 363, 'span': 'results hold up in deep RL', 'annotation': 'Research Values'}, {'start': 371, 'end': 415, 'span': 'deals with perceptually complex environments', 'annotation': 'Research Values'}, {'start': 531, 'end': 601, 'span': 'environments that control for specific factors that affect performance', 'annotation': 'Research Values'}, {'start': 611, 'end': 626, 'span': 'reward sparsity', 'annotation': 'Research Values'}, {'start': 628, 'end': 640, 'span': 'reward delay', 'annotation': 'Research Values'}, {'start': 650, 'end': 671, 'span': 'perceptual complexity', 'annotation': 'Research Values'}, {'start': 744, 'end': 788, 'span': 'reproduce classic results in modern settings', 'annotation': 'Research Values'}, {'start': 837, 'end': 851, 'span': 'inferior to TD', 'annotation': 'Research Values'}, {'start': 910, 'end': 934, 'span': 'viable alternative to TD', 'annotation': 'Research Values'}]
1001	5	When a reader is first introduced to an entity, its referring expression must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how expressions that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a model to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30% absolute improvement over a majority-class baseline.		[]	[{'start': 121, 'end': 133, 'span': 'widely known', 'annotation': 'Research Values'}, {'start': 673, 'end': 687, 'span': 'shorter length', 'annotation': 'Research Values'}, {'start': 689, 'end': 712, 'span': 'less use of appositives', 'annotation': 'Research Values'}, {'start': 714, 'end': 731, 'span': 'more definiteness', 'annotation': 'Research Values'}, {'start': 885, 'end': 936, 'span': 'absolute improvement over a majority-class baseline', 'annotation': 'Research Values'}]
1001	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.		[]	[{'start': 60, 'end': 77, 'span': 'performance gains', 'annotation': 'Research Values'}, {'start': 203, 'end': 250, 'span': 'learning better Chinese language representation', 'annotation': 'Research Values'}, {'start': 322, 'end': 388, 'span': 'not able to encode semantic information contained in Chinese words', 'annotation': 'Research Values'}, {'start': 489, 'end': 532, 'span': 'suffer from deficient semantic interactions', 'annotation': 'Research Values'}, {'start': 537, 'end': 602, 'span': 'fail to capture the semantic relation between words and character', 'annotation': 'Research Values'}, {'start': 647, 'end': 653, 'span': 'simple', 'annotation': 'Research Values'}, {'start': 658, 'end': 667, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 980, 'end': 1008, 'span': 'value in realistic scenarios', 'annotation': 'Research Values'}, {'start': 1025, 'end': 1044, 'span': 'easily incorporated', 'annotation': 'Research Values'}, {'start': 1087, 'end': 1129, 'span': 'without modifying the production pipelines', 'annotation': 'Research Values'}, {'start': 1210, 'end': 1230, 'span': 'superior performance', 'annotation': 'Research Values'}]
1002	14	We introduce a novel dataset of real multi-destination trips booked through Booking.com's online travel platform. The dataset consists of 1.5 million reservations representing 359,000 unique journeys made across 39,000 destinations. As such, the data is particularly well suited to model sequential recommendation and retrieval problems in a high cardinality target space. To preserve user privacy and protect business-sensitive statistics, the data is fully anonymized, sampled and limited to five user origin markets. Even so, the dataset is representative of the general travel purchase behavior and therefore presents a uniquely valuable resource for Machine Learning and information retrieval researchers. This work provides an overview of the dataset. It reports several benchmark results for relevant recommendation problems obtained as part of the recently held Booking.com data challenge during the WSDM WebTour workshop.		[]	[{'start': 15, 'end': 20, 'span': 'novel', 'annotation': 'Research Values'}, {'start': 279, 'end': 336, 'span': 'to model sequential recommendation and retrieval problems', 'annotation': 'Research Values'}, {'start': 721, 'end': 741, 'span': 'provides an overview', 'annotation': 'Research Values'}, {'start': 761, 'end': 794, 'span': 'reports several benchmark results', 'annotation': 'Research Values'}]
1002	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.		[]	[{'start': 690, 'end': 713, 'span': 'introduce multi-channel', 'annotation': 'Research Values'}, {'start': 739, 'end': 771, 'span': 'learn multimodal representations', 'annotation': 'Research Values'}, {'start': 846, 'end': 872, 'span': 'multimodal in-depth fusion', 'annotation': 'Research Values'}, {'start': 916, 'end': 937, 'span': 'predict the sentiment', 'annotation': 'Research Values'}, {'start': 1044, 'end': 1088, 'span': 'effectiveness of our approach for multimodal', 'annotation': 'Research Values'}]
1002	6	"Subspace clustering has been widely applied to detect meaningful clusters in high-dimensional data spaces. A main challenge in subspace clustering is to quickly calculate a ""good"" affinity matrix. ℓ0, ℓ1, ℓ2 or nuclear norm regularization is used to construct the affinity matrix in many subspace clustering methods because of their theoretical guarantees and empirical success. However, they suffer from the following problems: (1) ℓ2 and nuclear norm regularization require very strong assumptions to guarantee a subspace-preserving affinity; (2) although ℓ1 regularization can be guaranteed to give a subspace-preserving affinity under certain conditions, it needs more time to solve a large-scale convex optimization problem; (3) ℓ0 regularization can yield a tradeoff between computationally efficient and subspace-preserving affinity by using the orthogonal matching pursuit (OMP) algorithm, but this still takes more time to search the solution in OMP when the number of data points is large. In order to overcome these problems, we first propose a learned OMP (LOMP) algorithm to learn a single hidden neural network (SHNN) to fast approximate the ℓ0code. We then exploit a sparse subspace clustering method based on ℓ0 code which is fast computed by SHNN. Two sufficient conditions are presented to guarantee that our method can give a subspace-preserving affinity. Experiments on handwritten digit and face clustering show that our method not only quickly computes the ℓ0 code, but also outperforms the relevant subspace clustering methods in clustering results. In particular, our method achieves the state-of-the-art clustering accuracy (94.32%) on MNIST."		[]	"[{'start': 47, 'end': 73, 'span': 'detect meaningful clusters', 'annotation': 'Research Values'}, {'start': 153, 'end': 196, 'span': 'quickly calculate a ""good"" affinity matrix.', 'annotation': 'Research Values'}, {'start': 250, 'end': 279, 'span': 'construct the affinity matrix', 'annotation': 'Research Values'}, {'start': 481, 'end': 499, 'span': 'strong assumptions', 'annotation': 'Research Values'}, {'start': 503, 'end': 543, 'span': 'guarantee a subspace-preserving affinity', 'annotation': 'Research Values'}, {'start': 668, 'end': 686, 'span': 'more time to solve', 'annotation': 'Research Values'}, {'start': 764, 'end': 806, 'span': 'tradeoff between computationally efficient', 'annotation': 'Research Values'}, {'start': 1088, 'end': 1124, 'span': 'learn a single hidden neural network', 'annotation': 'Research Values'}, {'start': 1172, 'end': 1197, 'span': 'exploit a sparse subspace', 'annotation': 'Research Values'}, {'start': 1269, 'end': 1290, 'span': 'sufficient conditions', 'annotation': 'Research Values'}, {'start': 1599, 'end': 1628, 'span': 'achieves the state-of-the-art', 'annotation': 'Research Values'}, {'start': 1640, 'end': 1657, 'span': 'accuracy (94.32%)', 'annotation': 'Research Values'}]"
1002	7	Government documents must be reviewed to identify and protect any sensitive information, such as personal information, before the documents can be released to the public. However, in the era of digital government documents, such as e-mail, traditional sensitivity review procedures are no longer practical, for example due to the volume of documents to be reviewed. Therefore, there is a need for new technology assisted review protocols to integrate automatic sensitivity classification into the sensitivity review process. Moreover, to effectively assist sensitivity review, such assistive technologies must incorporate reviewer feedback to enable sensitivity classifiers to quickly learn and adapt to the sensitivities within a collection, when the types of sensitivity are not known a priori. In this work, we present a thorough evaluation of active learning strategies for sensitivity review. Moreover, we present an active learning strategy that integrates reviewer feedback, from sensitive text annotations, to identify features of sensitivity that enable us to learn an effective sensitivity classifier (0.7 Balanced Accuracy) using significantly less reviewer effort, according to the sign test (\(p<0.01\)). Moreover, this approach results in a 51% reduction in the number of documents required to be reviewed to achieve the same level of classification accuracy, compared to when the approach is deployed without annotation features.		[]	[{'start': 41, 'end': 61, 'span': 'identify and protect', 'annotation': 'Research Values'}, {'start': 397, 'end': 437, 'span': 'new technology assisted review protocols', 'annotation': 'Research Values'}, {'start': 441, 'end': 487, 'span': 'integrate automatic sensitivity classification', 'annotation': 'Research Values'}, {'start': 538, 'end': 575, 'span': 'effectively assist sensitivity review', 'annotation': 'Research Values'}, {'start': 610, 'end': 639, 'span': 'incorporate reviewer feedback', 'annotation': 'Research Values'}, {'start': 643, 'end': 673, 'span': 'enable sensitivity classifiers', 'annotation': 'Research Values'}, {'start': 677, 'end': 690, 'span': 'quickly learn', 'annotation': 'Research Values'}, {'start': 695, 'end': 722, 'span': 'adapt to the sensitivities ', 'annotation': 'Research Values'}, {'start': 952, 'end': 980, 'span': 'integrates reviewer feedback', 'annotation': 'Research Values'}, {'start': 1078, 'end': 1110, 'span': 'effective sensitivity classifier', 'annotation': 'Research Values'}, {'start': 1125, 'end': 1133, 'span': 'Accuracy', 'annotation': 'Research Values'}, {'start': 1141, 'end': 1175, 'span': 'significantly less reviewer effort', 'annotation': 'Research Values'}, {'start': 1264, 'end': 1304, 'span': '51% reduction in the number of documents', 'annotation': 'Research Values'}]
1002	9	"ntroductory computer networks courses often include descriptive coverage of the network protocol headers. A straightforward listing of the headers and their meanings can lead to questions like ""Will this be on the test""? Programming exercises may involve selecting values for some aspects of a protocol but tend to abstract away the details, and depend on prior programming skills. In addition, campuses without dedicated network lab facilities may have limited ability to experiment with protocols on an existing institutional network. The Python-based Scapy package provides explicit, detailed control of the contents of header fields, and includes graphical visualization features that offer easy feedback. Programming ability is helpful but not necessary; the interactive Python environment permits step-by-step and guided exploration of the various protocols. Effective use of scapy requires root (administrator) privileges; a virtual machine environment such as that provided by Oracle VirtualBox allows complete control and access to the operating system. This talk is about scapy-based lab modules that the author is developing, which provide active, hands-on exposure to and manipulation of network headers. So far, a Transport-layer activity and a preliminary Link-layer activity have been written. The current activities will be discussed, along with ideas for additional modules."		[]	[{'start': 454, 'end': 483, 'span': 'limited ability to experiment', 'annotation': 'Research Values'}, {'start': 577, 'end': 603, 'span': 'explicit, detailed control', 'annotation': 'Research Values'}, {'start': 695, 'end': 708, 'span': 'easy feedback', 'annotation': 'Research Values'}, {'start': 865, 'end': 878, 'span': 'Effective use', 'annotation': 'Research Values'}, {'start': 1010, 'end': 1037, 'span': 'complete control and access', 'annotation': 'Research Values'}, {'start': 1151, 'end': 1167, 'span': 'active, hands-on', 'annotation': 'Research Values'}, {'start': 1372, 'end': 1390, 'span': 'additional modules', 'annotation': 'Research Values'}]
1002	11	The paper addresses the problem of forecasting consumer expenditure from social media data. Previous research of the topic exploited the intuition that search engine traffic reflects purchase intentions and constructed predictive models of consumer behaviour from search query volumes. In contrast, we derive predictors from explicit expressions of purchase intentions found in social media posts. Two types of predictors created from these expressions are explored: those based on word embeddings and those based on topical word clusters. We introduce a new clustering method, which takes into account temporal co-occurrence of words, in addition to their semantic similarity, in order to create predictors relevant to the forecasting problem. The predictors are evaluated against baselines that use only macroeconomic variables, and against models trained on search traffic data. Conducting experiments with three different regression methods on Facebook and Twitter data, we find that both word embeddings and word clusters help to reduce forecasting errors in comparison to purely macroeconomic models. In most experimental settings, the error reduction is statistically significant, and is comparable to error reduction achieved with search traffic variables.		[]	[{'start': 35, 'end': 67, 'span': 'forecasting consumer expenditure', 'annotation': 'Research Values'}, {'start': 219, 'end': 258, 'span': 'predictive models of consumer behaviour', 'annotation': 'Research Values'}, {'start': 309, 'end': 345, 'span': 'predictors from explicit expressions', 'annotation': 'Research Values'}, {'start': 555, 'end': 576, 'span': 'new clustering method', 'annotation': 'Research Values'}, {'start': 690, 'end': 716, 'span': 'create predictors relevant', 'annotation': 'Research Values'}, {'start': 1013, 'end': 1060, 'span': 'word clusters help to reduce forecasting errors', 'annotation': 'Research Values'}, {'start': 1142, 'end': 1157, 'span': 'error reduction', 'annotation': 'Research Values'}, {'start': 1209, 'end': 1233, 'span': 'error reduction achieved', 'annotation': 'Research Values'}]
1002	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.		[]	[{'start': 276, 'end': 284, 'span': 'superior', 'annotation': 'Research Values'}, {'start': 736, 'end': 753, 'span': 'able to reproduce', 'annotation': 'Research Values'}, {'start': 833, 'end': 845, 'span': 'not inferior', 'annotation': 'Research Values'}]
1002	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.		[]	[{'start': 49, 'end': 77, 'span': 'remarkable performance gains', 'annotation': 'Research Values'}, {'start': 177, 'end': 211, 'span': 'successively proposed for learning', 'annotation': 'Research Values'}, {'start': 489, 'end': 532, 'span': 'suffer from deficient semantic interactions', 'annotation': 'Research Values'}, {'start': 537, 'end': 574, 'span': 'fail to capture the semantic relation', 'annotation': 'Research Values'}, {'start': 647, 'end': 667, 'span': 'simple yet effective', 'annotation': 'Research Values'}, {'start': 774, 'end': 830, 'span': 'CLOWER implicitly encodes the coarse-grained information', 'annotation': 'Research Values'}, {'start': 974, 'end': 1008, 'span': 'great value in realistic scenarios', 'annotation': 'Research Values'}, {'start': 1210, 'end': 1270, 'span': 'superior performance of CLOWER over several state-of-the-art', 'annotation': 'Research Values'}]
1002	5	When a reader is first introduced to an entity, its referring expression must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how expressions that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a model to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30% absolute improvement over a majority-class baseline.		[]	[{'start': 771, 'end': 795, 'span': 'build a model to predict', 'annotation': 'Research Values'}, {'start': 878, 'end': 936, 'span': '10-30% absolute improvement over a majority-class baseline', 'annotation': 'Research Values'}]
1002	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.		[]	[{'start': 672, 'end': 723, 'span': 'develop a general framework for recommender systems', 'annotation': 'Research Values'}, {'start': 812, 'end': 846, 'span': 'constrained optimization framework', 'annotation': 'Research Values'}, {'start': 1232, 'end': 1294, 'span': 'efficient optimization solution for ranking and recommendation', 'annotation': 'Research Values'}, {'start': 1342, 'end': 1378, 'span': 'implement the framework at Uber Eats', 'annotation': 'Research Values'}, {'start': 1557, 'end': 1603, 'span': 'demonstrate the effectiveness of our framework', 'annotation': 'Research Values'}, {'start': 1697, 'end': 1733, 'span': 'framework has been deployed globally', 'annotation': 'Research Values'}]
1002	10	This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.		[]	[{'start': 52, 'end': 83, 'span': 'developing knowledge extraction', 'annotation': 'Research Values'}, {'start': 136, 'end': 186, 'span': 'constructing an entreprise-specific knowledge base', 'annotation': 'Research Values'}, {'start': 222, 'end': 250, 'span': 'entity detection and linking', 'annotation': 'Research Values'}, {'start': 252, 'end': 274, 'span': 'coreference resolution', 'annotation': 'Research Values'}, {'start': 280, 'end': 299, 'span': 'relation extraction', 'annotation': 'Research Values'}, {'start': 499, 'end': 511, 'span': 'new resource', 'annotation': 'Research Values'}, {'start': 590, 'end': 603, 'span': 'balanced data', 'annotation': 'Research Values'}]
1002	12	Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.		[]	[{'start': 41, 'end': 55, 'span': 'dated overview', 'annotation': 'Research Values'}, {'start': 248, 'end': 264, 'span': 'not reproducible', 'annotation': 'Research Values'}, {'start': 355, 'end': 375, 'span': 'not always available', 'annotation': 'Research Values'}, {'start': 522, 'end': 554, 'span': 'construct event-specific corpora', 'annotation': 'Research Values'}, {'start': 633, 'end': 661, 'span': 'relatively simple IR methods', 'annotation': 'Research Values'}, {'start': 752, 'end': 789, 'span': 'performance of various TLS algorithms', 'annotation': 'Research Values'}, {'start': 821, 'end': 841, 'span': 'match a stronger one', 'annotation': 'Research Values'}, {'start': 845, 'end': 875, 'span': 'employing a stronger IR method', 'annotation': 'Research Values'}, {'start': 1023, 'end': 1050, 'span': 'advocate for integrating IR', 'annotation': 'Research Values'}, {'start': 1100, 'end': 1124, 'span': 'common static background', 'annotation': 'Research Values'}]
1002	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.		[]	[{'start': 30, 'end': 72, 'span': 'improves upon the current state of the art', 'annotation': 'Research Values'}, {'start': 76, 'end': 123, 'span': 'increasing the usability and practical security', 'annotation': 'Research Values'}, {'start': 160, 'end': 199, 'span': 'More users are able to send and receive', 'annotation': 'Research Values'}, {'start': 217, 'end': 267, 'span': 'without mistakenly revealing sensitive information', 'annotation': 'Research Values'}, {'start': 327, 'end': 371, 'span': 'positively affect the usability and security', 'annotation': 'Research Values'}, {'start': 483, 'end': 539, 'span': 'high usability, few mistakes, and a strong understanding', 'annotation': 'Research Values'}, {'start': 622, 'end': 679, 'span': 'manual encryption has no effect on usability or security.', 'annotation': 'Research Values'}]
1002	13	Trust prediction, aiming to predict the trust relations between users in a social network, is a key to helping users discover the reliable information. Many trust prediction methods are proposed based on the low-rank assumption of a trust network. However, one typical property of the trust network is that the trust relations follow the power-law distribution, i.e., few users are trusted by many other users, while most tail users have few trustors. Due to these tail users, the fundamental low-rank assumption made by existing methods is seriously violated and becomes unrealistic. In this paper, we propose a simple yet effective method to address the problem of the violated low-rank assumption. Instead of discovering the low-rank component of the trust network alone, we learn a sparse component of the trust network to describe the tail users simultaneously. With both of the learned low-rank and sparse components, the trust relations in the whole network can be better captured. Moreover, the transitive closure structure of the trust relations is also integrated into our model. We then derive an effective iterative algorithm to infer the parameters of our model, along with the proof of correctness. Extensive experimental results on real-world trust networks demonstrate the superior performance of our proposed method over the state-of-the-arts.		[]	[{'start': 28, 'end': 55, 'span': 'predict the trust relations', 'annotation': 'Research Values'}, {'start': 613, 'end': 640, 'span': 'simple yet effective method', 'annotation': 'Research Values'}, {'start': 644, 'end': 663, 'span': 'address the problem', 'annotation': 'Research Values'}, {'start': 778, 'end': 802, 'span': 'learn a sparse component', 'annotation': 'Research Values'}, {'start': 972, 'end': 987, 'span': 'better captured', 'annotation': 'Research Values'}, {'start': 1098, 'end': 1137, 'span': 'derive an effective iterative algorithm', 'annotation': 'Research Values'}, {'start': 1141, 'end': 1161, 'span': 'infer the parameters', 'annotation': 'Research Values'}, {'start': 1289, 'end': 1309, 'span': 'superior performance', 'annotation': 'Research Values'}, {'start': 1333, 'end': 1360, 'span': 'over the state-of-the-arts.', 'annotation': 'Research Values'}]
1002	8	We study the problem of multi-dimensional revenue maximization when selling m items to a buyer that has additive valuations for them, drawn from a (possibly correlated) prior distribution. Unlike traditional Bayesian auction design, we assume that the seller has a very restricted knowledge of this prior: they only know the mean μj and an upper bound σj on the standard deviation of each item’s marginal distribution. Our goal is to design mechanisms that achieve good revenue against an ideal optimal auction that has full knowledge of the distribution in advance. Informally, our main contribution is a tight quantification of the interplay between the dispersity of the priors and the aforementioned robust approximation ratio. Furthermore, this can be achieved by very simple selling mechanisms. More precisely, we show that selling the items via separate price lotteries achieves an O(log r) approximation ratio where r = maxj(σj/μj) is the maximum coefficient of variation across the items. To prove the result, we leverage a price lottery for the single-item case. If forced to restrict ourselves to deterministic mechanisms, this guarantee degrades to O(r2). Assuming independence of the item valuations, these ratios can be further improved by pricing the full bundle. For the case of identical means and variances, in particular, we get a guarantee of O(log (r/m)) that converges to optimality as the number of items grows large. We demonstrate the optimality of the preceding mechanisms by providing matching lower bounds. Our tight analysis for the single-item deterministic case resolves an open gap from the work of Azar and Micali (ITCS’13). As a by-product, we also show how one can directly use our upper bounds to improve and extend previous results related to the parametric auctions of Azar et al. (SODA’13).		[]	[{'start': 42, 'end': 62, 'span': 'revenue maximization', 'annotation': 'Research Values'}, {'start': 457, 'end': 477, 'span': 'achieve good revenue', 'annotation': 'Research Values'}, {'start': 489, 'end': 510, 'span': 'ideal optimal auction', 'annotation': 'Research Values'}, {'start': 606, 'end': 643, 'span': 'tight quantification of the interplay', 'annotation': 'Research Values'}, {'start': 769, 'end': 799, 'span': 'very simple selling mechanisms', 'annotation': 'Research Values'}, {'start': 1139, 'end': 1166, 'span': 'guarantee degrades to O(r2)', 'annotation': 'Research Values'}, {'start': 1242, 'end': 1277, 'span': 'improved by pricing the full bundle', 'annotation': 'Research Values'}, {'start': 1350, 'end': 1375, 'span': 'guarantee of O(log (r/m))', 'annotation': 'Research Values'}, {'start': 1394, 'end': 1440, 'span': 'optimality as the number of items grows large.', 'annotation': 'Research Values'}, {'start': 1460, 'end': 1470, 'span': 'optimality', 'annotation': 'Research Values'}, {'start': 1512, 'end': 1520, 'span': 'matching', 'annotation': 'Research Values'}, {'start': 1593, 'end': 1613, 'span': 'resolves an open gap', 'annotation': 'Research Values'}, {'start': 1732, 'end': 1768, 'span': ' improve and extend previous results', 'annotation': 'Research Values'}]
1003	14	We introduce a novel dataset of real multi-destination trips booked through Booking.com's online travel platform. The dataset consists of 1.5 million reservations representing 359,000 unique journeys made across 39,000 destinations. As such, the data is particularly well suited to model sequential recommendation and retrieval problems in a high cardinality target space. To preserve user privacy and protect business-sensitive statistics, the data is fully anonymized, sampled and limited to five user origin markets. Even so, the dataset is representative of the general travel purchase behavior and therefore presents a uniquely valuable resource for Machine Learning and information retrieval researchers. This work provides an overview of the dataset. It reports several benchmark results for relevant recommendation problems obtained as part of the recently held Booking.com data challenge during the WSDM WebTour workshop.		[]	[{'start': 15, 'end': 20, 'span': 'novel', 'annotation': 'Research Values'}, {'start': 267, 'end': 372, 'span': 'well suited to model sequential recommendation and retrieval problems in a high cardinality target space.', 'annotation': 'Research Values'}, {'start': 376, 'end': 397, 'span': 'preserve user privacy', 'annotation': 'Research Values'}, {'start': 402, 'end': 440, 'span': 'protect business-sensitive statistics,', 'annotation': 'Research Values'}, {'start': 459, 'end': 469, 'span': 'anonymized', 'annotation': 'Research Values'}, {'start': 471, 'end': 478, 'span': 'sampled', 'annotation': 'Research Values'}, {'start': 483, 'end': 519, 'span': 'limited to five user origin markets.', 'annotation': 'Research Values'}, {'start': 544, 'end': 598, 'span': 'representative of the general travel purchase behavior', 'annotation': 'Research Values'}, {'start': 624, 'end': 641, 'span': 'uniquely valuable', 'annotation': 'Research Values'}]
1003	7	Government documents must be reviewed to identify and protect any sensitive information, such as personal information, before the documents can be released to the public. However, in the era of digital government documents, such as e-mail, traditional sensitivity review procedures are no longer practical, for example due to the volume of documents to be reviewed. Therefore, there is a need for new technology assisted review protocols to integrate automatic sensitivity classification into the sensitivity review process. Moreover, to effectively assist sensitivity review, such assistive technologies must incorporate reviewer feedback to enable sensitivity classifiers to quickly learn and adapt to the sensitivities within a collection, when the types of sensitivity are not known a priori. In this work, we present a thorough evaluation of active learning strategies for sensitivity review. Moreover, we present an active learning strategy that integrates reviewer feedback, from sensitive text annotations, to identify features of sensitivity that enable us to learn an effective sensitivity classifier (0.7 Balanced Accuracy) using significantly less reviewer effort, according to the sign test (\(p<0.01\)). Moreover, this approach results in a 51% reduction in the number of documents required to be reviewed to achieve the same level of classification accuracy, compared to when the approach is deployed without annotation features.		[]	[{'start': 286, 'end': 306, 'span': 'no longer practical,', 'annotation': 'Research Values'}, {'start': 441, 'end': 487, 'span': 'integrate automatic sensitivity classification', 'annotation': 'Research Values'}, {'start': 538, 'end': 549, 'span': 'effectively', 'annotation': 'Research Values'}, {'start': 610, 'end': 639, 'span': 'incorporate reviewer feedback', 'annotation': 'Research Values'}, {'start': 677, 'end': 690, 'span': 'quickly learn', 'annotation': 'Research Values'}, {'start': 695, 'end': 721, 'span': 'adapt to the sensitivities', 'annotation': 'Research Values'}, {'start': 824, 'end': 832, 'span': 'thorough', 'annotation': 'Research Values'}, {'start': 952, 'end': 981, 'span': 'integrates reviewer feedback,', 'annotation': 'Research Values'}, {'start': 1078, 'end': 1087, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 1135, 'end': 1176, 'span': 'using significantly less reviewer effort,', 'annotation': 'Research Values'}, {'start': 1265, 'end': 1379, 'span': 'reduction in the number of documents required to be reviewed to achieve the same level of classification accuracy,', 'annotation': 'Research Values'}]
1003	9	"ntroductory computer networks courses often include descriptive coverage of the network protocol headers. A straightforward listing of the headers and their meanings can lead to questions like ""Will this be on the test""? Programming exercises may involve selecting values for some aspects of a protocol but tend to abstract away the details, and depend on prior programming skills. In addition, campuses without dedicated network lab facilities may have limited ability to experiment with protocols on an existing institutional network. The Python-based Scapy package provides explicit, detailed control of the contents of header fields, and includes graphical visualization features that offer easy feedback. Programming ability is helpful but not necessary; the interactive Python environment permits step-by-step and guided exploration of the various protocols. Effective use of scapy requires root (administrator) privileges; a virtual machine environment such as that provided by Oracle VirtualBox allows complete control and access to the operating system. This talk is about scapy-based lab modules that the author is developing, which provide active, hands-on exposure to and manipulation of network headers. So far, a Transport-layer activity and a preliminary Link-layer activity have been written. The current activities will be discussed, along with ideas for additional modules."		[]	[{'start': 52, 'end': 105, 'span': 'descriptive coverage of the network protocol headers.', 'annotation': 'Research Values'}, {'start': 315, 'end': 341, 'span': 'abstract away the details,', 'annotation': 'Research Values'}, {'start': 346, 'end': 381, 'span': 'depend on prior programming skills.', 'annotation': 'Research Values'}, {'start': 454, 'end': 498, 'span': 'limited ability to experiment with protocols', 'annotation': 'Research Values'}, {'start': 577, 'end': 586, 'span': 'explicit,', 'annotation': 'Research Values'}, {'start': 587, 'end': 637, 'span': 'detailed control of the contents of header fields,', 'annotation': 'Research Values'}, {'start': 651, 'end': 709, 'span': 'graphical visualization features that offer easy feedback.', 'annotation': 'Research Values'}, {'start': 710, 'end': 759, 'span': 'Programming ability is helpful but not necessary;', 'annotation': 'Research Values'}, {'start': 803, 'end': 815, 'span': 'step-by-step', 'annotation': 'Research Values'}, {'start': 820, 'end': 864, 'span': 'guided exploration of the various protocols.', 'annotation': 'Research Values'}, {'start': 1010, 'end': 1026, 'span': 'complete control', 'annotation': 'Research Values'}, {'start': 1031, 'end': 1062, 'span': 'access to the operating system.', 'annotation': 'Research Values'}, {'start': 1151, 'end': 1158, 'span': 'active,', 'annotation': 'Research Values'}, {'start': 1159, 'end': 1176, 'span': 'hands-on exposure', 'annotation': 'Research Values'}, {'start': 1184, 'end': 1216, 'span': 'manipulation of network headers.', 'annotation': 'Research Values'}]
1003	11	The paper addresses the problem of forecasting consumer expenditure from social media data. Previous research of the topic exploited the intuition that search engine traffic reflects purchase intentions and constructed predictive models of consumer behaviour from search query volumes. In contrast, we derive predictors from explicit expressions of purchase intentions found in social media posts. Two types of predictors created from these expressions are explored: those based on word embeddings and those based on topical word clusters. We introduce a new clustering method, which takes into account temporal co-occurrence of words, in addition to their semantic similarity, in order to create predictors relevant to the forecasting problem. The predictors are evaluated against baselines that use only macroeconomic variables, and against models trained on search traffic data. Conducting experiments with three different regression methods on Facebook and Twitter data, we find that both word embeddings and word clusters help to reduce forecasting errors in comparison to purely macroeconomic models. In most experimental settings, the error reduction is statistically significant, and is comparable to error reduction achieved with search traffic variables.		[]	[{'start': 123, 'end': 202, 'span': 'exploited the intuition that search engine traffic reflects purchase intentions', 'annotation': 'Research Values'}, {'start': 302, 'end': 397, 'span': 'derive predictors from explicit expressions of purchase intentions found in social media posts.', 'annotation': 'Research Values'}, {'start': 708, 'end': 716, 'span': 'relevant', 'annotation': 'Research Values'}, {'start': 1035, 'end': 1060, 'span': 'reduce forecasting errors', 'annotation': 'Research Values'}, {'start': 1142, 'end': 1187, 'span': 'error reduction is statistically significant,', 'annotation': 'Research Values'}, {'start': 1195, 'end': 1264, 'span': 'comparable to error reduction achieved with search traffic variables.', 'annotation': 'Research Values'}]
1003	10	This paper presents an effort within our company of developing knowledge extraction pipeline for English, which can be further used for constructing an entreprise-specific knowledge base. We present a system consisting of entity detection and linking, coreference resolution, and relation extraction based on the Wikidata schema. We highlight existing challenges of knowledge extraction by evaluating the deployed pipeline on real-world data. We also make available a database, which can serve as a new resource for sentential relation extraction, and we underline the importance of having balanced data for training classification models.		[]	[{'start': 152, 'end': 171, 'span': 'entreprise-specific', 'annotation': 'Research Values'}, {'start': 390, 'end': 442, 'span': 'evaluating the deployed pipeline on real-world data.', 'annotation': 'Research Values'}, {'start': 590, 'end': 603, 'span': 'balanced data', 'annotation': 'Research Values'}]
1003	3	Many online personalization platforms today are recommending heterogeneous contents in a multi-sided marketplace consisting of consumers, merchants and other partners. For a recommender system to be successful in these contexts, it faces two main challenges. First, each side in the marketplace has different and potentially conflicting utilities. Recommending for a multi-sided marketplace therefore entails jointly optimizing multiple objectives with trade-offs. Second, the off-the-shelf recommendation algorithms are not applicable to the heterogeneous content space, where a recommendation item could be an aggregation of other recommendation items. In this work, we develop a general framework for recommender systems in a multi-sided marketplace with heterogeneous and hierarchical contents. We propose a constrained optimization framework with machine learning models for each objective as inputs, and a probabilistic structural model for users’ engagement patterns on heterogeneous contents. Our proposed structural modeling approach ensures consistent user experience across different levels of aggregation of the contents, and provides levels of transparency to the merchants and content providers. We further develop an efficient optimization solution for ranking and recommendation in large-scale online systems in real time. We implement the framework at Uber Eats, one of the largest online food delivery platforms in the world and a three-sided marketplace consisting of eaters, restaurant partners and delivery partners. Online experiments demonstrate the effectiveness of our framework in ranking heterogeneous contents and optimizing for the three sides in the marketplace. Our framework has been deployed globally as the recommendation algorithm for Uber Eats’ homepage.		[]	[{'start': 199, 'end': 209, 'span': 'successful', 'annotation': 'Research Values'}, {'start': 409, 'end': 464, 'span': 'jointly optimizing multiple objectives with trade-offs.', 'annotation': 'Research Values'}, {'start': 521, 'end': 571, 'span': 'not applicable to the heterogeneous content space,', 'annotation': 'Research Values'}, {'start': 682, 'end': 689, 'span': 'general', 'annotation': 'Research Values'}, {'start': 1051, 'end': 1133, 'span': 'consistent user experience across different levels of aggregation of the contents,', 'annotation': 'Research Values'}, {'start': 1147, 'end': 1209, 'span': 'levels of transparency to the merchants and content providers.', 'annotation': 'Research Values'}, {'start': 1232, 'end': 1241, 'span': 'efficient', 'annotation': 'Research Values'}, {'start': 1573, 'end': 1586, 'span': 'effectiveness', 'annotation': 'Research Values'}, {'start': 1607, 'end': 1637, 'span': 'ranking heterogeneous contents', 'annotation': 'Research Values'}, {'start': 1642, 'end': 1692, 'span': 'optimizing for the three sides in the marketplace.', 'annotation': 'Research Values'}, {'start': 1716, 'end': 1733, 'span': 'deployed globally', 'annotation': 'Research Values'}]
1003	12	Timeline summarization (TLS) generates a dated overview of real-world events based on event-specific corpora. The two standard datasets for this task were collected using Google searches for news reports on given events. Not only is this IR method not reproducible at different search times, it also uses components (such as document popularity) that are not always available for any large news corpus. It is unclear how TLS algorithms fare when provided with event corpora collected with varying IR methods. We therefore construct event-specific corpora from a large static background corpus, the newsroom dataset, using differing, relatively simple IR methods based on raw text alone. We show that the choice of IR method plays a crucial role in the performance of various TLS algorithms. A weak TLS algorithm can even match a stronger one by employing a stronger IR method in the data collection phase. Furthermore, the results of TLS systems are often highly sensitive to additional sentence filtering. We consequently advocate for integrating IR into the development of TLS systems and having a common static background corpus for evaluation of TLS systems.		[]	[{'start': 248, 'end': 291, 'span': 'not reproducible at different search times,', 'annotation': 'Research Values'}, {'start': 300, 'end': 402, 'span': 'uses components (such as document popularity) that are not always available for any large news corpus.', 'annotation': 'Research Values'}, {'start': 532, 'end': 546, 'span': 'event-specific', 'annotation': 'Research Values'}, {'start': 562, 'end': 567, 'span': 'large', 'annotation': 'Research Values'}, {'start': 568, 'end': 574, 'span': 'static', 'annotation': 'Research Values'}, {'start': 622, 'end': 631, 'span': 'differing', 'annotation': 'Research Values'}, {'start': 633, 'end': 650, 'span': 'relatively simple', 'annotation': 'Research Values'}, {'start': 793, 'end': 797, 'span': 'weak', 'annotation': 'Research Values'}, {'start': 829, 'end': 837, 'span': 'stronger', 'annotation': 'Research Values'}, {'start': 845, 'end': 905, 'span': 'employing a stronger IR method in the data collection phase.', 'annotation': 'Research Values'}, {'start': 956, 'end': 1006, 'span': 'highly sensitive to additional sentence filtering.', 'annotation': 'Research Values'}, {'start': 1100, 'end': 1106, 'span': 'common', 'annotation': 'Research Values'}, {'start': 1107, 'end': 1113, 'span': 'static', 'annotation': 'Research Values'}]
1003	4	Private Webmail 2.0 (Pwm 2.0) improves upon the current state of the art by increasing the usability and practical security of secure email for ordinary users. More users are able to send and receive encrypted emails without mistakenly revealing sensitive information. In this paper we describe four user interface traits that positively affect the usability and security of Pwm 2.0. In a user study involving 51 participants we validate that these interface modifications result in high usability, few mistakes, and a strong understanding of the protection provided to secure email messages. We also show that the use of manual encryption has no effect on usability or security.		[]	[{'start': 91, 'end': 100, 'span': 'usability', 'annotation': 'Research Values'}, {'start': 105, 'end': 123, 'span': 'practical security', 'annotation': 'Research Values'}, {'start': 183, 'end': 268, 'span': 'send and receive encrypted emails without mistakenly revealing sensitive information.', 'annotation': 'Research Values'}, {'start': 349, 'end': 358, 'span': 'usability', 'annotation': 'Research Values'}, {'start': 363, 'end': 371, 'span': 'security', 'annotation': 'Research Values'}, {'start': 483, 'end': 498, 'span': 'high usability,', 'annotation': 'Research Values'}, {'start': 499, 'end': 512, 'span': 'few mistakes,', 'annotation': 'Research Values'}, {'start': 517, 'end': 592, 'span': 'a strong understanding of the protection provided to secure email messages.', 'annotation': 'Research Values'}, {'start': 657, 'end': 666, 'span': 'usability', 'annotation': 'Research Values'}, {'start': 670, 'end': 678, 'span': 'security', 'annotation': 'Research Values'}]
1003	13	Trust prediction, aiming to predict the trust relations between users in a social network, is a key to helping users discover the reliable information. Many trust prediction methods are proposed based on the low-rank assumption of a trust network. However, one typical property of the trust network is that the trust relations follow the power-law distribution, i.e., few users are trusted by many other users, while most tail users have few trustors. Due to these tail users, the fundamental low-rank assumption made by existing methods is seriously violated and becomes unrealistic. In this paper, we propose a simple yet effective method to address the problem of the violated low-rank assumption. Instead of discovering the low-rank component of the trust network alone, we learn a sparse component of the trust network to describe the tail users simultaneously. With both of the learned low-rank and sparse components, the trust relations in the whole network can be better captured. Moreover, the transitive closure structure of the trust relations is also integrated into our model. We then derive an effective iterative algorithm to infer the parameters of our model, along with the proof of correctness. Extensive experimental results on real-world trust networks demonstrate the superior performance of our proposed method over the state-of-the-arts.		[]	[{'start': 541, 'end': 559, 'span': 'seriously violated', 'annotation': 'Research Values'}, {'start': 572, 'end': 583, 'span': 'unrealistic', 'annotation': 'Research Values'}, {'start': 613, 'end': 619, 'span': 'simple', 'annotation': 'Research Values'}, {'start': 624, 'end': 633, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 1108, 'end': 1117, 'span': 'effective', 'annotation': 'Research Values'}, {'start': 1213, 'end': 1222, 'span': 'Extensive', 'annotation': 'Research Values'}, {'start': 1247, 'end': 1272, 'span': 'real-world trust networks', 'annotation': 'Research Values'}, {'start': 1289, 'end': 1360, 'span': 'superior performance of our proposed method over the state-of-the-arts.', 'annotation': 'Research Values'}]
1003	8	We study the problem of multi-dimensional revenue maximization when selling m items to a buyer that has additive valuations for them, drawn from a (possibly correlated) prior distribution. Unlike traditional Bayesian auction design, we assume that the seller has a very restricted knowledge of this prior: they only know the mean μj and an upper bound σj on the standard deviation of each item’s marginal distribution. Our goal is to design mechanisms that achieve good revenue against an ideal optimal auction that has full knowledge of the distribution in advance. Informally, our main contribution is a tight quantification of the interplay between the dispersity of the priors and the aforementioned robust approximation ratio. Furthermore, this can be achieved by very simple selling mechanisms. More precisely, we show that selling the items via separate price lotteries achieves an O(log r) approximation ratio where r = maxj(σj/μj) is the maximum coefficient of variation across the items. To prove the result, we leverage a price lottery for the single-item case. If forced to restrict ourselves to deterministic mechanisms, this guarantee degrades to O(r2). Assuming independence of the item valuations, these ratios can be further improved by pricing the full bundle. For the case of identical means and variances, in particular, we get a guarantee of O(log (r/m)) that converges to optimality as the number of items grows large. We demonstrate the optimality of the preceding mechanisms by providing matching lower bounds. Our tight analysis for the single-item deterministic case resolves an open gap from the work of Azar and Micali (ITCS’13). As a by-product, we also show how one can directly use our upper bounds to improve and extend previous results related to the parametric auctions of Azar et al. (SODA’13).		[]	[{'start': 457, 'end': 477, 'span': 'achieve good revenue', 'annotation': 'Research Values'}, {'start': 606, 'end': 611, 'span': 'tight', 'annotation': 'Research Values'}, {'start': 774, 'end': 780, 'span': 'simple', 'annotation': 'Research Values'}, {'start': 1539, 'end': 1544, 'span': 'tight', 'annotation': 'Research Values'}, {'start': 1733, 'end': 1740, 'span': 'improve', 'annotation': 'Research Values'}, {'start': 1745, 'end': 1768, 'span': 'extend previous results', 'annotation': 'Research Values'}]
1003	6	"Subspace clustering has been widely applied to detect meaningful clusters in high-dimensional data spaces. A main challenge in subspace clustering is to quickly calculate a ""good"" affinity matrix. ℓ0, ℓ1, ℓ2 or nuclear norm regularization is used to construct the affinity matrix in many subspace clustering methods because of their theoretical guarantees and empirical success. However, they suffer from the following problems: (1) ℓ2 and nuclear norm regularization require very strong assumptions to guarantee a subspace-preserving affinity; (2) although ℓ1 regularization can be guaranteed to give a subspace-preserving affinity under certain conditions, it needs more time to solve a large-scale convex optimization problem; (3) ℓ0 regularization can yield a tradeoff between computationally efficient and subspace-preserving affinity by using the orthogonal matching pursuit (OMP) algorithm, but this still takes more time to search the solution in OMP when the number of data points is large. In order to overcome these problems, we first propose a learned OMP (LOMP) algorithm to learn a single hidden neural network (SHNN) to fast approximate the ℓ0code. We then exploit a sparse subspace clustering method based on ℓ0 code which is fast computed by SHNN. Two sufficient conditions are presented to guarantee that our method can give a subspace-preserving affinity. Experiments on handwritten digit and face clustering show that our method not only quickly computes the ℓ0 code, but also outperforms the relevant subspace clustering methods in clustering results. In particular, our method achieves the state-of-the-art clustering accuracy (94.32%) on MNIST."		[]	[{'start': 333, 'end': 355, 'span': 'theoretical guarantees', 'annotation': 'Research Values'}, {'start': 360, 'end': 378, 'span': 'empirical success.', 'annotation': 'Research Values'}, {'start': 468, 'end': 544, 'span': 'require very strong assumptions to guarantee a subspace-preserving affinity;', 'annotation': 'Research Values'}, {'start': 662, 'end': 729, 'span': 'needs more time to solve a large-scale convex optimization problem;', 'annotation': 'Research Values'}, {'start': 797, 'end': 806, 'span': 'efficient', 'annotation': 'Research Values'}, {'start': 811, 'end': 839, 'span': 'subspace-preserving affinity', 'annotation': 'Research Values'}, {'start': 913, 'end': 999, 'span': 'takes more time to search the solution in OMP when the number of data points is large.', 'annotation': 'Research Values'}, {'start': 1135, 'end': 1139, 'span': 'fast', 'annotation': 'Research Values'}, {'start': 1242, 'end': 1246, 'span': 'fast', 'annotation': 'Research Values'}, {'start': 1345, 'end': 1373, 'span': 'subspace-preserving affinity', 'annotation': 'Research Values'}, {'start': 1458, 'end': 1465, 'span': 'quickly', 'annotation': 'Research Values'}, {'start': 1497, 'end': 1549, 'span': 'outperforms the relevant subspace clustering methods', 'annotation': 'Research Values'}, {'start': 1611, 'end': 1648, 'span': ' state-of-the-art clustering accuracy', 'annotation': 'Research Values'}]
1003	1	With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.		[]	[{'start': 322, 'end': 385, 'span': 'only considered the representation of a single image-text post ', 'annotation': 'Research Values'}, {'start': 389, 'end': 463, 'span': 'failed to capture the global co-occurrence characteristics of the dataset.', 'annotation': 'Research Values'}, {'start': 649, 'end': 680, 'span': 'capture hidden representations.', 'annotation': 'Research Values'}, {'start': 739, 'end': 823, 'span': 'learn multimodal representations based on the global characteristics of the dataset.', 'annotation': 'Research Values'}, {'start': 916, 'end': 958, 'span': 'predict the sentiment of image-text pairs.', 'annotation': 'Research Values'}, {'start': 959, 'end': 968, 'span': 'Extensive', 'annotation': 'Research Values'}, {'start': 1000, 'end': 1018, 'span': 'publicly available', 'annotation': 'Research Values'}, {'start': 1044, 'end': 1057, 'span': 'effectiveness', 'annotation': 'Research Values'}]
1003	0	Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.		[]	[{'start': 371, 'end': 415, 'span': 'deals with perceptually complex environments', 'annotation': 'Research Values'}, {'start': 549, 'end': 602, 'span': 'control for specific factors that affect performance,', 'annotation': 'Research Values'}, {'start': 744, 'end': 788, 'span': 'reproduce classic results in modern settings', 'annotation': 'Research Values'}, {'start': 863, 'end': 892, 'span': 'rewards are sparse or delayed', 'annotation': 'Research Values'}, {'start': 910, 'end': 928, 'span': 'viable alternative', 'annotation': 'Research Values'}]
1003	5	When a reader is first introduced to an entity, its referring expression must describe the entity. For entities that are widely known, a single word or phrase often suffices. This paper presents the first study of how expressions that refer to the same entity develop over time. We track thousands of person and organization entities over 20 years of New York Times (NYT). As entities move from hearer-new (first introduction to the NYT audience) to hearer-old (common knowledge) status, we show empirically that the referring expressions along this trajectory depend on the type of the entity, and exhibit linguistic properties related to becoming common knowledge (e.g., shorter length, less use of appositives, more definiteness). These properties can also be used to build a model to predict how long it will take for an entity to reach hearer-old status. Our results reach 10-30% absolute improvement over a majority-class baseline.		[]	[{'start': 199, 'end': 204, 'span': 'first', 'annotation': 'Research Values'}, {'start': 878, 'end': 905, 'span': '10-30% absolute improvement', 'annotation': 'Research Values'}]
1003	2	Pre-trained Language Models (PLMs) have achieved remarkable performance gains across numerous downstream tasks in natural language understanding. Various Chinese PLMs have been successively proposed for learning better Chinese language representation. However, most current models use Chinese characters as inputs and are not able to encode semantic information contained in Chinese words. While recent pre-trained models incorporate both words and characters simultaneously, they usually suffer from deficient semantic interactions and fail to capture the semantic relation between words and characters. To address the above issues, we propose a simple yet effective PLM CLOWER, which adopts the Contrastive Learning Over Word and charactER representations. In particular, CLOWER implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters) through contrastive learning on multi-grained information. CLOWER is of great value in realistic scenarios since it can be easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines. Extensive experiments conducted on a range of downstream tasks demonstrate the superior performance of CLOWER over several state-of-the-art baselines.		[]	[{'start': 60, 'end': 77, 'span': 'performance gains', 'annotation': 'Research Values'}, {'start': 203, 'end': 251, 'span': 'learning better Chinese language representation.', 'annotation': 'Research Values'}, {'start': 281, 'end': 313, 'span': 'use Chinese characters as inputs', 'annotation': 'Research Values'}, {'start': 322, 'end': 389, 'span': 'not able to encode semantic information contained in Chinese words.', 'annotation': 'Research Values'}, {'start': 421, 'end': 475, 'span': ' incorporate both words and characters simultaneously,', 'annotation': 'Research Values'}, {'start': 501, 'end': 532, 'span': 'deficient semantic interactions', 'annotation': 'Research Values'}, {'start': 537, 'end': 604, 'span': 'fail to capture the semantic relation between words and characters.', 'annotation': 'Research Values'}, {'start': 781, 'end': 901, 'span': 'implicitly encodes the coarse-grained information (i.e., words) into the fine-grained representations (i.e., characters)', 'annotation': 'Research Values'}, {'start': 1025, 'end': 1130, 'span': 'easily incorporated into any existing fine-grained based PLMs without modifying the production pipelines.', 'annotation': 'Research Values'}, {'start': 1131, 'end': 1140, 'span': 'Extensive', 'annotation': 'Research Values'}, {'start': 1163, 'end': 1193, 'span': 'on a range of downstream tasks', 'annotation': 'Research Values'}, {'start': 1210, 'end': 1230, 'span': 'superior performance', 'annotation': 'Research Values'}]
