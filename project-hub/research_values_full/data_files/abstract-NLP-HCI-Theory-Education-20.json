{"id": "0", "text": ["Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance."]}
{"id": "1", "text": ["This paper describes the continuation of a project that aims at establishing an interoperable annotation schema for quantification phenomena as part of the ISO suite of standards for semantic annotation, known as the Semantic Annotation Framework. After a break, caused by the Covid-19 pandemic, the project was relaunched in early 2022 with a second working draft of an annotation scheme, which is discussed in this paper. Keywords: semantic annotation, quantification, interoperability, annotation schema, ISO standard"]}
{"id": "2", "text": ["Many deep learning architectures have been proposed to model the compositionality in text sequences, requiring substantial number of parameters and expensive computations. However, there has not been a rigorous evaluation regarding the added value of sophisticated compositional functions. In this paper, we conduct a point-by-point comparative study between Simple Word-Embedding-based Models (SWEMs), consisting of parameter-free pooling operations, relative to word-embedding-based RNN/CNN models. Surprisingly, SWEMs exhibit comparable or even superior performance in the majority of cases considered. Based upon this understanding, we propose two additional pooling strategies over learned word embeddings: (i) a max-pooling operation for improved interpretability; and (ii) a hierarchical pooling operation, which preserves spatial (n-gram) information within text sequences. We present experiments on 17 datasets encompassing three tasks: (i) (long) document classification; (ii) text sequence matching; and (iii) short text tasks, including classification and tagging."]}
{"id": "3", "text": ["While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled."]}
{"id": "4", "text": ["This paper describes our model for the reading comprehension task of the MRQA shared task. We propose CLER, which stands for Cross-task Learning with Expert Representation for the generalization of reading and understanding. To generalize its capabilities, the proposed model is composed of three key ideas: multi-task learning, mixture of experts, and ensemble. In-domain datasets are used to train and validate our model, and other out-of-domain datasets are used to validate the generalization of our model\u2019s performances. In a submission run result, the proposed model achieved an average F1 score of 66.1 % in the out-of-domain setting, which is a 4.3 percentage point improvement over the official BERT baseline model."]}
{"id": "5", "text": ["Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT."]}
{"id": "6", "text": ["Recent work on question generation has largely focused on factoid questions such as who, what, where, when about basic facts. Generating open-ended why, how, what, etc. questions that require long-form answers have proven more difficult. To facilitate the generation of open-ended questions, we propose CONSISTENT, a new end-to-end system for generating open-ended questions that are answerable from and faithful to the input text. Using news articles as a trustworthy foundation for experimentation, we demonstrate our model's strength over several baselines using both automatic and human=based evaluations. We contribute an evaluation dataset of expert-generated open-ended questions.We discuss potential downstream applications for news media organizations."]}
{"id": "7", "text": ["The automatic generation of Multiple Choice Questions (MCQ) has the potential to reduce the time educators spend on student assessment significantly. However, existing evaluation metrics for MCQ generation, such as BLEU, ROUGE, and METEOR, focus on the n-gram based similarity of the generated MCQ to the gold sample in the dataset and disregard their educational value.They fail to evaluate the MCQ\u2019s ability to assess the student\u2019s knowledge of the corresponding target fact. To tackle this issue, we propose a novel automatic evaluation metric, coined Knowledge Dependent Answerability (KDA), which measures the MCQ\u2019s answerability given knowledge of the target fact. Specifically, we first show how to measure KDA based on student responses from a human survey.Then, we propose two automatic evaluation metrics, KDA_disc and KDA_cont, that approximate KDA by leveraging pre-trained language models to imitate students\u2019 problem-solving behavior.Through our human studies, we show that KDA_disc and KDA_soft have strong correlations with both (1) KDA and (2) usability in an actual classroom setting, labeled by experts. Furthermore, when combined with n-gram based similarity metrics, KDA_disc and KDA_cont are shown to have a strong predictive power for various expert-labeled MCQ quality measures."]}
{"id": "8", "text": ["Knowledge distillation is a critical technique to transfer knowledge between models, typically from a large model (the teacher) to a more fine-grained one (the student). The objective function of knowledge distillation is typically the cross-entropy between the teacher and the student\u2019s output distributions. However, for structured prediction problems, the output space is exponential in size; therefore, the cross-entropy objective becomes intractable to compute and optimize directly. In this paper, we derive a factorized form of the knowledge distillation objective for structured prediction, which is tractable for many typical choices of the teacher and student models. In particular, we show the tractability and empirical effectiveness of structural knowledge distillation between sequence labeling and dependency parsing models under four different scenarios: 1) the teacher and student share the same factorization form of the output structure scoring function; 2) the student factorization produces more fine-grained substructures than the teacher factorization; 3) the teacher factorization produces more fine-grained substructures than the student factorization; 4) the factorization forms from the teacher and the student are incompatible."]}
{"id": "9", "text": ["Humans can learn to operate the user interface (UI) of an application by reading an instruction manual or how-to guide. Along with text, these resources include visual content such as UI screenshots and images of application icons referenced in the text. We explore how to leverage this data to learn generic visio-linguistic representations of UI screens and their components. These representations are useful in many real applications, such as accessibility, voice navigation, and task automation. Prior UI representation models rely on UI metadata (UI trees and accessibility labels), which is often missing, incompletely defined, or not accessible. We avoid such a dependency, and propose Lexi, a pre-trained vision and language model designed to handle the unique features of UI screens, including their text richness and context sensitivity. To train Lexi we curate the UICaption dataset consisting of 114k UI images paired with descriptions of their functionality. We evaluate Lexi on four tasks: UI action entailment, instruction-based UI image retrieval, grounding referring expressions, and UI entity recognition."]}
{"id": "10", "text": ["Transformer models are expensive to fine-tune, slow for inference, and have large storage requirements. Recent approaches tackle these shortcomings by training smaller models, dynamically reducing the model size, and by training light-weight adapters. In this paper, we propose AdapterDrop, removing adapters from lower transformer layers during training and inference, which incorporates concepts from all three directions. We show that AdapterDrop can dynamically reduce the computational overhead when performing inference over multiple tasks simultaneously, with minimal decrease in task performances. We further prune adapters from AdapterFusion, which improves the inference efficiency while maintaining the task performances entirely."]}
{"id": "11", "text": ["Structured document understanding has attracted considerable attention and made significant progress recently, owing to its crucial role in intelligent document processing. However, most existing related models can only deal with the document data of specific language(s) (typically English) included in the pre-training collection, which is extremely limited. To address this issue, we propose a simple yet effective Language-independent Layout Transformer (LiLT) for structured document understanding. LiLT can be pre-trained on the structured documents of a single language and then directly fine-tuned on other languages with the corresponding off-the-shelf monolingual/multilingual pre-trained textual models. Experimental results on eight languages have shown that LiLT can achieve competitive or even superior performance on diverse widely-used downstream benchmarks, which enables language-independent benefit from the pre-training of document layout structure. Code and model are publicly available at https://github.com/jpWang/LiLT."]}
{"id": "12", "text": ["The current aspect extraction methods suffer from boundary errors. In general, these errors lead to a relatively minor difference between the extracted aspects and the ground-truth. However, they hurt the performance severely. In this paper, we propose to utilize a pointer network for repositioning the boundaries. Recycling mechanism is used, which enables the training data to be collected without manual intervention. We conduct the experiments on the benchmark datasets SE14 of laptop and SE14-16 of restaurant. Experimental results show that our method achieves substantial improvements over the baseline, and outperforms state-of-the-art methods."]}
{"id": "13", "text": ["While data augmentation is an important trick to boost the accuracy of deep learning methods in computer vision tasks, its study in natural language tasks is still very limited. In this paper, we present a novel data augmentation method for neural machine translation.Different from previous augmentation methods that randomly drop, swap or replace words with other words in a sentence, we softly augment a randomly chosen word in a sentence by its contextual mixture of multiple related words. More accurately, we replace the one-hot representation of a word by a distribution (provided by a language model) over the vocabulary, i.e., replacing the embedding of this word by a weighted combination of multiple semantically similar words. Since the weights of those words depend on the contextual information of the word to be replaced,the newly generated sentences capture much richer information than previous augmentation methods. Experimental results on both small scale and large scale machine translation data sets demonstrate the superiority of our method over strong baselines."]}
{"id": "14", "text": ["Recently, neural network models have achieved consistent improvements in statistical machine translation. However, most networks only use one-hot encoded input vectors of words as their input. In this work, we investigated the exponentially decaying bag-of-words input features for feed-forward neural network translation models and proposed to train the decay rates along with other weight parameters. This novel bag-of-words model improved our phrase-based state-of-the-art system, which already includes a neural network translation model, by up to 0.5% BLEU and 0.6% TER on three different translation tasks and even achieved a similar performance to the bidirectional LSTM translation model."]}
{"id": "15", "text": ["We present the shared task on Fine-Grained Propaganda Detection, which was organized as part of the NLP4IF workshop at EMNLP-IJCNLP 2019. There were two subtasks. FLC is a fragment-level task that asks for the identification of propagandist text fragments in a news article and also for the prediction of the specific propaganda technique used in each such fragment (18-way classification task). SLC is a sentence-level binary classification task asking to detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri.org/nlp4if-shared-task/."]}
{"id": "16", "text": ["(T)ACSA tasks, including aspect-category sentiment analysis (ACSA) and targeted aspect-category sentiment analysis (TACSA), aims at identifying sentiment polarity on predefined categories. Incremental learning on new categories is necessary for (T)ACSA real applications. Though current multi-task learning models achieve good performance in (T)ACSA tasks, they suffer from catastrophic forgetting problems in (T)ACSA incremental learning tasks. In this paper, to make multi-task learning feasible for incremental learning, we proposed Category Name Embedding network (CNE-net). We set both encoder and decoder shared among all categories to weaken the catastrophic forgetting problem. Besides the origin input sentence, we applied another input feature, i.e., category name, for task discrimination. Our model achieved state-of-the-art on two (T)ACSA benchmark datasets. Furthermore, we proposed a dataset for (T)ACSA incremental learning and achieved the best performance compared with other strong baselines."]}
{"id": "17", "text": ["Slot filling and intent detection are two main tasks in spoken language understanding (SLU) system. In this paper, we propose a novel non-autoregressive model named SlotRefine for joint intent detection and slot filling. Besides, we design a novel two-pass iteration mechanism to handle the uncoordinated slots problem caused by conditional independence of non-autoregressive model. Experiments demonstrate that our model significantly outperforms previous models in slot filling task, while considerably speeding up the decoding (up to X 10.77). In-depth analyses show that 1) pretraining schemes could further enhance our model; 2) two-pass mechanism indeed remedy the uncoordinated slots."]}
{"id": "18", "text": ["This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French. We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero system and NMT decoding alone. We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence. We give detailed analysis for the results of the multimodal NMT model. Our system obtained the first place for the English-to-French task according to human evaluation."]}
{"id": "19", "text": ["We introduce Dynatask: an open source system for setting up custom NLP tasks that aims to greatly lower the technical knowledge and effort required for hosting and evaluating state-of-the-art NLP models, as well as for conducting model in the loop data collection with crowdworkers. Dynatask is integrated with Dynabench, a research platform for rethinking benchmarking in AI that facilitates human and model in the loop data collection and evaluation. To create a task, users only need to write a short task configuration file from which the relevant web interfaces and model hosting infrastructure are automatically generated. The system is available at https://dynabench.org/ and the full library can be found at https://github.com/facebookresearch/dynabench."]}
{"id": "20", "text": ["Board games present accessibility barriers for players with visual impairment since they often employ visuals alone to communicate gameplay information. Our research focuses on board game accessibility for those with visual impairment. This paper describes a three-phase study conducted to develop board game accessibility adaptation guidelines. These guidelines were developed through a user-centered design approach that included in-depth interviews and a series of user studies using two adapted board games. Our findings indicate that participants with and without visual impairment were able to play the adapted games, exhibiting a balanced experience whereby participants had complete autonomy and were provided with equal chances of victory. Our paper also contributes to the game and accessibility communities through the development of adaptation guidelines that allow board games to become inclusive irrespective of a player's visual impairment."]}
{"id": "21", "text": ["A prominent approach to combating online misinformation is to debunk false content. Here we investigate downstream consequences of social corrections on users\u2019 subsequent sharing of other content. Being corrected might make users more attentive to accuracy, thus improving their subsequent sharing. Alternatively, corrections might not improve subsequent sharing - or even backfire - by making users feel defensive, or by shifting their attention away from accuracy (e.g., towards various social factors). We identified N=2,000 users who shared false political news on Twitter, and replied to their false tweets with links to fact-checking websites. We find causal evidence that being corrected decreases the quality, and increases the partisan slant and language toxicity, of the users\u2019 subsequent retweets (but has no significant effect on primary tweets). This suggests that being publicly corrected by another user shifts one's attention away from accuracy - presenting an important challenge for social correction approaches."]}
{"id": "22", "text": ["Most hospitals make efforts to provide socio-emotional support for patients and their families during care. In order to expand the service provided by certified child life specialists, we created a social robot and a virtual avatar that augment part of the care CCLS offers to patients by engaging pediatric patients in playful interactions and promoting their socio-emotional wellbeing. We ran a randomized controlled trial in a form of a Wizard-of-Oz study at a local pediatric hospital to study how three different interactive media (a plush teddy bear, a virtual agent on a screen, and a social robot) influence the pediatric patient's affect, joyful play, and social interactions with others. Behavioral analyses of verbal utterance transcriptions and children's physical behavior revealed that the social robot is most effective in producing socially energetic conversations as well as increasing positivity and promoting multi-party interactions. The virtual avatar was socially engaging but children tended to attend more exclusively to a virtual avatar and were less responsive to others. The plush toy was least engaging of the three interventions, but children touched it the most. Based on these findings, we recommend use cases for each agent appropriate for individual pediatric patients' health conditions and needs. These analyses of behavioral data suggest the benefit of deploying a physically embodied social robot in pediatric inpatient-care contexts on young patients'; social and emotional wellbeing."]}
{"id": "23", "text": ["HCI researchers work within spaces of possibility for potential designs of technology. New methods (e.g., user centrism); expected types of interaction (user with device); and potential applications (urban navigation) can extend the boundaries of these possibilities. However, structural and systemic factors can also foreclose them. A recent wide and shallow survey of 116 individuals involved in technology development across 26 countries in sub-Saharan Africa reveals how factors of political economy significantly impact upon technological possibilities. Monopolies, international power dynamics, race, and access to capital open or constrain technological possibilities at least as much as device-centric or user-focused constraints do. Though their thrust may have been anticipated by reference to political economic trends, the structural constraints we found were underestimated by technologists even a decade ago. We discuss the implications for technology development in Africa and beyond."]}
{"id": "24", "text": ["The Technical Reasoning hypothesis in cognitive neuroscience posits that humans engage in physical tool use by reasoning about mechanical interactions among objects. By modeling the use of objects as tools based on their abstract properties, this theory explains how tools can be re-purposed beyond their assigned function. This paper assesses the relevance of Technical Reasoning to digital tool use. We conducted an experiment with 16 participants that forced them to re-purpose commands to complete a text layout task. We analyzed self-reported scores of creative personality and experience with text editing, and found a significant association between re-purposing performance and creativity, but not with experience. Our results suggest that while most participants engaged in Technical Reasoning to re-purpose digital tools, some experienced \u201cfunctional fixedness.\u201d This work contributes Technical Reasoning as a theoretical model for the design of digital tools."]}
{"id": "25", "text": ["Users with visual impairments find it difficult to enjoy real-time 2D interactive applications on the touchscreen. Touchscreen applications such as sports games often require simultaneous recognition of and interaction with multiple moving targets through vision. To mitigate this issue, we propose ThroughHand, a novel tactile interaction that enables users with visual impairments to interact with multiple dynamic objects in real time. We designed the ThroughHand interaction to utilize the potential of the human tactile sense that spatially registers both sides of the hand with respect to each other. ThroughHand allows interaction with multiple objects by enabling users to perceive the objects using the palm while providing a touch input space on the back of the same hand. A user study verified that ThroughHand enables users to locate stimuli on the palm with a margin of error of approximately 13 mm and effectively provides a real-time 2D interaction experience for users with visual impairments."]}
{"id": "26", "text": ["How effectively do we adhere to nudges and interventions that help us control our online browsing habits? If we have a temporary lapse and disable the behavior change system, do we later resume our adherence, or has the dam broken? In this paper, we investigate these questions through log analyses of 8,000+ users on HabitLab, a behavior change platform that helps users reduce their time online. We find that, while users typically begin with high-challenge interventions, over time they allow themselves to slip into easier and easier interventions. Despite this, many still expect to return to the harder interventions imminently: they repeatedly choose to be asked to change difficulty again on the next visit, declining to have the system save their preference for easy interventions."]}
{"id": "27", "text": ["Throughout the world, organizations empower youth to participate in civic engagement to impact social change, and adult-youth collaborations are instrumental to the success of such initiatives. However, little is known about how technology supports this activism work, despite the fact that tools such as Social Networking Applications (SNAs) are increasingly being leveraged in such contexts. We report results from a qualitative study of SNA use within a youth empowerment organization. Using the analytical lens of object-oriented publics, our findings reveal opportunities and challenges that youth and staff face when they use SNAs. We describe the illegibility of youth outreach efforts on SNAs, and how this illegibility complicated staff attempts to hold youth accountable. We also characterize how youth and staff differed in what they felt were socially appropriate uses of SNA features, and tensions that arose in the co-use of these tools. We conclude with implications for the design of collaborative technologies that support youth-led activism in organizational contexts."]}
{"id": "28", "text": ["A long-standing challenge of video-mediated communication systems is to correctly represent remote participant gaze direction in local environments. A telepresence robot with a movable display that shows the face of a remote participant is a promising approach for solving this issue. Researchers generally consider that display orientation is effective for local participants to properly estimate the gaze direction of remote participants. We investigate how subjects estimate gaze direction of a remote participant (\"Looker\") when his/her face is displayed on a rotatable flat display. Our experiment reveals that both the Looker's head-eye rotation in the display and display rotation affect subject estimation, but the effect of the display rotation is relatively small. Furthermore, we reveal that subjects tend to overestimate Looker gaze direction. Based on our results, we propose a design implication for a telepresence robot to reduce overestimation and properly represent the remote participant gaze direction."]}
{"id": "29", "text": ["Sensing interfaces relying on head or facial gestures provide effective solutions for hands-free scenarios. Most of these interfaces utilize sensors attached to the face, as well as into the mouth, being either obtrusive or limited in input bandwidth. In this paper, we propose ChewIt -- a novel intraoral input interface. ChewIt resembles an edible object that allows users to perform various hands-free input operations, both simply and discreetly. Our design is informed by a series of studies investigating the implications of shape, size, locations for comfort, discreetness, maneuverability, and obstructiveness. Additionally, we evaluated potential gestures that users could use to interact with such an intraoral interface."]}
{"id": "30", "text": ["We present a computational model to predict users' spatio-temporal visual attention on WIMP-style (windows, icons, menus, pointer) graphical user interfaces. Like existing models of bottom-up visual attention in computer vision, our model does not require any eye tracking equipment. Instead, it predicts attention solely using information available to the interface, specifically users' mouse and keyboard input as well as the UI components they interact with. To study our model in a principled way, we further introduce a method to synthesize user interface layouts that are functionally equivalent to real-world interfaces, such as from Gmail, Facebook, or GitHub. We first quantitatively analyze attention allocation and its correlation with user input and UI components using ground-truth gaze, mouse, and keyboard data of 18 participants performing a text editing task. We then show that our model predicts attention maps more accurately than state-of-the-art methods. Our results underline the significant potential of spatio-temporal attention modeling for user interface evaluation, optimization, or even simulation."]}
{"id": "31", "text": ["Introducing interactivity to films has proven a longstanding and difficult challenge due to their narrative-driven, linear and theatre-based nature. Previous research has suggested that Brain-Computer Interfaces (BCI) may be a promising approach but also revealed a tension between being immersed in the film and thinking about control. We report a performance-led and in-the-wild study of a BCI film called The MOMENT covering its design rationale and how it was experienced by the public as controllers, non-controllers and repeat viewers. Our findings suggest that BCI movies should be designed to be credibly controllable, generate personal versions, be watchable as linear films, encourage repeat viewing and fit the medium of cinema. They also reveal how viewers appreciated the sense of editing their own personal cuts, suggesting a new stance on introducing interactivity into lean-back media in which filmmakers release editorial control to users to make their own versions."]}
{"id": "32", "text": ["Gaze estimation has widespread applications. However, little work has explored gaze estimation on smartphones, even though they are fast becoming ubiquitous. This paper presents ScreenGlint, a novel approach which exploits the glint (reflection) of the screen on the user's cornea for gaze estimation, using only the image captured by the front-facing camera. We first conduct a user study on common postures during smartphone use. We then design an experiment to evaluate the accuracy of ScreenGlint under varying face-to-screen distances. An in-depth evaluation involving multiple users is conducted and the impact of head pose variations is investigated. ScreenGlint achieves an overall angular error of 2.44\u00ba without head pose variations, and 2.94\u00ba with head pose variations. Our technique compares favorably to state-of-the-art research works, indicating that the glint of the screen is an effective and practical cue to gaze estimation on the smartphone platform. We believe that this work can open up new possibilities for practical and ubiquitous gaze-aware applications."]}
{"id": "33", "text": ["In this paper, we present a drawing workstation for blind people using a two-dimensional tactile pin-matrix display for in- and output. Four different input modalities, namely menu-based, gesture-based, freehand-stylus and a Time-of-Flight (ToF) depth segmentation of real-world object silhouettes, are utilized to create graphical shapes. Users can freely manipulate shapes after creation. Twelve blind users evaluated and compared the four image creation modalities. During evaluation, participants had to copy four different images. The results show that all modalities are highly appropriate for non-visual drawing tasks. There is no generally preferred drawing modality, but most participants rated the robust and well-known menu-based interaction as very good. Furthermore, menu was second in performance and the most accurate drawing modality. Our evaluation demonstrated direct manipulation works well for blind users at the position of the reading hand. In general, our drawing tool allows blind users to create appealing images."]}
{"id": "34", "text": ["Linguists use annotated text collections to validate, refute and refine a hypothesis about the written language. This research requires the creation and analysis of complex queries which are often above the technical expertise of the domain users. In this paper, we present a tool-design which enables language researchers to easily query annotated text corpora and conduct a comparative multi-faceted analysis on a single screen. The results of the iterative design process, including requirement analysis, multiple prototyping and user evaluation sessions, and expert reviews, are documented in detail. Our tool, called CorpSum, shows a 43.12 point increase in the mean SUS score in a randomized within-subjects test and an improvement of 3.18 times in mean task completion duration compared to a conventional solution. Two detailed case studies with linguists demonstrate a significant improvement for solving the real-world problems of the domain users."]}
{"id": "35", "text": ["Many working professionals commute via public transit, yet they have limited tools for learning about their urban neighborhoods and fellow commuters. We designed a location-based game called City Explorer to investigate how transit commuters capture, share, and view community information that is specifically tied to locations. Through a four-week field study, we found that participants valued the increased awareness of their personal travel routines that they gained through City Explorer. When viewing community information, they preferred information that was factual rather than opinion-based and was presented at the start and end of their commutes. Participants found less value in connecting with other transit riders because transit rides were often seen as opportunities to disengage from others. We discuss how location-based technologies can be designed to display factual community information before, during, and at the end of transit commutes."]}
{"id": "36", "text": ["Laser cutter users face difficulties distinguishing between visually similar materials. This can lead to problems, such as using the wrong power/speed settings or accidentally cutting hazardous materials. To support users, we present SensiCut, an integrated material sensing platform for laser cutters. SensiCut enables material awareness beyond what users are able to see and reliably differentiates among similar-looking types. It achieves this by detecting materials\u2019 surface structures using speckle sensing and deep learning. SensiCut consists of a compact hardware add-on for laser cutters and a user interface that integrates material sensing into the laser cutting workflow. In addition to improving the traditional workflow and its safety1, SensiCut enables new applications, such as automatically partitioning designs when engraving on multi-material objects or adjusting their geometry based on the kerf of the identified material. We evaluate SensiCut\u2019s accuracy for different types of materials under different sheet orientations and illumination conditions."]}
{"id": "37", "text": ["This paper presents FlexHaptics, a design method for creating custom haptic input interfaces. Our approach leverages planar compliant structures whose force-deformation relationship can be altered by adjusting the geometries. Embedded with such structures, a FlexHaptics module exerts a fine-tunable haptic effect (i.e., resistance, detent, or bounce) along a movement path (i.e., linear, rotary, or ortho-planar). These modules can work separately or combine into an interface with complex movement paths and haptic effects. To enable the parametric design of FlexHaptic modules, we provide a design editor that converts user-specified haptic properties into underlying mechanical structures of haptic modules. We validate our approach and demonstrate the potential of FlexHaptic modules through six application examples, including a slider control for a painting application and a piano keyboard interface on touchscreens, a tactile low vision timer, VR game controllers, and a compound input device of a joystick and a two-step button."]}
{"id": "38", "text": ["During the last decade, people have started to experiment with insertable technology like RFID or NFC chips and use them for e.g. identification. However, little is known about how people in fact interact with and adapt insertables. We conducted a video analysis of 122 YouTube videos to gain insight into the interaction with the insertables. Second, we implemented an online survey to complement our data from the video analysis. Our findings show that there are many opportunities for interaction with insertables both for task-oriented and creative purposes. However, there are also multiple challenges and obstacles as well as side effects and health concerns. Our findings conclude that the current infrastructure is not ready to support the use of insertables yet, and we discuss implications of this."]}
{"id": "39", "text": ["Smart Donations is a blockchain-based platform that offers users \u2018contracts\u2019 that donate funds to certain causes in response to real-world events e.g., whenever an earthquake is detected or an activist tweets about refugees. We designed Smart donations with Oxfam Australia, trialled it for 8-weeks with 86 people, recorded platform analytics and qualitatively analysed questionnaires and interviews about user experiences. Temporal qualities emerge when automation enforces conditions that contributed to participants\u2019 awareness of events that are usually unconscious, and senses of immediacy in contributing to crisis response and ongoing involvement in situations far-away while awaiting conditions to be met. We suggest data driven automation can reveal diverse temporal registers, in real-world phenomena, sociality, morality and everyday life, which contributes to experiencing a \u2018right time\u2019 to donate that is not limited to productivity or efficiency. Thus, we recommend a sensitivity to right time in designing for multiple temporalities in FinTech more generally."]}
{"id": "40", "text": ["We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and accounts for potential serial dependence by including block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods. When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score."]}
{"id": "41", "text": ["We study the combinatorial pure exploration problem Best-Set in stochastic multi-armed bandits. In a Best-Set instance, we are given $n$ arms with unknown reward distributions, as well as a family $\\mathcal{F}$ of feasible subsets over the arms. Our goal is to identify the feasible subset in $\\mathcal{F}$ with the maximum total mean using as few samples as possible. The problem generalizes the classical best arm identification problem and the top-$k$ arm identification problem, both of which have attracted significant attention in recent years. We provide a novel instance-wise lower bound for the sample complexity of the problem, as well as a nontrivial sampling algorithm, matching the lower bound up to a factor of $\\ln|\\mathcal{F}|$. For an important class of combinatorial families, we also provide polynomial time implementation of the sampling algorithm, using the equivalence of separation and optimization for convex program, and approximate Pareto curves in multi-objective optimization. We also show that the $\\ln|\\mathcal{F}|$ factor is inevitable in general through a nontrivial lower bound construction. Our results significantly improve several previous results for several important combinatorial constraints, and provide a tighter understanding of the general Best-Set problem.  We further introduce an even more general problem, formulated in geometric terms. We are given $n$ Gaussian arms with unknown means and unit variance. Consider the $n$-dimensional Euclidean space $\\mathbb{R}^n$, and a collection $\\mathcal{O}$ of disjoint subsets. Our goal is to determine the subset in $\\mathcal{O}$ that contains the $n$-dimensional vector of the means. The problem generalizes most pure exploration bandit problems studied in the literature. We provide the first nearly optimal sample complexity upper and lower bounds for the problem."]}
{"id": "42", "text": ["Depth separation results propose a possible theoretical explanation for the benefits of deep neural networks over shallower architectures, establishing that the former possess superior approximation capabilities. However, there are no known results in which the deeper architecture leverages this advantage into a provable optimization guarantee. We prove that when the data are generated by a distribution with radial symmetry which satisfies some mild assumptions, gradient descent can efficiently learn ball indicator functions using a depth 2 neural network with two layers of sigmoidal activations, and where the hidden layer is held fixed throughout training. By building on and refining existing techniques for approximation lower bounds of neural networks with a single layer of non-linearities, we show that there are d-dimensional radial distributions on the data such that ball indicators cannot be learned efficiently by any algorithm to accuracy better than \u03a9(d\u22124), nor by a standard gradient descent implementation to accuracy better than a constant. These results establish what is to the best of our knowledge, the first optimization-based separations where the approximation benefits of the stronger architecture provably manifest in practice. Our proof technique introduces new tools and ideas that may be of independent interest in the theoretical study of both the approximation and optimization of neural networks."]}
{"id": "43", "text": ["We solve an open question from Lu et al. [2017], by showing that any target network with inputs in R d can be approximated by a width O ( d ) network (independent of the target network\u2019s architecture), whose number of parameters is essentially larger only by a linear factor. In light of previous depth separation theorems, which imply that a similar result cannot hold when the roles of width and depth are interchanged, it follows that depth plays a more signi\ufb01cant role than width in the expressive power of neural networks. We extend our results to constructing networks with bounded weights, and to constructing networks with width at most d + 2 , which is close to the minimal possible width due to previous lower bounds. Both of these constructions cause an extra polynomial factor in the number of parameters over the target network. We also show an exact representation of wide and shallow networks using deep and narrow networks which, in certain cases, does not increase the number of parameters over the target network."]}
{"id": "44", "text": ["This paper studies the problem of detecting the presence of a small dense community planted in a large Erd\\H{o}s-R\\'enyi random graph $\\mathcal{G}(N,q)$, where the edge probability within the community exceeds $q$ by a constant factor. Assuming the hardness of the planted clique detection problem, we show that the computational complexity of detecting the community exhibits the following phase transition phenomenon: As the graph size $N$ grows and the graph becomes sparser according to $q=N^{-\\alpha}$, there exists a critical value of $\\alpha = \\frac{2}{3}$, below which there exists a computationally intensive procedure that can detect far smaller communities than any computationally efficient procedure, and above which a linear-time procedure is statistically optimal. The results also lead to the average-case hardness results for recovering the dense community and approximating the densest $K$-subgraph."]}
{"id": "45", "text": ["Hamilton and Moitra (2021) showed that, in certain regimes, it is not possible to accelerate Riemannian gradient descent in the hyperbolic plane if we restrict ourselves to algorithms which make queries in a (large) bounded domain and which receive gradients and function values corrupted by a (small) amount of noise. We show that acceleration remains unachievable for any deterministic algorithm which receives exact gradient and function-value information (unbounded queries, no noise). Our results hold for a large class of Hadamard manifolds including hyperbolic spaces and the symmetric space SL( n ) / SO( n ) of positive de\ufb01nite n \u00d7 n matrices of determinant one. This cements a surprising gap between the complexity of convex optimization and geodesically convex optimization: for hyperbolic spaces, Riemannian gradient descent is optimal on the class of smooth and strongly geodesically convex functions (in the regime where the condition number scales with the radius of the optimization domain). The key idea for proving the lower bound consists of perturbing squared distance functions with sums of bump functions chosen by a resisting oracle."]}
{"id": "46", "text": ["We study Boolean functions of an arbitrary number of input variables that can be realized by simple iterative constructions based on constant-size primitives. This restricted type of construction needs little global coordination or control and thus is a candidate for neurally feasible computation. Valiant's construction of a majority function can be realized in this manner and, as we show, can be generalized to any uniform threshold function. We study the rate of convergence, finding that while linear convergence to the correct function can be achieved for any threshold using a fixed set of primitives, for quadratic convergence, the size of the primitives must grow as the threshold approaches 0 or 1. We also study finite realizations of this process and the learnability of the functions realized. We show that the constructions realized are accurate outside a small interval near the target threshold, where the size of the construction grows as the inverse square of the interval width. This phenomenon, that errors are higher closer to thresholds (and thresholds closer to the boundary are harder to represent), is a well-known cognitive finding."]}
{"id": "47", "text": ["Conventional wisdom in the sampling literature, backed by a popular diffusion scaling limit, suggests that the mixing time of the Metropolis-Adjusted Langevin Algorithm (MALA) scales as O(d), where d is the dimension. However, the diffusion scaling limit requires stringent assumptions on the target distribution and is asymptotic in nature. In contrast, the best known nonasymptotic mixing time bound for MALA on the class of log-smooth and strongly log-concave distributions is O(d). In this work, we establish that the mixing time of MALA on this class of target distributions is \u0398\u0303(d) under a warm start. Our upper bound proof introduces a new technique based on a projection characterization of the Metropolis adjustment which reduces the study of MALA to the well-studied discretization analysis of the Langevin SDE and bypasses direct computation of the acceptance probability."]}
{"id": "48", "text": ["In this paper, we present and analyze a simple and robust spectral algorithm for the stochastic block model with $k$ blocks, for any $k$ fixed. Our algorithm works with graphs having constant edge density, under an optimal condition on the gap between the density inside a block and the density between the blocks. As a co-product, we settle an open question posed by Abbe et. al. concerning censor block models."]}
{"id": "49", "text": ["Determinantal Point Processes (DPPs) are a widely used probabilistic model for negatively corre-lated sets. DPPs have been successfully employed in Machine Learning applications to select a diverse, yet representative subset of data. In these applications, the parameters of the DPP need to be \ufb01tted to match the data; typically, we seek a set of parameters that maximize the likelihood of the data. The algorithms used for this task to date either optimize over a limited family of DPPs, or use local improvement heuristics that do not provide theoretical guarantees of optimality. It is natural to ask if there exist ef\ufb01cient algorithms for \ufb01nding a maximum likelihood DPP model for a given data set. In seminal work on DPPs in Machine Learning, Kulesza conjectured in his PhD Thesis (2012) that the problem is NP-complete. The lack of a formal proof prompted Brunel, Moitra, Rigollet and Urschel (2017a) to conjecture that, in opposition to Kulesza\u2019s conjecture, there exists a polynomial-time algorithm for computing a maximum-likelihood DPP. They also presented some preliminary evidence supporting their conjecture. In this work we prove Kulesza\u2019s conjecture. In fact, we prove the following stronger hardness of approximation result: even computing a 1 \u2212 1 polylog N -approximation to the maximum log-likelihood of a DPP on a ground set of N elements is NP-complete. At the same time, we also obtain the \ufb01rst polynomial-time algorithm that achieves a nontrivial worst-case approximation to the optimal log-likelihood: the approximation factor is unconditionally (for data sets that consist of al., 2013b; et al., 2015; Affandi et al., 2013a), signal processing (Xu and Ou, Krause et al., Guestrin et al., 2005), clustering (Zou and 2012; Kang, 2013; and Ghahramani, 2013), recommendation systems (Zhou et al., 2010), revenue maximization (Dughmi et al., 2009), multi-agent reinforcement and al., 2020), modeling neural sketching for linear and low-rank"]}
{"id": "50", "text": ["The limit of the entropy in the stochastic block model (SBM) has been characterized in the sparse regime for the special case of disassortative communities [COKPZ17] and for the classical case of assortative communities but in the dense regime [DAM16]. The problem has not been closed in the classical sparse and assortative case. This paper establishes the result in this case for any SNR besides for the interval (1, 3.513). It further gives an approximation to the limit in this window. The result is obtained by expressing the global SBM entropy as an integral of local tree entropies in a broadcasting on tree model with erasure side-information. The main technical advancement then relies on showing the irrelevance of the boundary in such a model, also studied with variants in [KMS16], [MNS16] and [MX15]. In particular, we establish the uniqueness of the BP fixed point in the survey model for any SNR above 3.513 or below 1. This only leaves a narrow region in the plane between SNR and survey strength where the uniqueness of BP conjectured in these papers remains unproved."]}
{"id": "51", "text": ["We study the sample complexity of learning threshold functions under the constraint of differential privacy. It is assumed that each labeled example in the training data is the information of one individual and we would like to come up with a generalizing hypothesis $h$ while guaranteeing differential privacy for the individuals. Intuitively, this means that any single labeled example in the training data should not have a significant effect on the choice of the hypothesis. This problem has received much attention recently; unlike the non-private case, where the sample complexity is independent of the domain size and just depends on the desired accuracy and confidence, for private learning the sample complexity must depend on the domain size $X$ (even for approximate differential privacy). Alon et al. (STOC 2019) showed a lower bound of $\\Omega(\\log^*|X|)$ on the sample complexity and Bun et al. (FOCS 2015) presented an approximate-private learner with sample complexity $\\tilde{O}\\left(2^{\\log^*|X|}\\right)$. In this work we reduce this gap significantly, almost settling the sample complexity. We first present a new upper bound (algorithm) of $\\tilde{O}\\left(\\left(\\log^*|X|\\right)^2\\right)$ on the sample complexity and then present an improved version with sample complexity $\\tilde{O}\\left(\\left(\\log^*|X|\\right)^{1.5}\\right)$.  Our algorithm is constructed for the related interior point problem, where the goal is to find a point between the largest and smallest input elements. It is based on selecting an input-dependent hash function and using it to embed the database into a domain whose size is reduced logarithmically; this results in a new database, an interior point of which can be used to generate an interior point in the original database in a differentially private manner."]}
{"id": "52", "text": ["We consider the problem of the Zinkevich (2003)-style dynamic regret minimization in online learning with exp-concave losses. We show that whenever improper learning is allowed, a Strongly Adaptive online learner achieves the dynamic regret of \u00d5(dnC n \u2228d logn) where Cn is the total variation (a.k.a. path length) of the an arbitrary sequence of comparators that may not be known to the learner ahead of time. Achieving this rate was highly nontrivial even for squared losses in 1D where the best known upper bound was O( \u221a nCn \u2228 logn) (Yuan and Lamperski, 2019). Our new proof techniques make elegant use of the intricate structures of the primal and dual variables imposed by the KKT conditions and could be of independent interest. Finally, we apply our results to the classical statistical problem of locally adaptive non-parametric regression (Mammen, 1991; Donoho and Johnstone, 1998) and obtain a stronger and more flexible algorithm that do not require any statistical assumptions or any hyperparameter tuning."]}
{"id": "53", "text": ["We study the problem of learning a node-labeled tree given independent traces from an appropriately defined deletion channel. This problem, tree trace reconstruction, generalizes string trace reconstruction, which corresponds to the tree being a path. For many classes of trees, including complete trees and spiders, we provide algorithms that reconstruct the labels using only a polynomial number of traces. This exhibits a stark contrast to known results on string trace reconstruction, which require exponentially many traces, and where a central open problem is to determine whether a polynomial number of traces suffice. Our techniques combine novel combinatorial and complex analytic methods."]}
{"id": "54", "text": ["We introduce online learning with vector costs (\\OLVCp) where in each time step $t \\in \\{1,\\ldots, T\\}$, we need to play an action $i \\in \\{1,\\ldots,n\\}$ that incurs an unknown vector cost in $[0,1]^{d}$. The goal of the online algorithm is to minimize the $\\ell_p$ norm of the sum of its cost vectors. This captures the classical online learning setting for $d=1$, and is interesting for general $d$ because of applications like online scheduling where we want to balance the load between different machines (dimensions).  We study \\OLVCp in both stochastic and adversarial arrival settings, and give a general procedure to reduce the problem from $d$ dimensions to a single dimension. This allows us to use classical online learning algorithms in both full and bandit feedback models to obtain (near) optimal results. In particular, we obtain a single algorithm (up to the choice of learning rate) that gives sublinear regret for stochastic arrivals and a tight $O(\\min\\{p, \\log d\\})$ competitive ratio for adversarial arrivals.  The \\OLVCp problem also occurs as a natural subproblem when trying to solve the popular Bandits with Knapsacks (\\BwK) problem. This connection allows us to use our \\OLVCp techniques to obtain (near) optimal results for \\BwK in both stochastic and adversarial settings. In particular, we obtain a tight $O(\\log d \\cdot \\log T)$ competitive ratio algorithm for adversarial \\BwK, which improves over the $O(d \\cdot \\log T)$ competitive ratio algorithm of Immorlica et al. [FOCS'19]."]}
{"id": "55", "text": ["We study the sample complexity of learning neural networks by providing new bounds on their Rademacher complexity, assuming norm constraints on the parameter matrix of each layer. Compared to previous work, these complexity bounds have improved dependence on the network depth and, under some additional assumptions, are fully independent of the network size (both depth and width). These results are derived using some novel techniques, which may be of independent interest."]}
{"id": "56", "text": ["We propose and analyze a variant of the classic Polyak-Ruppert averaging scheme, broadly used in stochastic gradient methods. Rather than a uniform average of the iterates, we consider a weighted average, with weights decaying in a geometric fashion. In the context of linear least squares regression, we show that this averaging scheme has a the same regularizing effect, and indeed is asymptotically equivalent, to ridge regression. In particular, we derive finite-sample bounds for the proposed approach that match the best known results for regularized stochastic gradient methods."]}
{"id": "57", "text": ["It is well known that Sparse PCA (Sparse Principal Component Analysis) is NP-hard to solve exactly on worst-case instances. What is the complexity of solving Sparse PCA approximately? Our contributions include: 1. a simple and efficient algorithm that achieves ann 1=3 -approximation; 2. NP-hardness of approximation to within (1 \"), for some small constant\" > 0; 3. SSE-hardness of approximation to within any constant factor; and 4. an exp exp p log logn (\u201cquasi-quasi-polynomial\u201d) gap for the standard semidefinite program."]}
{"id": "58", "text": ["This paper derives a new strong Gaussian approximation bound for the sum of independent random vectors. The approach relies on the optimal transport theory and yields explicit dependence on the dimension size p and the sample size n . This dependence establishes a new fundamental limit for all practical applications of statistical learning theory. Particularly, based on this bound, we prove approximation in distribution for the maximum norm in a high-dimensional setting ( p > n )."]}
{"id": "59", "text": ["We study a variant of the sparse PCA (principal component analysis) problem in the \"hard\" regime, where the inference task is possible yet no polynomial-time algorithm is known to exist. Prior work, based on the low-degree likelihood ratio, has conjectured a precise expression for the best possible (sub-exponential) runtime throughout the hard regime. Following instead a statistical physics inspired point of view, we show bounds on the depth of free energy wells for various Gibbs measures naturally associated to the problem. These free energy wells imply hitting time lower bounds that corroborate the low-degree conjecture: we show that a class of natural MCMC (Markov chain Monte Carlo) methods (with worst-case initialization) cannot solve sparse PCA with less than the conjectured runtime. These lower bounds apply to a wide range of values for two tuning parameters: temperature and sparsity misparametrization. Finally, we prove that the Overlap Gap Property (OGP), a structural property that implies failure of certain local search algorithms, holds in a significant part of the hard regime."]}
{"id": "60", "text": ["By 2026, today's fifth graders will be entering college and our first-year college students will be the assistant professors. The workforce will see (r)evolutionary changes in the workplace at the human-technology frontier. Participants will be asked to think strategically about how we will reach the future we want to live. Content delivery mechanisms and active learning/project-based experiences for undergraduate students have developed and matured at a dramatic pace in recent years. How can we leverage what we already know, working to improve and broaden undergraduate STEM education and computing education in particular? How do we prepare students to solve the wicked societal problems of the future? Will students require problem-solving skills that transcend disciplines? Is interdisciplinarity teachable at the undergraduate computer science level? Which skills/knowledge will employers and graduate schools require in 10/20 years? The National Science Foundation Division of Undergraduate Education is preparing to have a nationwide dialogue with STEM communities and industry partners on this topic. Participants in this highly interactive session will collectively construct a shared vision for the future of computing sciences education, and outline a research agenda for the future."]}
{"id": "61", "text": ["Nowadays, researchers are looking for ways to adapt education to include technology and more engaging methods. For computer science courses, the use of problem-based learning, virtual tutors, learning style personalization and game programming are possible ways to achieve this goal. In this context, this work proposes and evaluates an approach to generate different kinds of virtual tutors (with various levels of interaction) for problem-based game programming classes. The system was evaluated in an experiment where two types of virtual tutors were proposed and generated considering two virtual game programming classes. Results showed that the use of virtual tutors promoted efficient learning and that the availability of different kinds of tutors can optimize individual engagement."]}
{"id": "62", "text": ["The Blackbox project collects programming activity data from users of BlueJ, a Java IDE aimed at novices. The Blackbox data set has grown very large, with several terabytes of source code data. This is a double-edged sword; it provides large amounts of data for analysis, but it can be difficult for newcomer researchers to get started with analysis. In this workshop we will introduce attendees to analysing the data by using Blackbox Mini: a new curated subset of the Blackbox data set, designed to help researchers try out their source code analyses on a smaller data set. The workshop will be run by the designers and maintainers of the Blackbox (and Blackbox Mini) data sets. Attendees at the workshop will learn how to work with the Blackbox Mini data set, including basic source code analysis. The Blackbox Mini data set can provide useful publishable research results itself, or the analyses can be carried over and run on larger subsets of the full Blackbox data set."]}
{"id": "63", "text": ["Each year, we graduate a fair number of students with college degrees in computing-related disciplines. Each year, we lose a fair number of students out of the discipline. We spend a good deal of time focusing on what is causing students to leave or fail. We do not spend a lot of time focusing on the learning journey and supporting students throughout their transformative experiences. Threshold concepts are those concepts or learning experiences which define the overall disciplinary learning journey, enabling the learner to see from a new perspective, participate in discourse previously unavailable to them, and engage with the world from a transformed frame of understanding. They are often those concepts difficult for students to understand within a discipline such as \"personhood' in Philosophy, 'gravity' in Physics, or 'limit' in Mathematics. As a result of this transformation and the resultant traversal of a liminal state, elements of an identity developed. In this poster, we present our current work identifying potential threshold concepts experienced by intermediate computer science students and consider the impact of these concepts on their development of an identity within computer science."]}
{"id": "64", "text": ["Many K-12 and university classrooms are now using block programming languages (e.g., Scratch, App Inventor, Code.org) to help students learn how to program. These block programming languages are popular because of their simplicity and \"tinkerability\" allowing novice users to create a project within minutes of first being exposed to the language. Unfortunately, these languages are highly dependent on the mouse and keyboard making them nearly inaccessible for those users with visual or motor impairments. This poster presents CodeBox64, a simplified input modality that is able to program block programming languages in a more tactile approach; it is a Tactile Input Modality (TIM). Because of the simplicity of CodeBox64, it allows visually impaired students to navigate the buttons and knobs with ease. CodeBox64 consists of four navigational buttons (i.e., up, down, left, right), a back button, and an enter button. It also contains an RFID sensor board that allows the user to use physical Lego blocks to execute commands of a block language. While CodeBox64 was originally developed to work with a custom, Blockly language, JamBlocks, it has the potential to work with other block languages. CodeBox64 demonstrates one possible methodology for enabling block languages to be accessible to those users with visual impairments."]}
{"id": "65", "text": ["We present MIPSUnit, a unit test framework for MIPS assembly. MIPSUnit's primary benefit is that it reduces the time needed to grade assembly language assignments. It also provides a time-efficient means for giving students additional testing experience; therefore, it can serve as one component of a curriculum-wide emphasis on testing. MIPSUnit is a suite of two tools: MUnit, which allows users to test their assembly code with JUnit tests, and MSpec, which uses RSpec-style unit tests."]}
{"id": "66", "text": ["Research shows the benefit of using active learning in computer science education; however, only limited resources (such as, prior publications) exist for systems courses (including architecture, networking, operating systems). This BoF brings together practitioners of various levels of experience to discuss ways to augment or replace traditional lecturing. We will discuss different techniques, possible materials available, and results measured. This BoF should benefit both instructors considering adopting techniques and instructors looking to discuss issues with their usage."]}
{"id": "67", "text": ["At the direction of the ACM Education Board, the IT2017 Task Group was formed with the charge of updating the joint ACM and IEEE Computer Society Curriculum Guidelines for Undergraduate Degree Programs in Information Technology, known as IT2008. The revised document, called IT2017, should be appropriately forward looking given the significant advances in information technology that have occurred since 2008. Participants attending the BOF will contribute their insights and assist with the revision process to update IT2008. Discussions will center on delineating knowledge areas and learning outcomes specific to IT; exploring the current and future roles of IT in computing disciplines; recommending changes to improve the usefulness of the report; and planning further communications to fully engage the academic and professional community in the revision process. The objective is to ensure that the updated document is a forward-looking curriculum framework of the disciplinary content and practices in the field of information technology and remains relevant into 2020 and beyond for incoming students, computing departments with IT programs, accreditation bodies, and employers in the U.S. and anywhere else in the world."]}
{"id": "68", "text": ["As students transition into higher education, their experience can be quite new and foreign to them - at times acting as a barrier into higher education. This experience, while an individual one, consists of many concerns that are shared amongst these transitioning students. Partly as a result of these concerns, retention rates of first year Computer Science students suffer. These topics were discussed and analysed at length in various studies, panels and working groups to create an initial body of understanding, influenced by a number of institutions across the world. However, the recent and unprecedented COVID-19 pandemic has caused further disruption: it is likely that students transitioning into higher education during the pandemic were faced with unparalleled concerns, anxieties and stresses."]}
{"id": "69", "text": ["The entire enterprise of computer science education is predicated on the ability to develop and sustain students' interest in the subject. Given how fundamental this aspect of the educational process is, our understanding of what experiences are actually driving the development of interest in computer science remains far from complete. This BOF session seeks to inform the direction of a new research project dedicated to investigating this question, by eliciting a discussion from expert practitioners about what they know from their experiences about how interest in CS develops."]}
{"id": "70", "text": ["Rapid and dramatic demographic and technological changes require our nation's schools, colleges, and universities to work with agility in preparing students - particularly those from diverse backgrounds - for careers in science, technology, engineering, and math (STEM) fields, including computer science. Exploring the interplay of technology, education, and inclusion over the past 50 years, Dr. Hrabowski examines what this means for our future work in higher education. Computing education provides a critically important case. As the sector evolves rapidly, many well-paid jobs go unfilled and we must explore ways to draw on talent wherever it is found. Emphasizing themes from his TED talk on student success, Dr. Hrabowski focuses our attention on the importance of high expectations and hard work, building community among students, faculty engagement with students, and rigorous assessment of what works. He assesses the way innovative approaches - including course re-design, active and experiential learning, research, and partnerships with companies and agencies - promote student success, inclusive excellence, and achievement for all students both in STEM generally and computer science in particular."]}
{"id": "71", "text": ["K-12 computer science (CS) teachers are often the only teachers of the subject at their school. Many school-based administrators and personnel lack the content knowledge to support their ongoing professional growth. How then can an ecosystem of support be developed to support K-12 CS teachers? We have created several tools aligned to the CSTA Standards for CS Teachers that support administrators, instructional specialists, and teacher leaders to provide evidence-based feedback and promote the ongoing development of CS teachers at their schools. These tools, including a CS coaching toolkit and instructional practice evidence guide, have the potential to drive impactful, job-embedded development."]}
{"id": "72", "text": ["Cyber defence exercises (CDX) represent a popular form of hands-on security training. Learners are usually divided into several teams that have to defend or attack virtual IT infrastructure (red vs. blue teams). CDXs are prepared for learners whose level of skills, knowledge, and background may be unknown or very diverse. This is evident in the case of high-profile international CDXs with hundreds of participants coming from government agencies, military, academia, and the private sector. In this poster, we present techniques for distributing learners into teams with respect to their level of proficiency and the prerequisite skills required by the exercise. Our aim is to reach a balance between proficiency and the exercise to make the exercise beneficial for the learners and an effective investment for sponsors. The poster describes three methods and compares their advantages and disadvantages. First, we present self-assessment questionnaires, which we have already used in four runs of a national CDX for 80 participants. We outline our findings from an analysis of the learners' self-assessment before and after the exercise, and the score they achieved during the exercise. Second, we introduce a promising method for testing the prerequisites of the exercise. This is still a work in progress but we believe that this method enables the better assessment of learners' skills with respect to the exercise content, and supports the game balance better. Finally, we compare both methods to a na\u00efve one that shuffles participants into teams randomly."]}
{"id": "73", "text": ["The paper presents research conducted with high school (HS) students (N=86) learning object-oriented programming (OOP) and computer science HS teachers (N=48). The focus was on students' and teachers' understanding of the this reference. Proper conceptualization of this indicates an understanding of objects in general and of the current object, and it involves various aspects of programming variants. Students' preferences as to the use of the this reference were also examined. Findings revealed a lack of understanding of both the implication and the implementation of this; only 45% of the students expressed understanding of when we must use this; only 60% expressed understanding of when not to use this, and only 24% expressed clear understanding in their definition of this. Even correct answers do not necessarily indicate conceptual understanding, rather a repetition of definitions or programming habits, or a reliance on operative aspects of the implementation. The teachers expressed a considerable lack of clarity in accurately characterizing the correctness of students' answers."]}
{"id": "74", "text": ["This paper contains a description of a follow-on to a pilot study in which students performed reflective activities as part of the design process in an advanced programming course. Students produced an initial design for their programs that was due within a week after the program was assigned. Along with their projects, students submitted a document reflecting the final design and an analysis of the changes between them. Requirements for the analysis were made more explicit than those in the pilot study. The format of the document was specified and the task was described to the students as a technical writing activity. Results of the work are reported and a comparison with prior work that did not have a specified structure for the student analysis are described."]}
{"id": "75", "text": ["The Computer-Science Student-Centered Instructional Continuum (CS-SCIC) is a new framework to support PreK-12 instructors in their lesson design. Educators are faced with choices when building lessons; there is a tension between direct instruction, constructivism and constructionism and difficulty in providing differentiated instruction. Theoretically aligned to Vygotsky's zone of proximal development, CS-SCIC places research-based instructional strategies on a simple learning continuum. Teachers use the continuum to discuss, review and design learning events. Used internationally, initial qualitative feedback from teachers who attended pilot CS-SIC workshops was emphatically positive. Future work includes more feedback from academia and formal research, including pre and post-professional development workshop surveys."]}
{"id": "76", "text": ["Computing, together with jewelry design and 3D printing offers an innovative introduction to the possibilities of technology and programming. This combination was presented to participants of a weeklong summer technology day camp focusing on computer-aided design (CAD) using jewelry design and modeling. The participants then saw their design creations built through 3D printing. In this paper we present the design of a weeklong day camp called \"Computing and Jewelry Design\". Initial findings are positive and suggest opportunities for improvement."]}
{"id": "77", "text": ["We need to greatly expand the community of faculty teaching cybersecurity using hands-on exercises. The number of security-focused competitions and exercises has increased in recent years so that faculty need to choose those that can best be integrated into their courses. As a community, we can contribute to each other/s projects. We would like to bring together both new and experienced faculty to build community. Our Facebook group (https://www.facebook.com/groups/TeachingCyber/) allows us to work together outside of SIGCSE events, and we are developing a critical mass of users. In the BoF, participants can sign up for the closed group. Participants will discuss answers to specific questions on teaching security. One goal of our BoF is to provide support for new members to learn about resources and get help. These resources include exercises, webinars, and slides. We will share experiences, practices and ongoing efforts, including our own (e.g. Security Injections, the Security Knitting Kit project, and EDURange). The BoF also benefits experienced members, helping them to disseminate their work and reach other faculty with similar interests. As a community we have begun to share exercises and discuss what works and what problems students and instructors have encountered. We will discuss ways to integrate security-related exercises into existing courses. The questions we will ask are, What materials and exercises have you tried? What are your experiences? How do we expand and improve cybersecurity education?"]}
{"id": "78", "text": ["How has CS education changed since SIGCSE first began in 1968? This panel of presenters, with 135 years of teaching instruction between them, will remind some attendees of what used to be and educate newer members on what life was like pre-internet. Topics to be discussed include changes in content, presentation medium, equipment and environment, students and audience, and pedagogy. Audience members will be engaged through peer instruction questions during and between topic discussions and may use their cell phones or laptops to respond to questions. This panel will allow our community to acknowledge the diversity and experience in our organization and celebrate our heritage. We believe old-timers who wish to reminisce and young, new instructors who are interested in a historical aspect will be interested in the session."]}
{"id": "79", "text": ["For novice programmers one of the most problematic concepts is variable assignment and evaluation. Several questions emerge in the mind of the beginner, such as what does x = 7 + 4 or x = x + 1 really mean? For instance, many students initially think that such statements store the entire calculation in variable x, evaluating the result lazily when actually needed. The common increment pattern x = x + 1 is even believed to be outright impossible. This paper discusses a multi-year project examining how high school students think of assignments and variables. In particular, where does the misconception of storing entire calculations come from? Can we explain the students' thinking and help them develop correct models of how programming works? It is particularly striking that a model of the computer as a machine with algebraic capabilities would indeed produce the observed misconceptions. The misconception might simply be attributed to the expectation that the computer performs computations the exact same way students are taught to in mathematics."]}
